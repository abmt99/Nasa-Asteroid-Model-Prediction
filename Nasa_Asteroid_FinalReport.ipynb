{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Nasa_Asteroid_FinalReport.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPUk2wUVE8MjOLFyXG+uQq9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abmt99/Nasa-Asteroid-Model-Prediction/blob/master/Nasa_Asteroid_FinalReport.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVLKxVkq7gsI",
        "colab_type": "text"
      },
      "source": [
        "#NASA ASTEROID CLASSIFICATION\n",
        "## A)Data Preparation and Analysis \n",
        "Curation of database"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMrMolEgUnz0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ssdCe-QasfU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from google.colab import files\n",
        "import seaborn as sns\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PD-aCyO3tHk-",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "outputId": "824602a0-f21c-4105-ec16-8de375a20eaa"
      },
      "source": [
        "uploaded = files.upload()\n",
        "#URL for database:\n",
        "myurl='https://github.com/abmt99/Nasa_Asteroid/blob/master/Nasa_asteroid.csv'\n",
        "Data=pd.read_csv('Nasa_asteroid.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a2469ded-07b4-49ed-9cc1-8886da942cd3\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-a2469ded-07b4-49ed-9cc1-8886da942cd3\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving Nasa_asteroid.csv to Nasa_asteroid (6).csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjZbaFmH4KW7",
        "colab_type": "text"
      },
      "source": [
        "Removing Null value records"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQGD921gtgsR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "da195eae-0d3b-40bc-9217-7d56aac1ac0f"
      },
      "source": [
        "#print(Data.shape)\n",
        "Data.isnull().values.any()\n",
        "# If there are blank cells\n",
        "Data = Data.dropna()\n",
        "print('After removing records containing null values: ')\n",
        "#print(Data.shape)\n",
        "Data.Hazardous = Data.Hazardous.astype(int)\n",
        "#this means there are no null val\n",
        "Data.head"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "After removing records containing null values: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.head of       Neo Reference ID     Name  ...  Equinox  Hazardous\n",
              "0              3703080  3703080  ...    J2000          1\n",
              "1              3723955  3723955  ...    J2000          0\n",
              "2              2446862  2446862  ...    J2000          1\n",
              "3              3092506  3092506  ...    J2000          0\n",
              "4              3514799  3514799  ...    J2000          1\n",
              "...                ...      ...  ...      ...        ...\n",
              "4682           3759007  3759007  ...    J2000          0\n",
              "4683           3759295  3759295  ...    J2000          0\n",
              "4684           3759714  3759714  ...    J2000          0\n",
              "4685           3759720  3759720  ...    J2000          0\n",
              "4686           3772978  3772978  ...    J2000          0\n",
              "\n",
              "[4687 rows x 40 columns]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cP0XtTKGvuGO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Newdata=np.array(Data[['Absolute Magnitude','Est Dia in KM(max)','Relative Velocity km per sec','Miss Dist.(kilometers)','Miles per hour','Orbit ID','Orbit Uncertainity','Minimum Orbit Intersection','Jupiter Tisserand Invariant','Eccentricity','Semi Major Axis','Inclination','Asc Node Longitude','Perihelion Distance','Perihelion Arg','Perihelion Time','Mean Anomaly','Hazardous']])\n",
        "#print(Newdata)\n",
        "np.set_printoptions(precision = 2)\n",
        "np.set_printoptions(formatter = {'float': '{: 0.1f}'.format})\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QswKfP1FtphR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 743
        },
        "outputId": "6d2d7140-0a10-4220-a12c-e636ee6ee315"
      },
      "source": [
        "correlation = Data.corr()\n",
        "sns.heatmap(correlation, xticklabels=correlation.columns, yticklabels=correlation.columns)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fa6817ffa90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRmKuS9X-Bs6",
        "colab_type": "text"
      },
      "source": [
        "INPUT FEATURES VISUALIZATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMvef5Pn9jpb",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "outputId": "bfbafd45-73fb-41f0-a1e7-7b95513c9a37"
      },
      "source": [
        "uploaded = files.upload() \n",
        "newurl='https://github.com/abmt99/Nasa_Asteroid/blob/master/Reduced_Nasa_asteroid.csv'\n",
        "red=pd.read_csv('Reduced_Nasa_asteroidFinal.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d89f8cb1-ac83-4f1b-9716-806df133dda4\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-d89f8cb1-ac83-4f1b-9716-806df133dda4\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving Reduced_Nasa_asteroidFinal.csv to Reduced_Nasa_asteroidFinal (2).csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdCcdGUS-R6C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 964
        },
        "outputId": "8cfa30d9-a85f-4e62-e26b-45449d8f2a05"
      },
      "source": [
        "plt.rcParams[\"figure.figsize\"] = ((10,10))\n",
        "red.hist()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7fa6808ea518>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7fa68081f4e0>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7fa6807d6160>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7fa6807864a8>],\n",
              "       [<matplotlib.axes._subplots.AxesSubplot object at 0x7fa68073a748>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7fa6806eaac8>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7fa68071de48>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7fa6806db1d0>],\n",
              "       [<matplotlib.axes._subplots.AxesSubplot object at 0x7fa6806db240>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7fa68063f908>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7fa6805f2c88>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7fa6805a5fd0>],\n",
              "       [<matplotlib.axes._subplots.AxesSubplot object at 0x7fa6805e13c8>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7fa680594748>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7fa680543ac8>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7fa6804f8e48>],\n",
              "       [<matplotlib.axes._subplots.AxesSubplot object at 0x7fa6804b7208>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7fa680466588>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7fa68049c908>,\n",
              "        <matplotlib.axes._subplots.AxesSubplot object at 0x7fa68044cc88>]],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 20 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wy1KBCq07_oE",
        "colab_type": "text"
      },
      "source": [
        "Calculate mean,std,min,max"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rq9jC_8W_uSo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "outputId": "51540ea6-113a-4ad2-c908-8c6eed3ca9c4"
      },
      "source": [
        "red.describe().T"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "      <th>min</th>\n",
              "      <th>25%</th>\n",
              "      <th>50%</th>\n",
              "      <th>75%</th>\n",
              "      <th>max</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Absolute Magnitude</th>\n",
              "      <td>4687.0</td>\n",
              "      <td>2.226786e+01</td>\n",
              "      <td>2.890972e+00</td>\n",
              "      <td>1.116000e+01</td>\n",
              "      <td>2.010000e+01</td>\n",
              "      <td>2.190000e+01</td>\n",
              "      <td>2.450000e+01</td>\n",
              "      <td>3.210000e+01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Est Dia in KM(max)</th>\n",
              "      <td>4687.0</td>\n",
              "      <td>4.575089e-01</td>\n",
              "      <td>8.263912e-01</td>\n",
              "      <td>2.259644e-03</td>\n",
              "      <td>7.482384e-02</td>\n",
              "      <td>2.477650e-01</td>\n",
              "      <td>5.675969e-01</td>\n",
              "      <td>3.483694e+01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Relative _Velocity km per sec</th>\n",
              "      <td>4687.0</td>\n",
              "      <td>1.397081e+01</td>\n",
              "      <td>7.293223e+00</td>\n",
              "      <td>3.355041e-01</td>\n",
              "      <td>8.432865e+00</td>\n",
              "      <td>1.291789e+01</td>\n",
              "      <td>1.807765e+01</td>\n",
              "      <td>4.463375e+01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Miss_Dist.(kilometers)</th>\n",
              "      <td>4687.0</td>\n",
              "      <td>3.841347e+07</td>\n",
              "      <td>2.181110e+07</td>\n",
              "      <td>2.660989e+04</td>\n",
              "      <td>1.995928e+07</td>\n",
              "      <td>3.964771e+07</td>\n",
              "      <td>5.746863e+07</td>\n",
              "      <td>7.478160e+07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Miles_per_hour</th>\n",
              "      <td>4687.0</td>\n",
              "      <td>3.125131e+04</td>\n",
              "      <td>1.631421e+04</td>\n",
              "      <td>7.504891e+02</td>\n",
              "      <td>1.886348e+04</td>\n",
              "      <td>2.889603e+04</td>\n",
              "      <td>4.043789e+04</td>\n",
              "      <td>9.984123e+04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Orbit_ID</th>\n",
              "      <td>4687.0</td>\n",
              "      <td>2.830062e+01</td>\n",
              "      <td>3.829967e+01</td>\n",
              "      <td>1.000000e+00</td>\n",
              "      <td>9.000000e+00</td>\n",
              "      <td>1.600000e+01</td>\n",
              "      <td>3.100000e+01</td>\n",
              "      <td>6.110000e+02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Orbit_Uncertainity</th>\n",
              "      <td>4687.0</td>\n",
              "      <td>3.516962e+00</td>\n",
              "      <td>3.078307e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>3.000000e+00</td>\n",
              "      <td>6.000000e+00</td>\n",
              "      <td>9.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Minimum_Orbit Intersection</th>\n",
              "      <td>4687.0</td>\n",
              "      <td>8.232007e-02</td>\n",
              "      <td>9.029997e-02</td>\n",
              "      <td>2.060000e-06</td>\n",
              "      <td>1.458510e-02</td>\n",
              "      <td>4.736550e-02</td>\n",
              "      <td>1.235935e-01</td>\n",
              "      <td>4.778910e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Jupiter_Tisserand Invariant</th>\n",
              "      <td>4687.0</td>\n",
              "      <td>5.056111e+00</td>\n",
              "      <td>1.237818e+00</td>\n",
              "      <td>2.196000e+00</td>\n",
              "      <td>4.049500e+00</td>\n",
              "      <td>5.071000e+00</td>\n",
              "      <td>6.019000e+00</td>\n",
              "      <td>9.025000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Eccentricity</th>\n",
              "      <td>4687.0</td>\n",
              "      <td>3.825691e-01</td>\n",
              "      <td>1.804438e-01</td>\n",
              "      <td>7.522355e-03</td>\n",
              "      <td>2.408583e-01</td>\n",
              "      <td>3.724502e-01</td>\n",
              "      <td>5.124106e-01</td>\n",
              "      <td>9.602607e-01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Semi_Major Axis</th>\n",
              "      <td>4687.0</td>\n",
              "      <td>1.400264e+00</td>\n",
              "      <td>5.241539e-01</td>\n",
              "      <td>6.159204e-01</td>\n",
              "      <td>1.000635e+00</td>\n",
              "      <td>1.240981e+00</td>\n",
              "      <td>1.678364e+00</td>\n",
              "      <td>5.072008e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Inclination</th>\n",
              "      <td>4687.0</td>\n",
              "      <td>1.337384e+01</td>\n",
              "      <td>1.093623e+01</td>\n",
              "      <td>1.451294e-02</td>\n",
              "      <td>4.962341e+00</td>\n",
              "      <td>1.031184e+01</td>\n",
              "      <td>1.951168e+01</td>\n",
              "      <td>7.540667e+01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Asc_Node Longitude</th>\n",
              "      <td>4687.0</td>\n",
              "      <td>1.721573e+02</td>\n",
              "      <td>1.032768e+02</td>\n",
              "      <td>1.940674e-03</td>\n",
              "      <td>8.308121e+01</td>\n",
              "      <td>1.726254e+02</td>\n",
              "      <td>2.550269e+02</td>\n",
              "      <td>3.599059e+02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Perihelion_Distance</th>\n",
              "      <td>4687.0</td>\n",
              "      <td>8.133833e-01</td>\n",
              "      <td>2.420591e-01</td>\n",
              "      <td>8.074430e-02</td>\n",
              "      <td>6.308343e-01</td>\n",
              "      <td>8.331526e-01</td>\n",
              "      <td>9.972270e-01</td>\n",
              "      <td>1.299832e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Perihelion_Arg</th>\n",
              "      <td>4687.0</td>\n",
              "      <td>1.839322e+02</td>\n",
              "      <td>1.035130e+02</td>\n",
              "      <td>6.917625e-03</td>\n",
              "      <td>9.562592e+01</td>\n",
              "      <td>1.897616e+02</td>\n",
              "      <td>2.717776e+02</td>\n",
              "      <td>3.599931e+02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Perihelion_Time</th>\n",
              "      <td>4687.0</td>\n",
              "      <td>2.457728e+06</td>\n",
              "      <td>9.442264e+02</td>\n",
              "      <td>2.450100e+06</td>\n",
              "      <td>2.457815e+06</td>\n",
              "      <td>2.457973e+06</td>\n",
              "      <td>2.458108e+06</td>\n",
              "      <td>2.458839e+06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Mean_Anomaly</th>\n",
              "      <td>4687.0</td>\n",
              "      <td>1.811679e+02</td>\n",
              "      <td>1.075016e+02</td>\n",
              "      <td>3.191491e-03</td>\n",
              "      <td>8.700692e+01</td>\n",
              "      <td>1.857189e+02</td>\n",
              "      <td>2.765319e+02</td>\n",
              "      <td>3.599180e+02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Hazardous</th>\n",
              "      <td>4687.0</td>\n",
              "      <td>1.610838e-01</td>\n",
              "      <td>3.676475e-01</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.000000e+00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                count          mean  ...           75%           max\n",
              "Absolute Magnitude             4687.0  2.226786e+01  ...  2.450000e+01  3.210000e+01\n",
              "Est Dia in KM(max)             4687.0  4.575089e-01  ...  5.675969e-01  3.483694e+01\n",
              "Relative _Velocity km per sec  4687.0  1.397081e+01  ...  1.807765e+01  4.463375e+01\n",
              "Miss_Dist.(kilometers)         4687.0  3.841347e+07  ...  5.746863e+07  7.478160e+07\n",
              "Miles_per_hour                 4687.0  3.125131e+04  ...  4.043789e+04  9.984123e+04\n",
              "Orbit_ID                       4687.0  2.830062e+01  ...  3.100000e+01  6.110000e+02\n",
              "Orbit_Uncertainity             4687.0  3.516962e+00  ...  6.000000e+00  9.000000e+00\n",
              "Minimum_Orbit Intersection     4687.0  8.232007e-02  ...  1.235935e-01  4.778910e-01\n",
              "Jupiter_Tisserand Invariant    4687.0  5.056111e+00  ...  6.019000e+00  9.025000e+00\n",
              "Eccentricity                   4687.0  3.825691e-01  ...  5.124106e-01  9.602607e-01\n",
              "Semi_Major Axis                4687.0  1.400264e+00  ...  1.678364e+00  5.072008e+00\n",
              "Inclination                    4687.0  1.337384e+01  ...  1.951168e+01  7.540667e+01\n",
              "Asc_Node Longitude             4687.0  1.721573e+02  ...  2.550269e+02  3.599059e+02\n",
              "Perihelion_Distance            4687.0  8.133833e-01  ...  9.972270e-01  1.299832e+00\n",
              "Perihelion_Arg                 4687.0  1.839322e+02  ...  2.717776e+02  3.599931e+02\n",
              "Perihelion_Time                4687.0  2.457728e+06  ...  2.458108e+06  2.458839e+06\n",
              "Mean_Anomaly                   4687.0  1.811679e+02  ...  2.765319e+02  3.599180e+02\n",
              "Hazardous                      4687.0  1.610838e-01  ...  0.000000e+00  1.000000e+00\n",
              "\n",
              "[18 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0AHfsbGAP3n",
        "colab_type": "text"
      },
      "source": [
        "Output Visualization- Hazardous Or Non-Hazardous"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnI7DbeSAVAm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "outputId": "51382cce-b694-46eb-d709-03d6003fbda1"
      },
      "source": [
        "plt.hist(red.iloc[:, -1], 20, stacked=True, density=False)\n",
        "plt.title('Hazardous')\n",
        "plt.ylabel('count')\n",
        "plt.xlabel('Hazardous')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2AVKx-L-sAm",
        "colab_type": "text"
      },
      "source": [
        "##Split the database into Train data and Validation data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8a3bAwcl-xXU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "408cdcb3-23c1-47a4-99d0-9308cde8cfc7"
      },
      "source": [
        "np.random.shuffle(Newdata)\n",
        "X = Newdata[:, :-1]\n",
        "Y = Newdata[:, -1]\n",
        "\n",
        "index_30percent = int(0.3 * len(Newdata[:, 0]))\n",
        "print(index_30percent)\n",
        "# Split into training and validation\n",
        "XVALID = Newdata[:index_30percent, [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]]\n",
        "YVALID = Newdata[:index_30percent, 17]\n",
        "XTRAIN = Newdata[index_30percent:, [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]]\n",
        "YTRAIN = Newdata[index_30percent:, 17]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1406\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VYSuCZgBdc9",
        "colab_type": "text"
      },
      "source": [
        "Normalize data: \n",
        "\n",
        "Training data parameters are used for normalization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5klqWCSpBhIo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mean = XTRAIN.mean(axis=0)\n",
        "XTRAIN -= mean\n",
        "std = XTRAIN.std(axis=0)\n",
        "XTRAIN /= std\n",
        "\n",
        "XVALID -= mean\n",
        "XVALID /= std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuAERMmSz9rg",
        "colab_type": "text"
      },
      "source": [
        "## B) Model Selection and Evaluation\n",
        "Logistic Regression Model(Basic- 1 Layer)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fu62OxvU19x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a2a695be-7a00-4f38-ddbd-34e914e946db"
      },
      "source": [
        "\n",
        "#One layer\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "callback_a = ModelCheckpoint(filepath = 'Reduced_Nasa_asteroid.hdf5', monitor='val_loss', save_best_only = True, save_weights_only = True, verbose = 1)\n",
        "callback_b = EarlyStopping(monitor='val_loss', mode='min', patience=200, verbose=1)\n",
        "model = Sequential()\n",
        "model.add(Dense(1, input_dim = len(XTRAIN[0, :]), activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer = 'rmsprop', metrics=['accuracy'])\n",
        "history = model.fit(XTRAIN, YTRAIN, validation_data=(XVALID, YVALID), epochs=1056, batch_size=4, callbacks = [callback_a, callback_b])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1056\n",
            "811/821 [============================>.] - ETA: 0s - loss: 0.7349 - accuracy: 0.6107\n",
            "Epoch 00001: val_loss improved from inf to 0.51716, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.7320 - accuracy: 0.6126 - val_loss: 0.5172 - val_accuracy: 0.7681\n",
            "Epoch 2/1056\n",
            "799/821 [============================>.] - ETA: 0s - loss: 0.4489 - accuracy: 0.8176\n",
            "Epoch 00002: val_loss improved from 0.51716 to 0.37230, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4531 - accuracy: 0.8190 - val_loss: 0.3723 - val_accuracy: 0.8570\n",
            "Epoch 3/1056\n",
            "802/821 [============================>.] - ETA: 0s - loss: 0.3564 - accuracy: 0.8625\n",
            "Epoch 00003: val_loss improved from 0.37230 to 0.31134, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.3575 - accuracy: 0.8622 - val_loss: 0.3113 - val_accuracy: 0.8727\n",
            "Epoch 4/1056\n",
            "801/821 [============================>.] - ETA: 0s - loss: 0.3092 - accuracy: 0.8730\n",
            "Epoch 00004: val_loss improved from 0.31134 to 0.27389, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.3107 - accuracy: 0.8744 - val_loss: 0.2739 - val_accuracy: 0.8862\n",
            "Epoch 5/1056\n",
            "817/821 [============================>.] - ETA: 0s - loss: 0.2815 - accuracy: 0.8825\n",
            "Epoch 00005: val_loss improved from 0.27389 to 0.24810, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.2814 - accuracy: 0.8830 - val_loss: 0.2481 - val_accuracy: 0.9004\n",
            "Epoch 6/1056\n",
            "777/821 [===========================>..] - ETA: 0s - loss: 0.2590 - accuracy: 0.8896\n",
            "Epoch 00006: val_loss improved from 0.24810 to 0.22907, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.2591 - accuracy: 0.8897 - val_loss: 0.2291 - val_accuracy: 0.9097\n",
            "Epoch 7/1056\n",
            "780/821 [===========================>..] - ETA: 0s - loss: 0.2427 - accuracy: 0.8971\n",
            "Epoch 00007: val_loss improved from 0.22907 to 0.21320, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.2416 - accuracy: 0.8985 - val_loss: 0.2132 - val_accuracy: 0.9239\n",
            "Epoch 8/1056\n",
            "780/821 [===========================>..] - ETA: 0s - loss: 0.2277 - accuracy: 0.9038\n",
            "Epoch 00008: val_loss improved from 0.21320 to 0.20190, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.2279 - accuracy: 0.9037 - val_loss: 0.2019 - val_accuracy: 0.9296\n",
            "Epoch 9/1056\n",
            "788/821 [===========================>..] - ETA: 0s - loss: 0.2158 - accuracy: 0.9109\n",
            "Epoch 00009: val_loss improved from 0.20190 to 0.19132, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.2157 - accuracy: 0.9110 - val_loss: 0.1913 - val_accuracy: 0.9303\n",
            "Epoch 10/1056\n",
            "786/821 [===========================>..] - ETA: 0s - loss: 0.2072 - accuracy: 0.9125\n",
            "Epoch 00010: val_loss improved from 0.19132 to 0.18251, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.2065 - accuracy: 0.9141 - val_loss: 0.1825 - val_accuracy: 0.9353\n",
            "Epoch 11/1056\n",
            "770/821 [===========================>..] - ETA: 0s - loss: 0.1980 - accuracy: 0.9205\n",
            "Epoch 00011: val_loss improved from 0.18251 to 0.17516, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1982 - accuracy: 0.9211 - val_loss: 0.1752 - val_accuracy: 0.9388\n",
            "Epoch 12/1056\n",
            "814/821 [============================>.] - ETA: 0s - loss: 0.1916 - accuracy: 0.9238\n",
            "Epoch 00012: val_loss improved from 0.17516 to 0.16957, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1918 - accuracy: 0.9235 - val_loss: 0.1696 - val_accuracy: 0.9395\n",
            "Epoch 13/1056\n",
            "809/821 [============================>.] - ETA: 0s - loss: 0.1863 - accuracy: 0.9252\n",
            "Epoch 00013: val_loss improved from 0.16957 to 0.16387, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1860 - accuracy: 0.9259 - val_loss: 0.1639 - val_accuracy: 0.9395\n",
            "Epoch 14/1056\n",
            "815/821 [============================>.] - ETA: 0s - loss: 0.1808 - accuracy: 0.9264\n",
            "Epoch 00014: val_loss improved from 0.16387 to 0.15918, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1806 - accuracy: 0.9269 - val_loss: 0.1592 - val_accuracy: 0.9395\n",
            "Epoch 15/1056\n",
            "812/821 [============================>.] - ETA: 0s - loss: 0.1751 - accuracy: 0.9310\n",
            "Epoch 00015: val_loss improved from 0.15918 to 0.15341, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1763 - accuracy: 0.9302 - val_loss: 0.1534 - val_accuracy: 0.9438\n",
            "Epoch 16/1056\n",
            "789/821 [===========================>..] - ETA: 0s - loss: 0.1713 - accuracy: 0.9303\n",
            "Epoch 00016: val_loss improved from 0.15341 to 0.14971, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1723 - accuracy: 0.9293 - val_loss: 0.1497 - val_accuracy: 0.9431\n",
            "Epoch 17/1056\n",
            "777/821 [===========================>..] - ETA: 0s - loss: 0.1701 - accuracy: 0.9305\n",
            "Epoch 00017: val_loss improved from 0.14971 to 0.14540, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1685 - accuracy: 0.9308 - val_loss: 0.1454 - val_accuracy: 0.9481\n",
            "Epoch 18/1056\n",
            "771/821 [===========================>..] - ETA: 0s - loss: 0.1642 - accuracy: 0.9342\n",
            "Epoch 00018: val_loss improved from 0.14540 to 0.14202, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1649 - accuracy: 0.9333 - val_loss: 0.1420 - val_accuracy: 0.9495\n",
            "Epoch 19/1056\n",
            "798/821 [============================>.] - ETA: 0s - loss: 0.1615 - accuracy: 0.9364\n",
            "Epoch 00019: val_loss improved from 0.14202 to 0.13942, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1619 - accuracy: 0.9369 - val_loss: 0.1394 - val_accuracy: 0.9474\n",
            "Epoch 20/1056\n",
            "782/821 [===========================>..] - ETA: 0s - loss: 0.1584 - accuracy: 0.9383\n",
            "Epoch 00020: val_loss improved from 0.13942 to 0.13616, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1590 - accuracy: 0.9372 - val_loss: 0.1362 - val_accuracy: 0.9509\n",
            "Epoch 21/1056\n",
            "793/821 [===========================>..] - ETA: 0s - loss: 0.1569 - accuracy: 0.9373\n",
            "Epoch 00021: val_loss improved from 0.13616 to 0.13394, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1567 - accuracy: 0.9366 - val_loss: 0.1339 - val_accuracy: 0.9509\n",
            "Epoch 22/1056\n",
            "819/821 [============================>.] - ETA: 0s - loss: 0.1542 - accuracy: 0.9380\n",
            "Epoch 00022: val_loss improved from 0.13394 to 0.13144, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1542 - accuracy: 0.9381 - val_loss: 0.1314 - val_accuracy: 0.9531\n",
            "Epoch 23/1056\n",
            "807/821 [============================>.] - ETA: 0s - loss: 0.1510 - accuracy: 0.9377\n",
            "Epoch 00023: val_loss improved from 0.13144 to 0.12930, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1522 - accuracy: 0.9369 - val_loss: 0.1293 - val_accuracy: 0.9545\n",
            "Epoch 24/1056\n",
            "808/821 [============================>.] - ETA: 0s - loss: 0.1510 - accuracy: 0.9397\n",
            "Epoch 00024: val_loss improved from 0.12930 to 0.12768, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1504 - accuracy: 0.9397 - val_loss: 0.1277 - val_accuracy: 0.9545\n",
            "Epoch 25/1056\n",
            "810/821 [============================>.] - ETA: 0s - loss: 0.1478 - accuracy: 0.9414\n",
            "Epoch 00025: val_loss improved from 0.12768 to 0.12595, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1485 - accuracy: 0.9406 - val_loss: 0.1259 - val_accuracy: 0.9566\n",
            "Epoch 26/1056\n",
            "800/821 [============================>.] - ETA: 0s - loss: 0.1481 - accuracy: 0.9400\n",
            "Epoch 00026: val_loss improved from 0.12595 to 0.12432, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1467 - accuracy: 0.9403 - val_loss: 0.1243 - val_accuracy: 0.9545\n",
            "Epoch 27/1056\n",
            "806/821 [============================>.] - ETA: 0s - loss: 0.1446 - accuracy: 0.9401\n",
            "Epoch 00027: val_loss improved from 0.12432 to 0.12238, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1452 - accuracy: 0.9393 - val_loss: 0.1224 - val_accuracy: 0.9566\n",
            "Epoch 28/1056\n",
            "775/821 [===========================>..] - ETA: 0s - loss: 0.1433 - accuracy: 0.9426\n",
            "Epoch 00028: val_loss improved from 0.12238 to 0.12080, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1436 - accuracy: 0.9418 - val_loss: 0.1208 - val_accuracy: 0.9602\n",
            "Epoch 29/1056\n",
            "819/821 [============================>.] - ETA: 0s - loss: 0.1421 - accuracy: 0.9444\n",
            "Epoch 00029: val_loss improved from 0.12080 to 0.11913, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1422 - accuracy: 0.9442 - val_loss: 0.1191 - val_accuracy: 0.9602\n",
            "Epoch 30/1056\n",
            "816/821 [============================>.] - ETA: 0s - loss: 0.1411 - accuracy: 0.9427\n",
            "Epoch 00030: val_loss improved from 0.11913 to 0.11862, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1411 - accuracy: 0.9427 - val_loss: 0.1186 - val_accuracy: 0.9552\n",
            "Epoch 31/1056\n",
            "806/821 [============================>.] - ETA: 0s - loss: 0.1397 - accuracy: 0.9435\n",
            "Epoch 00031: val_loss improved from 0.11862 to 0.11656, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1399 - accuracy: 0.9430 - val_loss: 0.1166 - val_accuracy: 0.9580\n",
            "Epoch 32/1056\n",
            "801/821 [============================>.] - ETA: 0s - loss: 0.1373 - accuracy: 0.9438\n",
            "Epoch 00032: val_loss improved from 0.11656 to 0.11549, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1387 - accuracy: 0.9424 - val_loss: 0.1155 - val_accuracy: 0.9580\n",
            "Epoch 33/1056\n",
            "804/821 [============================>.] - ETA: 0s - loss: 0.1391 - accuracy: 0.9431\n",
            "Epoch 00033: val_loss improved from 0.11549 to 0.11426, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1376 - accuracy: 0.9439 - val_loss: 0.1143 - val_accuracy: 0.9587\n",
            "Epoch 34/1056\n",
            "811/821 [============================>.] - ETA: 0s - loss: 0.1364 - accuracy: 0.9445\n",
            "Epoch 00034: val_loss improved from 0.11426 to 0.11275, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1365 - accuracy: 0.9448 - val_loss: 0.1128 - val_accuracy: 0.9630\n",
            "Epoch 35/1056\n",
            "812/821 [============================>.] - ETA: 0s - loss: 0.1348 - accuracy: 0.9458\n",
            "Epoch 00035: val_loss improved from 0.11275 to 0.11207, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1353 - accuracy: 0.9454 - val_loss: 0.1121 - val_accuracy: 0.9616\n",
            "Epoch 36/1056\n",
            "803/821 [============================>.] - ETA: 0s - loss: 0.1353 - accuracy: 0.9461\n",
            "Epoch 00036: val_loss improved from 0.11207 to 0.11050, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1346 - accuracy: 0.9467 - val_loss: 0.1105 - val_accuracy: 0.9623\n",
            "Epoch 37/1056\n",
            "801/821 [============================>.] - ETA: 0s - loss: 0.1336 - accuracy: 0.9476\n",
            "Epoch 00037: val_loss improved from 0.11050 to 0.10863, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1337 - accuracy: 0.9473 - val_loss: 0.1086 - val_accuracy: 0.9637\n",
            "Epoch 38/1056\n",
            "809/821 [============================>.] - ETA: 0s - loss: 0.1326 - accuracy: 0.9453\n",
            "Epoch 00038: val_loss improved from 0.10863 to 0.10827, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1331 - accuracy: 0.9448 - val_loss: 0.1083 - val_accuracy: 0.9630\n",
            "Epoch 39/1056\n",
            "815/821 [============================>.] - ETA: 0s - loss: 0.1327 - accuracy: 0.9439\n",
            "Epoch 00039: val_loss improved from 0.10827 to 0.10782, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1322 - accuracy: 0.9442 - val_loss: 0.1078 - val_accuracy: 0.9637\n",
            "Epoch 40/1056\n",
            "812/821 [============================>.] - ETA: 0s - loss: 0.1314 - accuracy: 0.9474\n",
            "Epoch 00040: val_loss improved from 0.10782 to 0.10667, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1313 - accuracy: 0.9473 - val_loss: 0.1067 - val_accuracy: 0.9644\n",
            "Epoch 41/1056\n",
            "800/821 [============================>.] - ETA: 0s - loss: 0.1306 - accuracy: 0.9469\n",
            "Epoch 00041: val_loss improved from 0.10667 to 0.10562, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1308 - accuracy: 0.9467 - val_loss: 0.1056 - val_accuracy: 0.9637\n",
            "Epoch 42/1056\n",
            "810/821 [============================>.] - ETA: 0s - loss: 0.1305 - accuracy: 0.9460\n",
            "Epoch 00042: val_loss improved from 0.10562 to 0.10453, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1299 - accuracy: 0.9464 - val_loss: 0.1045 - val_accuracy: 0.9637\n",
            "Epoch 43/1056\n",
            "811/821 [============================>.] - ETA: 0s - loss: 0.1293 - accuracy: 0.9467\n",
            "Epoch 00043: val_loss improved from 0.10453 to 0.10354, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1294 - accuracy: 0.9467 - val_loss: 0.1035 - val_accuracy: 0.9651\n",
            "Epoch 44/1056\n",
            "806/821 [============================>.] - ETA: 0s - loss: 0.1291 - accuracy: 0.9479\n",
            "Epoch 00044: val_loss improved from 0.10354 to 0.10254, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1288 - accuracy: 0.9485 - val_loss: 0.1025 - val_accuracy: 0.9630\n",
            "Epoch 45/1056\n",
            "814/821 [============================>.] - ETA: 0s - loss: 0.1289 - accuracy: 0.9456\n",
            "Epoch 00045: val_loss improved from 0.10254 to 0.10209, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1283 - accuracy: 0.9461 - val_loss: 0.1021 - val_accuracy: 0.9637\n",
            "Epoch 46/1056\n",
            "797/821 [============================>.] - ETA: 0s - loss: 0.1261 - accuracy: 0.9467\n",
            "Epoch 00046: val_loss improved from 0.10209 to 0.10096, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.1276 - accuracy: 0.9470 - val_loss: 0.1010 - val_accuracy: 0.9637\n",
            "Epoch 47/1056\n",
            "810/821 [============================>.] - ETA: 0s - loss: 0.1271 - accuracy: 0.9481\n",
            "Epoch 00047: val_loss improved from 0.10096 to 0.10056, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 2s 2ms/step - loss: 0.1271 - accuracy: 0.9479 - val_loss: 0.1006 - val_accuracy: 0.9630\n",
            "Epoch 48/1056\n",
            "799/821 [============================>.] - ETA: 0s - loss: 0.1275 - accuracy: 0.9471\n",
            "Epoch 00048: val_loss improved from 0.10056 to 0.09977, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 2s 2ms/step - loss: 0.1264 - accuracy: 0.9479 - val_loss: 0.0998 - val_accuracy: 0.9651\n",
            "Epoch 49/1056\n",
            "789/821 [===========================>..] - ETA: 0s - loss: 0.1258 - accuracy: 0.9487\n",
            "Epoch 00049: val_loss improved from 0.09977 to 0.09912, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 2s 2ms/step - loss: 0.1263 - accuracy: 0.9482 - val_loss: 0.0991 - val_accuracy: 0.9644\n",
            "Epoch 50/1056\n",
            "797/821 [============================>.] - ETA: 0s - loss: 0.1231 - accuracy: 0.9489\n",
            "Epoch 00050: val_loss improved from 0.09912 to 0.09877, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 2s 2ms/step - loss: 0.1256 - accuracy: 0.9476 - val_loss: 0.0988 - val_accuracy: 0.9644\n",
            "Epoch 51/1056\n",
            "796/821 [============================>.] - ETA: 0s - loss: 0.1262 - accuracy: 0.9476\n",
            "Epoch 00051: val_loss improved from 0.09877 to 0.09800, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 2s 2ms/step - loss: 0.1251 - accuracy: 0.9482 - val_loss: 0.0980 - val_accuracy: 0.9644\n",
            "Epoch 52/1056\n",
            "796/821 [============================>.] - ETA: 0s - loss: 0.1260 - accuracy: 0.9472\n",
            "Epoch 00052: val_loss improved from 0.09800 to 0.09698, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.1247 - accuracy: 0.9479 - val_loss: 0.0970 - val_accuracy: 0.9651\n",
            "Epoch 53/1056\n",
            "806/821 [============================>.] - ETA: 0s - loss: 0.1249 - accuracy: 0.9473\n",
            "Epoch 00053: val_loss improved from 0.09698 to 0.09666, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1243 - accuracy: 0.9476 - val_loss: 0.0967 - val_accuracy: 0.9651\n",
            "Epoch 54/1056\n",
            "819/821 [============================>.] - ETA: 0s - loss: 0.1237 - accuracy: 0.9481\n",
            "Epoch 00054: val_loss improved from 0.09666 to 0.09630, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1238 - accuracy: 0.9482 - val_loss: 0.0963 - val_accuracy: 0.9644\n",
            "Epoch 55/1056\n",
            "815/821 [============================>.] - ETA: 0s - loss: 0.1230 - accuracy: 0.9488\n",
            "Epoch 00055: val_loss improved from 0.09630 to 0.09590, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1235 - accuracy: 0.9485 - val_loss: 0.0959 - val_accuracy: 0.9644\n",
            "Epoch 56/1056\n",
            "795/821 [============================>.] - ETA: 0s - loss: 0.1244 - accuracy: 0.9481\n",
            "Epoch 00056: val_loss improved from 0.09590 to 0.09540, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1232 - accuracy: 0.9491 - val_loss: 0.0954 - val_accuracy: 0.9644\n",
            "Epoch 57/1056\n",
            "817/821 [============================>.] - ETA: 0s - loss: 0.1232 - accuracy: 0.9492\n",
            "Epoch 00057: val_loss improved from 0.09540 to 0.09506, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1230 - accuracy: 0.9494 - val_loss: 0.0951 - val_accuracy: 0.9644\n",
            "Epoch 58/1056\n",
            "807/821 [============================>.] - ETA: 0s - loss: 0.1235 - accuracy: 0.9489\n",
            "Epoch 00058: val_loss improved from 0.09506 to 0.09461, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1226 - accuracy: 0.9494 - val_loss: 0.0946 - val_accuracy: 0.9644\n",
            "Epoch 59/1056\n",
            "812/821 [============================>.] - ETA: 0s - loss: 0.1225 - accuracy: 0.9495\n",
            "Epoch 00059: val_loss improved from 0.09461 to 0.09424, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1222 - accuracy: 0.9497 - val_loss: 0.0942 - val_accuracy: 0.9637\n",
            "Epoch 60/1056\n",
            "785/821 [===========================>..] - ETA: 0s - loss: 0.1219 - accuracy: 0.9494\n",
            "Epoch 00060: val_loss improved from 0.09424 to 0.09392, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1219 - accuracy: 0.9491 - val_loss: 0.0939 - val_accuracy: 0.9644\n",
            "Epoch 61/1056\n",
            "785/821 [===========================>..] - ETA: 0s - loss: 0.1182 - accuracy: 0.9510\n",
            "Epoch 00061: val_loss improved from 0.09392 to 0.09285, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1215 - accuracy: 0.9488 - val_loss: 0.0929 - val_accuracy: 0.9644\n",
            "Epoch 62/1056\n",
            "804/821 [============================>.] - ETA: 0s - loss: 0.1207 - accuracy: 0.9502\n",
            "Epoch 00062: val_loss improved from 0.09285 to 0.09242, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1212 - accuracy: 0.9497 - val_loss: 0.0924 - val_accuracy: 0.9651\n",
            "Epoch 63/1056\n",
            "783/821 [===========================>..] - ETA: 0s - loss: 0.1213 - accuracy: 0.9486\n",
            "Epoch 00063: val_loss improved from 0.09242 to 0.09202, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1210 - accuracy: 0.9491 - val_loss: 0.0920 - val_accuracy: 0.9637\n",
            "Epoch 64/1056\n",
            "788/821 [===========================>..] - ETA: 0s - loss: 0.1198 - accuracy: 0.9505\n",
            "Epoch 00064: val_loss did not improve from 0.09202\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1208 - accuracy: 0.9497 - val_loss: 0.0921 - val_accuracy: 0.9644\n",
            "Epoch 65/1056\n",
            "789/821 [===========================>..] - ETA: 0s - loss: 0.1214 - accuracy: 0.9509\n",
            "Epoch 00065: val_loss improved from 0.09202 to 0.09112, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1205 - accuracy: 0.9512 - val_loss: 0.0911 - val_accuracy: 0.9644\n",
            "Epoch 66/1056\n",
            "791/821 [===========================>..] - ETA: 0s - loss: 0.1208 - accuracy: 0.9501\n",
            "Epoch 00066: val_loss did not improve from 0.09112\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1203 - accuracy: 0.9500 - val_loss: 0.0913 - val_accuracy: 0.9651\n",
            "Epoch 67/1056\n",
            "794/821 [============================>.] - ETA: 0s - loss: 0.1210 - accuracy: 0.9503\n",
            "Epoch 00067: val_loss improved from 0.09112 to 0.09049, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1200 - accuracy: 0.9512 - val_loss: 0.0905 - val_accuracy: 0.9630\n",
            "Epoch 68/1056\n",
            "798/821 [============================>.] - ETA: 0s - loss: 0.1202 - accuracy: 0.9511\n",
            "Epoch 00068: val_loss improved from 0.09049 to 0.09017, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1198 - accuracy: 0.9512 - val_loss: 0.0902 - val_accuracy: 0.9651\n",
            "Epoch 69/1056\n",
            "784/821 [===========================>..] - ETA: 0s - loss: 0.1203 - accuracy: 0.9509\n",
            "Epoch 00069: val_loss improved from 0.09017 to 0.09002, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1198 - accuracy: 0.9515 - val_loss: 0.0900 - val_accuracy: 0.9644\n",
            "Epoch 70/1056\n",
            "774/821 [===========================>..] - ETA: 0s - loss: 0.1183 - accuracy: 0.9519\n",
            "Epoch 00070: val_loss improved from 0.09002 to 0.08942, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1195 - accuracy: 0.9506 - val_loss: 0.0894 - val_accuracy: 0.9637\n",
            "Epoch 71/1056\n",
            "783/821 [===========================>..] - ETA: 0s - loss: 0.1167 - accuracy: 0.9505\n",
            "Epoch 00071: val_loss improved from 0.08942 to 0.08865, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1194 - accuracy: 0.9512 - val_loss: 0.0887 - val_accuracy: 0.9666\n",
            "Epoch 72/1056\n",
            "798/821 [============================>.] - ETA: 0s - loss: 0.1191 - accuracy: 0.9502\n",
            "Epoch 00072: val_loss did not improve from 0.08865\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1190 - accuracy: 0.9500 - val_loss: 0.0888 - val_accuracy: 0.9659\n",
            "Epoch 73/1056\n",
            "807/821 [============================>.] - ETA: 0s - loss: 0.1198 - accuracy: 0.9514\n",
            "Epoch 00073: val_loss improved from 0.08865 to 0.08832, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1190 - accuracy: 0.9518 - val_loss: 0.0883 - val_accuracy: 0.9644\n",
            "Epoch 74/1056\n",
            "783/821 [===========================>..] - ETA: 0s - loss: 0.1197 - accuracy: 0.9518\n",
            "Epoch 00074: val_loss improved from 0.08832 to 0.08754, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1188 - accuracy: 0.9521 - val_loss: 0.0875 - val_accuracy: 0.9651\n",
            "Epoch 75/1056\n",
            "799/821 [============================>.] - ETA: 0s - loss: 0.1183 - accuracy: 0.9512\n",
            "Epoch 00075: val_loss did not improve from 0.08754\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1186 - accuracy: 0.9506 - val_loss: 0.0881 - val_accuracy: 0.9659\n",
            "Epoch 76/1056\n",
            "785/821 [===========================>..] - ETA: 0s - loss: 0.1192 - accuracy: 0.9516\n",
            "Epoch 00076: val_loss improved from 0.08754 to 0.08731, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1183 - accuracy: 0.9521 - val_loss: 0.0873 - val_accuracy: 0.9651\n",
            "Epoch 77/1056\n",
            "784/821 [===========================>..] - ETA: 0s - loss: 0.1176 - accuracy: 0.9522\n",
            "Epoch 00077: val_loss improved from 0.08731 to 0.08706, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1183 - accuracy: 0.9521 - val_loss: 0.0871 - val_accuracy: 0.9666\n",
            "Epoch 78/1056\n",
            "783/821 [===========================>..] - ETA: 0s - loss: 0.1168 - accuracy: 0.9524\n",
            "Epoch 00078: val_loss improved from 0.08706 to 0.08664, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1181 - accuracy: 0.9512 - val_loss: 0.0866 - val_accuracy: 0.9659\n",
            "Epoch 79/1056\n",
            "809/821 [============================>.] - ETA: 0s - loss: 0.1187 - accuracy: 0.9509\n",
            "Epoch 00079: val_loss improved from 0.08664 to 0.08638, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1180 - accuracy: 0.9512 - val_loss: 0.0864 - val_accuracy: 0.9673\n",
            "Epoch 80/1056\n",
            "775/821 [===========================>..] - ETA: 0s - loss: 0.1191 - accuracy: 0.9506\n",
            "Epoch 00080: val_loss did not improve from 0.08638\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1177 - accuracy: 0.9512 - val_loss: 0.0869 - val_accuracy: 0.9651\n",
            "Epoch 81/1056\n",
            "789/821 [===========================>..] - ETA: 0s - loss: 0.1144 - accuracy: 0.9525\n",
            "Epoch 00081: val_loss improved from 0.08638 to 0.08597, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1177 - accuracy: 0.9528 - val_loss: 0.0860 - val_accuracy: 0.9666\n",
            "Epoch 82/1056\n",
            "789/821 [===========================>..] - ETA: 0s - loss: 0.1166 - accuracy: 0.9525\n",
            "Epoch 00082: val_loss did not improve from 0.08597\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1175 - accuracy: 0.9515 - val_loss: 0.0861 - val_accuracy: 0.9659\n",
            "Epoch 83/1056\n",
            "798/821 [============================>.] - ETA: 0s - loss: 0.1176 - accuracy: 0.9514\n",
            "Epoch 00083: val_loss did not improve from 0.08597\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1173 - accuracy: 0.9512 - val_loss: 0.0863 - val_accuracy: 0.9659\n",
            "Epoch 84/1056\n",
            "788/821 [===========================>..] - ETA: 0s - loss: 0.1190 - accuracy: 0.9502\n",
            "Epoch 00084: val_loss improved from 0.08597 to 0.08533, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1173 - accuracy: 0.9512 - val_loss: 0.0853 - val_accuracy: 0.9651\n",
            "Epoch 85/1056\n",
            "789/821 [===========================>..] - ETA: 0s - loss: 0.1178 - accuracy: 0.9509\n",
            "Epoch 00085: val_loss did not improve from 0.08533\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1173 - accuracy: 0.9515 - val_loss: 0.0855 - val_accuracy: 0.9637\n",
            "Epoch 86/1056\n",
            "788/821 [===========================>..] - ETA: 0s - loss: 0.1184 - accuracy: 0.9511\n",
            "Epoch 00086: val_loss improved from 0.08533 to 0.08495, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1170 - accuracy: 0.9515 - val_loss: 0.0850 - val_accuracy: 0.9680\n",
            "Epoch 87/1056\n",
            "790/821 [===========================>..] - ETA: 0s - loss: 0.1163 - accuracy: 0.9525\n",
            "Epoch 00087: val_loss improved from 0.08495 to 0.08472, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1170 - accuracy: 0.9521 - val_loss: 0.0847 - val_accuracy: 0.9644\n",
            "Epoch 88/1056\n",
            "802/821 [============================>.] - ETA: 0s - loss: 0.1174 - accuracy: 0.9511\n",
            "Epoch 00088: val_loss improved from 0.08472 to 0.08446, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1171 - accuracy: 0.9509 - val_loss: 0.0845 - val_accuracy: 0.9651\n",
            "Epoch 89/1056\n",
            "790/821 [===========================>..] - ETA: 0s - loss: 0.1184 - accuracy: 0.9519\n",
            "Epoch 00089: val_loss improved from 0.08446 to 0.08434, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1171 - accuracy: 0.9528 - val_loss: 0.0843 - val_accuracy: 0.9644\n",
            "Epoch 90/1056\n",
            "807/821 [============================>.] - ETA: 0s - loss: 0.1180 - accuracy: 0.9517\n",
            "Epoch 00090: val_loss improved from 0.08434 to 0.08401, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1168 - accuracy: 0.9525 - val_loss: 0.0840 - val_accuracy: 0.9659\n",
            "Epoch 91/1056\n",
            "785/821 [===========================>..] - ETA: 0s - loss: 0.1183 - accuracy: 0.9519\n",
            "Epoch 00091: val_loss improved from 0.08401 to 0.08386, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1167 - accuracy: 0.9528 - val_loss: 0.0839 - val_accuracy: 0.9651\n",
            "Epoch 92/1056\n",
            "780/821 [===========================>..] - ETA: 0s - loss: 0.1124 - accuracy: 0.9538\n",
            "Epoch 00092: val_loss improved from 0.08386 to 0.08326, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1166 - accuracy: 0.9537 - val_loss: 0.0833 - val_accuracy: 0.9680\n",
            "Epoch 93/1056\n",
            "790/821 [===========================>..] - ETA: 0s - loss: 0.1175 - accuracy: 0.9522\n",
            "Epoch 00093: val_loss improved from 0.08326 to 0.08246, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1165 - accuracy: 0.9531 - val_loss: 0.0825 - val_accuracy: 0.9651\n",
            "Epoch 94/1056\n",
            "795/821 [============================>.] - ETA: 0s - loss: 0.1185 - accuracy: 0.9509\n",
            "Epoch 00094: val_loss did not improve from 0.08246\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1167 - accuracy: 0.9521 - val_loss: 0.0826 - val_accuracy: 0.9666\n",
            "Epoch 95/1056\n",
            "784/821 [===========================>..] - ETA: 0s - loss: 0.1172 - accuracy: 0.9531\n",
            "Epoch 00095: val_loss improved from 0.08246 to 0.08233, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1166 - accuracy: 0.9528 - val_loss: 0.0823 - val_accuracy: 0.9659\n",
            "Epoch 96/1056\n",
            "784/821 [===========================>..] - ETA: 0s - loss: 0.1165 - accuracy: 0.9531\n",
            "Epoch 00096: val_loss did not improve from 0.08233\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1163 - accuracy: 0.9531 - val_loss: 0.0824 - val_accuracy: 0.9651\n",
            "Epoch 97/1056\n",
            "796/821 [============================>.] - ETA: 0s - loss: 0.1161 - accuracy: 0.9526\n",
            "Epoch 00097: val_loss did not improve from 0.08233\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1163 - accuracy: 0.9518 - val_loss: 0.0828 - val_accuracy: 0.9630\n",
            "Epoch 98/1056\n",
            "795/821 [============================>.] - ETA: 0s - loss: 0.1157 - accuracy: 0.9538\n",
            "Epoch 00098: val_loss did not improve from 0.08233\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1165 - accuracy: 0.9528 - val_loss: 0.0828 - val_accuracy: 0.9644\n",
            "Epoch 99/1056\n",
            "777/821 [===========================>..] - ETA: 0s - loss: 0.1096 - accuracy: 0.9527\n",
            "Epoch 00099: val_loss did not improve from 0.08233\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1164 - accuracy: 0.9518 - val_loss: 0.0825 - val_accuracy: 0.9644\n",
            "Epoch 100/1056\n",
            "801/821 [============================>.] - ETA: 0s - loss: 0.1165 - accuracy: 0.9532\n",
            "Epoch 00100: val_loss did not improve from 0.08233\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1161 - accuracy: 0.9528 - val_loss: 0.0825 - val_accuracy: 0.9644\n",
            "Epoch 101/1056\n",
            "803/821 [============================>.] - ETA: 0s - loss: 0.1170 - accuracy: 0.9521\n",
            "Epoch 00101: val_loss improved from 0.08233 to 0.08205, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1163 - accuracy: 0.9525 - val_loss: 0.0820 - val_accuracy: 0.9644\n",
            "Epoch 102/1056\n",
            "821/821 [==============================] - ETA: 0s - loss: 0.1161 - accuracy: 0.9525\n",
            "Epoch 00102: val_loss improved from 0.08205 to 0.08174, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1161 - accuracy: 0.9525 - val_loss: 0.0817 - val_accuracy: 0.9659\n",
            "Epoch 103/1056\n",
            "788/821 [===========================>..] - ETA: 0s - loss: 0.1172 - accuracy: 0.9530\n",
            "Epoch 00103: val_loss did not improve from 0.08174\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1162 - accuracy: 0.9525 - val_loss: 0.0821 - val_accuracy: 0.9644\n",
            "Epoch 104/1056\n",
            "820/821 [============================>.] - ETA: 0s - loss: 0.1157 - accuracy: 0.9527\n",
            "Epoch 00104: val_loss did not improve from 0.08174\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1160 - accuracy: 0.9525 - val_loss: 0.0819 - val_accuracy: 0.9651\n",
            "Epoch 105/1056\n",
            "790/821 [===========================>..] - ETA: 0s - loss: 0.1155 - accuracy: 0.9528\n",
            "Epoch 00105: val_loss improved from 0.08174 to 0.08139, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1161 - accuracy: 0.9521 - val_loss: 0.0814 - val_accuracy: 0.9673\n",
            "Epoch 106/1056\n",
            "809/821 [============================>.] - ETA: 0s - loss: 0.1152 - accuracy: 0.9527\n",
            "Epoch 00106: val_loss improved from 0.08139 to 0.08075, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1159 - accuracy: 0.9525 - val_loss: 0.0807 - val_accuracy: 0.9659\n",
            "Epoch 107/1056\n",
            "800/821 [============================>.] - ETA: 0s - loss: 0.1160 - accuracy: 0.9531\n",
            "Epoch 00107: val_loss improved from 0.08075 to 0.08068, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1160 - accuracy: 0.9528 - val_loss: 0.0807 - val_accuracy: 0.9659\n",
            "Epoch 108/1056\n",
            "821/821 [==============================] - ETA: 0s - loss: 0.1160 - accuracy: 0.9518\n",
            "Epoch 00108: val_loss did not improve from 0.08068\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1160 - accuracy: 0.9518 - val_loss: 0.0810 - val_accuracy: 0.9651\n",
            "Epoch 109/1056\n",
            "783/821 [===========================>..] - ETA: 0s - loss: 0.1173 - accuracy: 0.9511\n",
            "Epoch 00109: val_loss did not improve from 0.08068\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1160 - accuracy: 0.9518 - val_loss: 0.0807 - val_accuracy: 0.9666\n",
            "Epoch 110/1056\n",
            "802/821 [============================>.] - ETA: 0s - loss: 0.1161 - accuracy: 0.9517\n",
            "Epoch 00110: val_loss improved from 0.08068 to 0.08054, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1160 - accuracy: 0.9515 - val_loss: 0.0805 - val_accuracy: 0.9666\n",
            "Epoch 111/1056\n",
            "798/821 [============================>.] - ETA: 0s - loss: 0.1154 - accuracy: 0.9521\n",
            "Epoch 00111: val_loss did not improve from 0.08054\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1161 - accuracy: 0.9518 - val_loss: 0.0809 - val_accuracy: 0.9659\n",
            "Epoch 112/1056\n",
            "784/821 [===========================>..] - ETA: 0s - loss: 0.1164 - accuracy: 0.9522\n",
            "Epoch 00112: val_loss did not improve from 0.08054\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1159 - accuracy: 0.9521 - val_loss: 0.0808 - val_accuracy: 0.9651\n",
            "Epoch 113/1056\n",
            "788/821 [===========================>..] - ETA: 0s - loss: 0.1175 - accuracy: 0.9511\n",
            "Epoch 00113: val_loss did not improve from 0.08054\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1158 - accuracy: 0.9518 - val_loss: 0.0806 - val_accuracy: 0.9651\n",
            "Epoch 114/1056\n",
            "794/821 [============================>.] - ETA: 0s - loss: 0.1165 - accuracy: 0.9521\n",
            "Epoch 00114: val_loss improved from 0.08054 to 0.08010, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1160 - accuracy: 0.9521 - val_loss: 0.0801 - val_accuracy: 0.9659\n",
            "Epoch 115/1056\n",
            "804/821 [============================>.] - ETA: 0s - loss: 0.1107 - accuracy: 0.9521\n",
            "Epoch 00115: val_loss improved from 0.08010 to 0.07983, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1160 - accuracy: 0.9521 - val_loss: 0.0798 - val_accuracy: 0.9659\n",
            "Epoch 116/1056\n",
            "777/821 [===========================>..] - ETA: 0s - loss: 0.1169 - accuracy: 0.9517\n",
            "Epoch 00116: val_loss did not improve from 0.07983\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1160 - accuracy: 0.9525 - val_loss: 0.0800 - val_accuracy: 0.9651\n",
            "Epoch 117/1056\n",
            "784/821 [===========================>..] - ETA: 0s - loss: 0.1176 - accuracy: 0.9506\n",
            "Epoch 00117: val_loss improved from 0.07983 to 0.07946, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1159 - accuracy: 0.9515 - val_loss: 0.0795 - val_accuracy: 0.9644\n",
            "Epoch 118/1056\n",
            "801/821 [============================>.] - ETA: 0s - loss: 0.1167 - accuracy: 0.9513\n",
            "Epoch 00118: val_loss did not improve from 0.07946\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1161 - accuracy: 0.9515 - val_loss: 0.0795 - val_accuracy: 0.9637\n",
            "Epoch 119/1056\n",
            "795/821 [============================>.] - ETA: 0s - loss: 0.1165 - accuracy: 0.9513\n",
            "Epoch 00119: val_loss did not improve from 0.07946\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1160 - accuracy: 0.9515 - val_loss: 0.0799 - val_accuracy: 0.9630\n",
            "Epoch 120/1056\n",
            "803/821 [============================>.] - ETA: 0s - loss: 0.1176 - accuracy: 0.9517\n",
            "Epoch 00120: val_loss did not improve from 0.07946\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1159 - accuracy: 0.9525 - val_loss: 0.0806 - val_accuracy: 0.9630\n",
            "Epoch 121/1056\n",
            "782/821 [===========================>..] - ETA: 0s - loss: 0.1121 - accuracy: 0.9511\n",
            "Epoch 00121: val_loss did not improve from 0.07946\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1165 - accuracy: 0.9521 - val_loss: 0.0805 - val_accuracy: 0.9637\n",
            "Epoch 122/1056\n",
            "820/821 [============================>.] - ETA: 0s - loss: 0.1160 - accuracy: 0.9527\n",
            "Epoch 00122: val_loss did not improve from 0.07946\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1160 - accuracy: 0.9528 - val_loss: 0.0798 - val_accuracy: 0.9637\n",
            "Epoch 123/1056\n",
            "772/821 [===========================>..] - ETA: 0s - loss: 0.1191 - accuracy: 0.9505\n",
            "Epoch 00123: val_loss improved from 0.07946 to 0.07940, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1160 - accuracy: 0.9521 - val_loss: 0.0794 - val_accuracy: 0.9644\n",
            "Epoch 124/1056\n",
            "802/821 [============================>.] - ETA: 0s - loss: 0.1160 - accuracy: 0.9520\n",
            "Epoch 00124: val_loss improved from 0.07940 to 0.07896, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1161 - accuracy: 0.9518 - val_loss: 0.0790 - val_accuracy: 0.9651\n",
            "Epoch 125/1056\n",
            "786/821 [===========================>..] - ETA: 0s - loss: 0.1158 - accuracy: 0.9517\n",
            "Epoch 00125: val_loss did not improve from 0.07896\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1161 - accuracy: 0.9518 - val_loss: 0.0791 - val_accuracy: 0.9637\n",
            "Epoch 126/1056\n",
            "802/821 [============================>.] - ETA: 0s - loss: 0.1170 - accuracy: 0.9504\n",
            "Epoch 00126: val_loss improved from 0.07896 to 0.07868, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1159 - accuracy: 0.9509 - val_loss: 0.0787 - val_accuracy: 0.9637\n",
            "Epoch 127/1056\n",
            "821/821 [==============================] - ETA: 0s - loss: 0.1161 - accuracy: 0.9521\n",
            "Epoch 00127: val_loss improved from 0.07868 to 0.07852, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1161 - accuracy: 0.9521 - val_loss: 0.0785 - val_accuracy: 0.9644\n",
            "Epoch 128/1056\n",
            "788/821 [===========================>..] - ETA: 0s - loss: 0.1167 - accuracy: 0.9511\n",
            "Epoch 00128: val_loss improved from 0.07852 to 0.07842, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1161 - accuracy: 0.9512 - val_loss: 0.0784 - val_accuracy: 0.9651\n",
            "Epoch 129/1056\n",
            "773/821 [===========================>..] - ETA: 0s - loss: 0.1186 - accuracy: 0.9492\n",
            "Epoch 00129: val_loss did not improve from 0.07842\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1160 - accuracy: 0.9506 - val_loss: 0.0789 - val_accuracy: 0.9651\n",
            "Epoch 130/1056\n",
            "786/821 [===========================>..] - ETA: 0s - loss: 0.1131 - accuracy: 0.9539\n",
            "Epoch 00130: val_loss did not improve from 0.07842\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1160 - accuracy: 0.9515 - val_loss: 0.0790 - val_accuracy: 0.9659\n",
            "Epoch 131/1056\n",
            "788/821 [===========================>..] - ETA: 0s - loss: 0.1156 - accuracy: 0.9521\n",
            "Epoch 00131: val_loss did not improve from 0.07842\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1161 - accuracy: 0.9512 - val_loss: 0.0785 - val_accuracy: 0.9659\n",
            "Epoch 132/1056\n",
            "787/821 [===========================>..] - ETA: 0s - loss: 0.1162 - accuracy: 0.9524\n",
            "Epoch 00132: val_loss improved from 0.07842 to 0.07831, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1161 - accuracy: 0.9515 - val_loss: 0.0783 - val_accuracy: 0.9644\n",
            "Epoch 133/1056\n",
            "784/821 [===========================>..] - ETA: 0s - loss: 0.1181 - accuracy: 0.9512\n",
            "Epoch 00133: val_loss improved from 0.07831 to 0.07821, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1162 - accuracy: 0.9518 - val_loss: 0.0782 - val_accuracy: 0.9651\n",
            "Epoch 134/1056\n",
            "794/821 [============================>.] - ETA: 0s - loss: 0.1148 - accuracy: 0.9531\n",
            "Epoch 00134: val_loss improved from 0.07821 to 0.07809, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1160 - accuracy: 0.9521 - val_loss: 0.0781 - val_accuracy: 0.9637\n",
            "Epoch 135/1056\n",
            "784/821 [===========================>..] - ETA: 0s - loss: 0.1179 - accuracy: 0.9525\n",
            "Epoch 00135: val_loss improved from 0.07809 to 0.07761, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1161 - accuracy: 0.9531 - val_loss: 0.0776 - val_accuracy: 0.9651\n",
            "Epoch 136/1056\n",
            "779/821 [===========================>..] - ETA: 0s - loss: 0.1168 - accuracy: 0.9519\n",
            "Epoch 00136: val_loss did not improve from 0.07761\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1163 - accuracy: 0.9515 - val_loss: 0.0778 - val_accuracy: 0.9644\n",
            "Epoch 137/1056\n",
            "791/821 [===========================>..] - ETA: 0s - loss: 0.1169 - accuracy: 0.9529\n",
            "Epoch 00137: val_loss improved from 0.07761 to 0.07744, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1163 - accuracy: 0.9525 - val_loss: 0.0774 - val_accuracy: 0.9637\n",
            "Epoch 138/1056\n",
            "793/821 [===========================>..] - ETA: 0s - loss: 0.1097 - accuracy: 0.9518\n",
            "Epoch 00138: val_loss did not improve from 0.07744\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1164 - accuracy: 0.9515 - val_loss: 0.0779 - val_accuracy: 0.9651\n",
            "Epoch 139/1056\n",
            "797/821 [============================>.] - ETA: 0s - loss: 0.1173 - accuracy: 0.9508\n",
            "Epoch 00139: val_loss did not improve from 0.07744\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1163 - accuracy: 0.9512 - val_loss: 0.0776 - val_accuracy: 0.9644\n",
            "Epoch 140/1056\n",
            "781/821 [===========================>..] - ETA: 0s - loss: 0.1168 - accuracy: 0.9507\n",
            "Epoch 00140: val_loss did not improve from 0.07744\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1163 - accuracy: 0.9509 - val_loss: 0.0784 - val_accuracy: 0.9644\n",
            "Epoch 141/1056\n",
            "784/821 [===========================>..] - ETA: 0s - loss: 0.1171 - accuracy: 0.9506\n",
            "Epoch 00141: val_loss did not improve from 0.07744\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1164 - accuracy: 0.9512 - val_loss: 0.0778 - val_accuracy: 0.9651\n",
            "Epoch 142/1056\n",
            "773/821 [===========================>..] - ETA: 0s - loss: 0.1167 - accuracy: 0.9515\n",
            "Epoch 00142: val_loss improved from 0.07744 to 0.07734, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1161 - accuracy: 0.9512 - val_loss: 0.0773 - val_accuracy: 0.9651\n",
            "Epoch 143/1056\n",
            "794/821 [============================>.] - ETA: 0s - loss: 0.1173 - accuracy: 0.9496\n",
            "Epoch 00143: val_loss did not improve from 0.07734\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1163 - accuracy: 0.9506 - val_loss: 0.0778 - val_accuracy: 0.9651\n",
            "Epoch 144/1056\n",
            "796/821 [============================>.] - ETA: 0s - loss: 0.1177 - accuracy: 0.9513\n",
            "Epoch 00144: val_loss did not improve from 0.07734\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1163 - accuracy: 0.9512 - val_loss: 0.0774 - val_accuracy: 0.9651\n",
            "Epoch 145/1056\n",
            "801/821 [============================>.] - ETA: 0s - loss: 0.1177 - accuracy: 0.9516\n",
            "Epoch 00145: val_loss improved from 0.07734 to 0.07707, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1164 - accuracy: 0.9518 - val_loss: 0.0771 - val_accuracy: 0.9644\n",
            "Epoch 146/1056\n",
            "779/821 [===========================>..] - ETA: 0s - loss: 0.1168 - accuracy: 0.9512\n",
            "Epoch 00146: val_loss improved from 0.07707 to 0.07698, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1164 - accuracy: 0.9509 - val_loss: 0.0770 - val_accuracy: 0.9644\n",
            "Epoch 147/1056\n",
            "789/821 [===========================>..] - ETA: 0s - loss: 0.1173 - accuracy: 0.9506\n",
            "Epoch 00147: val_loss did not improve from 0.07698\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1166 - accuracy: 0.9509 - val_loss: 0.0775 - val_accuracy: 0.9644\n",
            "Epoch 148/1056\n",
            "782/821 [===========================>..] - ETA: 0s - loss: 0.1179 - accuracy: 0.9520\n",
            "Epoch 00148: val_loss did not improve from 0.07698\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1167 - accuracy: 0.9525 - val_loss: 0.0776 - val_accuracy: 0.9651\n",
            "Epoch 149/1056\n",
            "820/821 [============================>.] - ETA: 0s - loss: 0.1169 - accuracy: 0.9512\n",
            "Epoch 00149: val_loss did not improve from 0.07698\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1168 - accuracy: 0.9512 - val_loss: 0.0781 - val_accuracy: 0.9637\n",
            "Epoch 150/1056\n",
            "773/821 [===========================>..] - ETA: 0s - loss: 0.1174 - accuracy: 0.9521\n",
            "Epoch 00150: val_loss did not improve from 0.07698\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1172 - accuracy: 0.9518 - val_loss: 0.0780 - val_accuracy: 0.9637\n",
            "Epoch 151/1056\n",
            "783/821 [===========================>..] - ETA: 0s - loss: 0.1172 - accuracy: 0.9531\n",
            "Epoch 00151: val_loss improved from 0.07698 to 0.07693, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1167 - accuracy: 0.9521 - val_loss: 0.0769 - val_accuracy: 0.9651\n",
            "Epoch 152/1056\n",
            "772/821 [===========================>..] - ETA: 0s - loss: 0.1169 - accuracy: 0.9501\n",
            "Epoch 00152: val_loss did not improve from 0.07693\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1168 - accuracy: 0.9509 - val_loss: 0.0774 - val_accuracy: 0.9637\n",
            "Epoch 153/1056\n",
            "783/821 [===========================>..] - ETA: 0s - loss: 0.1183 - accuracy: 0.9505\n",
            "Epoch 00153: val_loss did not improve from 0.07693\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1172 - accuracy: 0.9515 - val_loss: 0.0776 - val_accuracy: 0.9637\n",
            "Epoch 154/1056\n",
            "814/821 [============================>.] - ETA: 0s - loss: 0.1174 - accuracy: 0.9518\n",
            "Epoch 00154: val_loss did not improve from 0.07693\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1169 - accuracy: 0.9518 - val_loss: 0.0773 - val_accuracy: 0.9651\n",
            "Epoch 155/1056\n",
            "778/821 [===========================>..] - ETA: 0s - loss: 0.1187 - accuracy: 0.9508\n",
            "Epoch 00155: val_loss did not improve from 0.07693\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1169 - accuracy: 0.9518 - val_loss: 0.0779 - val_accuracy: 0.9644\n",
            "Epoch 156/1056\n",
            "795/821 [============================>.] - ETA: 0s - loss: 0.1173 - accuracy: 0.9519\n",
            "Epoch 00156: val_loss did not improve from 0.07693\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1169 - accuracy: 0.9518 - val_loss: 0.0783 - val_accuracy: 0.9644\n",
            "Epoch 157/1056\n",
            "803/821 [============================>.] - ETA: 0s - loss: 0.1172 - accuracy: 0.9527\n",
            "Epoch 00157: val_loss did not improve from 0.07693\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1171 - accuracy: 0.9528 - val_loss: 0.0781 - val_accuracy: 0.9644\n",
            "Epoch 158/1056\n",
            "778/821 [===========================>..] - ETA: 0s - loss: 0.1170 - accuracy: 0.9534\n",
            "Epoch 00158: val_loss did not improve from 0.07693\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1174 - accuracy: 0.9525 - val_loss: 0.0778 - val_accuracy: 0.9630\n",
            "Epoch 159/1056\n",
            "802/821 [============================>.] - ETA: 0s - loss: 0.1185 - accuracy: 0.9511\n",
            "Epoch 00159: val_loss did not improve from 0.07693\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1175 - accuracy: 0.9515 - val_loss: 0.0779 - val_accuracy: 0.9630\n",
            "Epoch 160/1056\n",
            "794/821 [============================>.] - ETA: 0s - loss: 0.1176 - accuracy: 0.9525\n",
            "Epoch 00160: val_loss did not improve from 0.07693\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1175 - accuracy: 0.9521 - val_loss: 0.0776 - val_accuracy: 0.9630\n",
            "Epoch 161/1056\n",
            "794/821 [============================>.] - ETA: 0s - loss: 0.1191 - accuracy: 0.9509\n",
            "Epoch 00161: val_loss did not improve from 0.07693\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1177 - accuracy: 0.9515 - val_loss: 0.0771 - val_accuracy: 0.9644\n",
            "Epoch 162/1056\n",
            "816/821 [============================>.] - ETA: 0s - loss: 0.1178 - accuracy: 0.9531\n",
            "Epoch 00162: val_loss did not improve from 0.07693\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1176 - accuracy: 0.9531 - val_loss: 0.0773 - val_accuracy: 0.9637\n",
            "Epoch 163/1056\n",
            "789/821 [===========================>..] - ETA: 0s - loss: 0.1192 - accuracy: 0.9512\n",
            "Epoch 00163: val_loss did not improve from 0.07693\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1175 - accuracy: 0.9515 - val_loss: 0.0775 - val_accuracy: 0.9637\n",
            "Epoch 164/1056\n",
            "783/821 [===========================>..] - ETA: 0s - loss: 0.1190 - accuracy: 0.9511\n",
            "Epoch 00164: val_loss did not improve from 0.07693\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1175 - accuracy: 0.9515 - val_loss: 0.0774 - val_accuracy: 0.9630\n",
            "Epoch 165/1056\n",
            "776/821 [===========================>..] - ETA: 0s - loss: 0.1197 - accuracy: 0.9507\n",
            "Epoch 00165: val_loss did not improve from 0.07693\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1178 - accuracy: 0.9515 - val_loss: 0.0771 - val_accuracy: 0.9637\n",
            "Epoch 166/1056\n",
            "791/821 [===========================>..] - ETA: 0s - loss: 0.1186 - accuracy: 0.9510\n",
            "Epoch 00166: val_loss did not improve from 0.07693\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1178 - accuracy: 0.9515 - val_loss: 0.0773 - val_accuracy: 0.9630\n",
            "Epoch 167/1056\n",
            "788/821 [===========================>..] - ETA: 0s - loss: 0.1187 - accuracy: 0.9505\n",
            "Epoch 00167: val_loss improved from 0.07693 to 0.07655, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1179 - accuracy: 0.9512 - val_loss: 0.0765 - val_accuracy: 0.9651\n",
            "Epoch 168/1056\n",
            "776/821 [===========================>..] - ETA: 0s - loss: 0.1187 - accuracy: 0.9510\n",
            "Epoch 00168: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1178 - accuracy: 0.9512 - val_loss: 0.0768 - val_accuracy: 0.9637\n",
            "Epoch 169/1056\n",
            "795/821 [============================>.] - ETA: 0s - loss: 0.1183 - accuracy: 0.9516\n",
            "Epoch 00169: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1177 - accuracy: 0.9512 - val_loss: 0.0771 - val_accuracy: 0.9637\n",
            "Epoch 170/1056\n",
            "799/821 [============================>.] - ETA: 0s - loss: 0.1170 - accuracy: 0.9521\n",
            "Epoch 00170: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1178 - accuracy: 0.9512 - val_loss: 0.0770 - val_accuracy: 0.9637\n",
            "Epoch 171/1056\n",
            "791/821 [===========================>..] - ETA: 0s - loss: 0.1194 - accuracy: 0.9513\n",
            "Epoch 00171: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1176 - accuracy: 0.9518 - val_loss: 0.0770 - val_accuracy: 0.9644\n",
            "Epoch 172/1056\n",
            "783/821 [===========================>..] - ETA: 0s - loss: 0.1203 - accuracy: 0.9499\n",
            "Epoch 00172: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1180 - accuracy: 0.9506 - val_loss: 0.0773 - val_accuracy: 0.9630\n",
            "Epoch 173/1056\n",
            "795/821 [============================>.] - ETA: 0s - loss: 0.1187 - accuracy: 0.9513\n",
            "Epoch 00173: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1181 - accuracy: 0.9515 - val_loss: 0.0770 - val_accuracy: 0.9623\n",
            "Epoch 174/1056\n",
            "784/821 [===========================>..] - ETA: 0s - loss: 0.1199 - accuracy: 0.9512\n",
            "Epoch 00174: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1182 - accuracy: 0.9515 - val_loss: 0.0768 - val_accuracy: 0.9637\n",
            "Epoch 175/1056\n",
            "776/821 [===========================>..] - ETA: 0s - loss: 0.1191 - accuracy: 0.9504\n",
            "Epoch 00175: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1181 - accuracy: 0.9512 - val_loss: 0.0770 - val_accuracy: 0.9637\n",
            "Epoch 176/1056\n",
            "786/821 [===========================>..] - ETA: 0s - loss: 0.1197 - accuracy: 0.9510\n",
            "Epoch 00176: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1181 - accuracy: 0.9515 - val_loss: 0.0769 - val_accuracy: 0.9637\n",
            "Epoch 177/1056\n",
            "777/821 [===========================>..] - ETA: 0s - loss: 0.1182 - accuracy: 0.9517\n",
            "Epoch 00177: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1181 - accuracy: 0.9515 - val_loss: 0.0771 - val_accuracy: 0.9644\n",
            "Epoch 178/1056\n",
            "794/821 [============================>.] - ETA: 0s - loss: 0.1191 - accuracy: 0.9509\n",
            "Epoch 00178: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1182 - accuracy: 0.9515 - val_loss: 0.0768 - val_accuracy: 0.9644\n",
            "Epoch 179/1056\n",
            "777/821 [===========================>..] - ETA: 0s - loss: 0.1180 - accuracy: 0.9527\n",
            "Epoch 00179: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1183 - accuracy: 0.9518 - val_loss: 0.0769 - val_accuracy: 0.9637\n",
            "Epoch 180/1056\n",
            "795/821 [============================>.] - ETA: 0s - loss: 0.1192 - accuracy: 0.9516\n",
            "Epoch 00180: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1184 - accuracy: 0.9518 - val_loss: 0.0771 - val_accuracy: 0.9630\n",
            "Epoch 181/1056\n",
            "779/821 [===========================>..] - ETA: 0s - loss: 0.1210 - accuracy: 0.9506\n",
            "Epoch 00181: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1184 - accuracy: 0.9515 - val_loss: 0.0772 - val_accuracy: 0.9630\n",
            "Epoch 182/1056\n",
            "800/821 [============================>.] - ETA: 0s - loss: 0.1195 - accuracy: 0.9500\n",
            "Epoch 00182: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1185 - accuracy: 0.9506 - val_loss: 0.0782 - val_accuracy: 0.9644\n",
            "Epoch 183/1056\n",
            "790/821 [===========================>..] - ETA: 0s - loss: 0.1209 - accuracy: 0.9522\n",
            "Epoch 00183: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1190 - accuracy: 0.9531 - val_loss: 0.0785 - val_accuracy: 0.9637\n",
            "Epoch 184/1056\n",
            "812/821 [============================>.] - ETA: 0s - loss: 0.1182 - accuracy: 0.9526\n",
            "Epoch 00184: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1191 - accuracy: 0.9518 - val_loss: 0.0771 - val_accuracy: 0.9637\n",
            "Epoch 185/1056\n",
            "779/821 [===========================>..] - ETA: 0s - loss: 0.1211 - accuracy: 0.9519\n",
            "Epoch 00185: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1194 - accuracy: 0.9525 - val_loss: 0.0776 - val_accuracy: 0.9644\n",
            "Epoch 186/1056\n",
            "786/821 [===========================>..] - ETA: 0s - loss: 0.1227 - accuracy: 0.9507\n",
            "Epoch 00186: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1190 - accuracy: 0.9521 - val_loss: 0.0774 - val_accuracy: 0.9637\n",
            "Epoch 187/1056\n",
            "789/821 [===========================>..] - ETA: 0s - loss: 0.1202 - accuracy: 0.9522\n",
            "Epoch 00187: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1191 - accuracy: 0.9518 - val_loss: 0.0779 - val_accuracy: 0.9644\n",
            "Epoch 188/1056\n",
            "795/821 [============================>.] - ETA: 0s - loss: 0.1100 - accuracy: 0.9531\n",
            "Epoch 00188: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1193 - accuracy: 0.9528 - val_loss: 0.0775 - val_accuracy: 0.9637\n",
            "Epoch 189/1056\n",
            "783/821 [===========================>..] - ETA: 0s - loss: 0.1201 - accuracy: 0.9524\n",
            "Epoch 00189: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1192 - accuracy: 0.9518 - val_loss: 0.0772 - val_accuracy: 0.9637\n",
            "Epoch 190/1056\n",
            "793/821 [===========================>..] - ETA: 0s - loss: 0.1205 - accuracy: 0.9521\n",
            "Epoch 00190: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1194 - accuracy: 0.9528 - val_loss: 0.0770 - val_accuracy: 0.9623\n",
            "Epoch 191/1056\n",
            "820/821 [============================>.] - ETA: 0s - loss: 0.1190 - accuracy: 0.9521\n",
            "Epoch 00191: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1189 - accuracy: 0.9521 - val_loss: 0.0775 - val_accuracy: 0.9616\n",
            "Epoch 192/1056\n",
            "791/821 [===========================>..] - ETA: 0s - loss: 0.1207 - accuracy: 0.9510\n",
            "Epoch 00192: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1193 - accuracy: 0.9518 - val_loss: 0.0770 - val_accuracy: 0.9616\n",
            "Epoch 193/1056\n",
            "783/821 [===========================>..] - ETA: 0s - loss: 0.1200 - accuracy: 0.9508\n",
            "Epoch 00193: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1189 - accuracy: 0.9509 - val_loss: 0.0775 - val_accuracy: 0.9616\n",
            "Epoch 194/1056\n",
            "803/821 [============================>.] - ETA: 0s - loss: 0.1211 - accuracy: 0.9514\n",
            "Epoch 00194: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1195 - accuracy: 0.9518 - val_loss: 0.0780 - val_accuracy: 0.9630\n",
            "Epoch 195/1056\n",
            "777/821 [===========================>..] - ETA: 0s - loss: 0.1231 - accuracy: 0.9511\n",
            "Epoch 00195: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1195 - accuracy: 0.9521 - val_loss: 0.0783 - val_accuracy: 0.9616\n",
            "Epoch 196/1056\n",
            "784/821 [===========================>..] - ETA: 0s - loss: 0.1213 - accuracy: 0.9506\n",
            "Epoch 00196: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1195 - accuracy: 0.9509 - val_loss: 0.0776 - val_accuracy: 0.9630\n",
            "Epoch 197/1056\n",
            "774/821 [===========================>..] - ETA: 0s - loss: 0.1210 - accuracy: 0.9519\n",
            "Epoch 00197: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1196 - accuracy: 0.9525 - val_loss: 0.0779 - val_accuracy: 0.9644\n",
            "Epoch 198/1056\n",
            "792/821 [===========================>..] - ETA: 0s - loss: 0.1208 - accuracy: 0.9514\n",
            "Epoch 00198: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1196 - accuracy: 0.9518 - val_loss: 0.0769 - val_accuracy: 0.9644\n",
            "Epoch 199/1056\n",
            "789/821 [===========================>..] - ETA: 0s - loss: 0.1114 - accuracy: 0.9515\n",
            "Epoch 00199: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1195 - accuracy: 0.9518 - val_loss: 0.0776 - val_accuracy: 0.9630\n",
            "Epoch 200/1056\n",
            "793/821 [===========================>..] - ETA: 0s - loss: 0.1220 - accuracy: 0.9511\n",
            "Epoch 00200: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1199 - accuracy: 0.9521 - val_loss: 0.0780 - val_accuracy: 0.9630\n",
            "Epoch 201/1056\n",
            "821/821 [==============================] - ETA: 0s - loss: 0.1201 - accuracy: 0.9512\n",
            "Epoch 00201: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1201 - accuracy: 0.9512 - val_loss: 0.0780 - val_accuracy: 0.9623\n",
            "Epoch 202/1056\n",
            "805/821 [============================>.] - ETA: 0s - loss: 0.1210 - accuracy: 0.9506\n",
            "Epoch 00202: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1200 - accuracy: 0.9509 - val_loss: 0.0777 - val_accuracy: 0.9637\n",
            "Epoch 203/1056\n",
            "773/821 [===========================>..] - ETA: 0s - loss: 0.1220 - accuracy: 0.9508\n",
            "Epoch 00203: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1201 - accuracy: 0.9512 - val_loss: 0.0776 - val_accuracy: 0.9644\n",
            "Epoch 204/1056\n",
            "790/821 [===========================>..] - ETA: 0s - loss: 0.1201 - accuracy: 0.9513\n",
            "Epoch 00204: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1200 - accuracy: 0.9512 - val_loss: 0.0778 - val_accuracy: 0.9630\n",
            "Epoch 205/1056\n",
            "792/821 [===========================>..] - ETA: 0s - loss: 0.1212 - accuracy: 0.9508\n",
            "Epoch 00205: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1201 - accuracy: 0.9512 - val_loss: 0.0776 - val_accuracy: 0.9637\n",
            "Epoch 206/1056\n",
            "785/821 [===========================>..] - ETA: 0s - loss: 0.1195 - accuracy: 0.9522\n",
            "Epoch 00206: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1201 - accuracy: 0.9525 - val_loss: 0.0773 - val_accuracy: 0.9630\n",
            "Epoch 207/1056\n",
            "786/821 [===========================>..] - ETA: 0s - loss: 0.1210 - accuracy: 0.9520\n",
            "Epoch 00207: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1202 - accuracy: 0.9512 - val_loss: 0.0770 - val_accuracy: 0.9644\n",
            "Epoch 208/1056\n",
            "793/821 [===========================>..] - ETA: 0s - loss: 0.1103 - accuracy: 0.9518\n",
            "Epoch 00208: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1202 - accuracy: 0.9509 - val_loss: 0.0773 - val_accuracy: 0.9637\n",
            "Epoch 209/1056\n",
            "803/821 [============================>.] - ETA: 0s - loss: 0.1199 - accuracy: 0.9524\n",
            "Epoch 00209: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1202 - accuracy: 0.9518 - val_loss: 0.0772 - val_accuracy: 0.9644\n",
            "Epoch 210/1056\n",
            "784/821 [===========================>..] - ETA: 0s - loss: 0.1206 - accuracy: 0.9515\n",
            "Epoch 00210: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.1204 - accuracy: 0.9521 - val_loss: 0.0776 - val_accuracy: 0.9637\n",
            "Epoch 211/1056\n",
            "785/821 [===========================>..] - ETA: 0s - loss: 0.1212 - accuracy: 0.9516\n",
            "Epoch 00211: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1204 - accuracy: 0.9509 - val_loss: 0.0768 - val_accuracy: 0.9644\n",
            "Epoch 212/1056\n",
            "798/821 [============================>.] - ETA: 0s - loss: 0.1212 - accuracy: 0.9514\n",
            "Epoch 00212: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1202 - accuracy: 0.9512 - val_loss: 0.0771 - val_accuracy: 0.9644\n",
            "Epoch 213/1056\n",
            "789/821 [===========================>..] - ETA: 0s - loss: 0.1219 - accuracy: 0.9506\n",
            "Epoch 00213: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1204 - accuracy: 0.9512 - val_loss: 0.0774 - val_accuracy: 0.9644\n",
            "Epoch 214/1056\n",
            "777/821 [===========================>..] - ETA: 0s - loss: 0.1224 - accuracy: 0.9501\n",
            "Epoch 00214: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.1206 - accuracy: 0.9503 - val_loss: 0.0776 - val_accuracy: 0.9637\n",
            "Epoch 215/1056\n",
            "806/821 [============================>.] - ETA: 0s - loss: 0.1220 - accuracy: 0.9513\n",
            "Epoch 00215: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.1206 - accuracy: 0.9518 - val_loss: 0.0774 - val_accuracy: 0.9644\n",
            "Epoch 216/1056\n",
            "814/821 [============================>.] - ETA: 0s - loss: 0.1212 - accuracy: 0.9506\n",
            "Epoch 00216: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.1209 - accuracy: 0.9506 - val_loss: 0.0770 - val_accuracy: 0.9637\n",
            "Epoch 217/1056\n",
            "789/821 [===========================>..] - ETA: 0s - loss: 0.1204 - accuracy: 0.9515\n",
            "Epoch 00217: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.1209 - accuracy: 0.9512 - val_loss: 0.0780 - val_accuracy: 0.9644\n",
            "Epoch 218/1056\n",
            "809/821 [============================>.] - ETA: 0s - loss: 0.1223 - accuracy: 0.9512\n",
            "Epoch 00218: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.1211 - accuracy: 0.9515 - val_loss: 0.0778 - val_accuracy: 0.9637\n",
            "Epoch 219/1056\n",
            "810/821 [============================>.] - ETA: 0s - loss: 0.1202 - accuracy: 0.9512\n",
            "Epoch 00219: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.1210 - accuracy: 0.9509 - val_loss: 0.0776 - val_accuracy: 0.9644\n",
            "Epoch 220/1056\n",
            "782/821 [===========================>..] - ETA: 0s - loss: 0.1205 - accuracy: 0.9527\n",
            "Epoch 00220: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.1209 - accuracy: 0.9521 - val_loss: 0.0770 - val_accuracy: 0.9637\n",
            "Epoch 221/1056\n",
            "798/821 [============================>.] - ETA: 0s - loss: 0.1217 - accuracy: 0.9521\n",
            "Epoch 00221: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.1210 - accuracy: 0.9518 - val_loss: 0.0771 - val_accuracy: 0.9644\n",
            "Epoch 222/1056\n",
            "798/821 [============================>.] - ETA: 0s - loss: 0.1225 - accuracy: 0.9514\n",
            "Epoch 00222: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.1211 - accuracy: 0.9515 - val_loss: 0.0776 - val_accuracy: 0.9644\n",
            "Epoch 223/1056\n",
            "812/821 [============================>.] - ETA: 0s - loss: 0.1219 - accuracy: 0.9514\n",
            "Epoch 00223: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.1211 - accuracy: 0.9518 - val_loss: 0.0777 - val_accuracy: 0.9644\n",
            "Epoch 224/1056\n",
            "815/821 [============================>.] - ETA: 0s - loss: 0.1217 - accuracy: 0.9512\n",
            "Epoch 00224: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.1214 - accuracy: 0.9512 - val_loss: 0.0779 - val_accuracy: 0.9644\n",
            "Epoch 225/1056\n",
            "803/821 [============================>.] - ETA: 0s - loss: 0.1229 - accuracy: 0.9502\n",
            "Epoch 00225: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1216 - accuracy: 0.9506 - val_loss: 0.0774 - val_accuracy: 0.9644\n",
            "Epoch 226/1056\n",
            "785/821 [===========================>..] - ETA: 0s - loss: 0.1196 - accuracy: 0.9519\n",
            "Epoch 00226: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1215 - accuracy: 0.9503 - val_loss: 0.0773 - val_accuracy: 0.9644\n",
            "Epoch 227/1056\n",
            "792/821 [===========================>..] - ETA: 0s - loss: 0.1229 - accuracy: 0.9501\n",
            "Epoch 00227: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1217 - accuracy: 0.9506 - val_loss: 0.0776 - val_accuracy: 0.9644\n",
            "Epoch 228/1056\n",
            "818/821 [============================>.] - ETA: 0s - loss: 0.1216 - accuracy: 0.9514\n",
            "Epoch 00228: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1216 - accuracy: 0.9515 - val_loss: 0.0778 - val_accuracy: 0.9644\n",
            "Epoch 229/1056\n",
            "791/821 [===========================>..] - ETA: 0s - loss: 0.1224 - accuracy: 0.9516\n",
            "Epoch 00229: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1217 - accuracy: 0.9515 - val_loss: 0.0773 - val_accuracy: 0.9644\n",
            "Epoch 230/1056\n",
            "789/821 [===========================>..] - ETA: 0s - loss: 0.1220 - accuracy: 0.9518\n",
            "Epoch 00230: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1216 - accuracy: 0.9515 - val_loss: 0.0772 - val_accuracy: 0.9637\n",
            "Epoch 231/1056\n",
            "779/821 [===========================>..] - ETA: 0s - loss: 0.1224 - accuracy: 0.9512\n",
            "Epoch 00231: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1216 - accuracy: 0.9518 - val_loss: 0.0771 - val_accuracy: 0.9637\n",
            "Epoch 232/1056\n",
            "782/821 [===========================>..] - ETA: 0s - loss: 0.1195 - accuracy: 0.9543\n",
            "Epoch 00232: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1215 - accuracy: 0.9518 - val_loss: 0.0772 - val_accuracy: 0.9630\n",
            "Epoch 233/1056\n",
            "789/821 [===========================>..] - ETA: 0s - loss: 0.1248 - accuracy: 0.9499\n",
            "Epoch 00233: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1218 - accuracy: 0.9512 - val_loss: 0.0777 - val_accuracy: 0.9644\n",
            "Epoch 234/1056\n",
            "796/821 [============================>.] - ETA: 0s - loss: 0.1235 - accuracy: 0.9504\n",
            "Epoch 00234: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1218 - accuracy: 0.9509 - val_loss: 0.0779 - val_accuracy: 0.9644\n",
            "Epoch 235/1056\n",
            "788/821 [===========================>..] - ETA: 0s - loss: 0.1213 - accuracy: 0.9530\n",
            "Epoch 00235: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1219 - accuracy: 0.9512 - val_loss: 0.0774 - val_accuracy: 0.9637\n",
            "Epoch 236/1056\n",
            "815/821 [============================>.] - ETA: 0s - loss: 0.1223 - accuracy: 0.9512\n",
            "Epoch 00236: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1219 - accuracy: 0.9512 - val_loss: 0.0775 - val_accuracy: 0.9637\n",
            "Epoch 237/1056\n",
            "783/821 [===========================>..] - ETA: 0s - loss: 0.1222 - accuracy: 0.9521\n",
            "Epoch 00237: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1220 - accuracy: 0.9512 - val_loss: 0.0777 - val_accuracy: 0.9644\n",
            "Epoch 238/1056\n",
            "786/821 [===========================>..] - ETA: 0s - loss: 0.1242 - accuracy: 0.9513\n",
            "Epoch 00238: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1222 - accuracy: 0.9518 - val_loss: 0.0783 - val_accuracy: 0.9637\n",
            "Epoch 239/1056\n",
            "785/821 [===========================>..] - ETA: 0s - loss: 0.1209 - accuracy: 0.9522\n",
            "Epoch 00239: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1222 - accuracy: 0.9509 - val_loss: 0.0781 - val_accuracy: 0.9644\n",
            "Epoch 240/1056\n",
            "775/821 [===========================>..] - ETA: 0s - loss: 0.1245 - accuracy: 0.9513\n",
            "Epoch 00240: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1222 - accuracy: 0.9512 - val_loss: 0.0779 - val_accuracy: 0.9644\n",
            "Epoch 241/1056\n",
            "813/821 [============================>.] - ETA: 0s - loss: 0.1226 - accuracy: 0.9508\n",
            "Epoch 00241: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1223 - accuracy: 0.9506 - val_loss: 0.0780 - val_accuracy: 0.9644\n",
            "Epoch 242/1056\n",
            "785/821 [===========================>..] - ETA: 0s - loss: 0.1240 - accuracy: 0.9500\n",
            "Epoch 00242: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1223 - accuracy: 0.9503 - val_loss: 0.0787 - val_accuracy: 0.9644\n",
            "Epoch 243/1056\n",
            "797/821 [============================>.] - ETA: 0s - loss: 0.1237 - accuracy: 0.9511\n",
            "Epoch 00243: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1224 - accuracy: 0.9512 - val_loss: 0.0783 - val_accuracy: 0.9644\n",
            "Epoch 244/1056\n",
            "779/821 [===========================>..] - ETA: 0s - loss: 0.1110 - accuracy: 0.9509\n",
            "Epoch 00244: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1225 - accuracy: 0.9503 - val_loss: 0.0778 - val_accuracy: 0.9637\n",
            "Epoch 245/1056\n",
            "786/821 [===========================>..] - ETA: 0s - loss: 0.1229 - accuracy: 0.9510\n",
            "Epoch 00245: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1226 - accuracy: 0.9509 - val_loss: 0.0774 - val_accuracy: 0.9637\n",
            "Epoch 246/1056\n",
            "800/821 [============================>.] - ETA: 0s - loss: 0.1234 - accuracy: 0.9522\n",
            "Epoch 00246: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1226 - accuracy: 0.9521 - val_loss: 0.0775 - val_accuracy: 0.9644\n",
            "Epoch 247/1056\n",
            "785/821 [===========================>..] - ETA: 0s - loss: 0.1259 - accuracy: 0.9497\n",
            "Epoch 00247: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1227 - accuracy: 0.9506 - val_loss: 0.0781 - val_accuracy: 0.9644\n",
            "Epoch 248/1056\n",
            "796/821 [============================>.] - ETA: 0s - loss: 0.1223 - accuracy: 0.9510\n",
            "Epoch 00248: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1227 - accuracy: 0.9506 - val_loss: 0.0785 - val_accuracy: 0.9630\n",
            "Epoch 249/1056\n",
            "772/821 [===========================>..] - ETA: 0s - loss: 0.1257 - accuracy: 0.9488\n",
            "Epoch 00249: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1229 - accuracy: 0.9503 - val_loss: 0.0787 - val_accuracy: 0.9644\n",
            "Epoch 250/1056\n",
            "786/821 [===========================>..] - ETA: 0s - loss: 0.1248 - accuracy: 0.9501\n",
            "Epoch 00250: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1231 - accuracy: 0.9506 - val_loss: 0.0788 - val_accuracy: 0.9637\n",
            "Epoch 251/1056\n",
            "813/821 [============================>.] - ETA: 0s - loss: 0.1239 - accuracy: 0.9499\n",
            "Epoch 00251: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1233 - accuracy: 0.9503 - val_loss: 0.0786 - val_accuracy: 0.9630\n",
            "Epoch 252/1056\n",
            "782/821 [===========================>..] - ETA: 0s - loss: 0.1125 - accuracy: 0.9508\n",
            "Epoch 00252: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1234 - accuracy: 0.9503 - val_loss: 0.0789 - val_accuracy: 0.9644\n",
            "Epoch 253/1056\n",
            "792/821 [===========================>..] - ETA: 0s - loss: 0.1240 - accuracy: 0.9511\n",
            "Epoch 00253: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1235 - accuracy: 0.9509 - val_loss: 0.0781 - val_accuracy: 0.9644\n",
            "Epoch 254/1056\n",
            "781/821 [===========================>..] - ETA: 0s - loss: 0.1251 - accuracy: 0.9507\n",
            "Epoch 00254: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1233 - accuracy: 0.9509 - val_loss: 0.0780 - val_accuracy: 0.9637\n",
            "Epoch 255/1056\n",
            "792/821 [===========================>..] - ETA: 0s - loss: 0.1253 - accuracy: 0.9501\n",
            "Epoch 00255: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1236 - accuracy: 0.9506 - val_loss: 0.0778 - val_accuracy: 0.9637\n",
            "Epoch 256/1056\n",
            "778/821 [===========================>..] - ETA: 0s - loss: 0.1248 - accuracy: 0.9502\n",
            "Epoch 00256: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1235 - accuracy: 0.9503 - val_loss: 0.0785 - val_accuracy: 0.9637\n",
            "Epoch 257/1056\n",
            "789/821 [===========================>..] - ETA: 0s - loss: 0.1252 - accuracy: 0.9506\n",
            "Epoch 00257: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1234 - accuracy: 0.9509 - val_loss: 0.0783 - val_accuracy: 0.9644\n",
            "Epoch 258/1056\n",
            "785/821 [===========================>..] - ETA: 0s - loss: 0.1247 - accuracy: 0.9516\n",
            "Epoch 00258: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1237 - accuracy: 0.9509 - val_loss: 0.0786 - val_accuracy: 0.9637\n",
            "Epoch 259/1056\n",
            "783/821 [===========================>..] - ETA: 0s - loss: 0.1231 - accuracy: 0.9505\n",
            "Epoch 00259: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1238 - accuracy: 0.9503 - val_loss: 0.0791 - val_accuracy: 0.9630\n",
            "Epoch 260/1056\n",
            "806/821 [============================>.] - ETA: 0s - loss: 0.1245 - accuracy: 0.9513\n",
            "Epoch 00260: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1236 - accuracy: 0.9512 - val_loss: 0.0795 - val_accuracy: 0.9637\n",
            "Epoch 261/1056\n",
            "792/821 [===========================>..] - ETA: 0s - loss: 0.1249 - accuracy: 0.9495\n",
            "Epoch 00261: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1239 - accuracy: 0.9497 - val_loss: 0.0795 - val_accuracy: 0.9637\n",
            "Epoch 262/1056\n",
            "780/821 [===========================>..] - ETA: 0s - loss: 0.1244 - accuracy: 0.9503\n",
            "Epoch 00262: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1240 - accuracy: 0.9503 - val_loss: 0.0791 - val_accuracy: 0.9651\n",
            "Epoch 263/1056\n",
            "805/821 [============================>.] - ETA: 0s - loss: 0.1247 - accuracy: 0.9494\n",
            "Epoch 00263: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1243 - accuracy: 0.9497 - val_loss: 0.0784 - val_accuracy: 0.9637\n",
            "Epoch 264/1056\n",
            "782/821 [===========================>..] - ETA: 0s - loss: 0.1267 - accuracy: 0.9504\n",
            "Epoch 00264: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1241 - accuracy: 0.9503 - val_loss: 0.0787 - val_accuracy: 0.9637\n",
            "Epoch 265/1056\n",
            "794/821 [============================>.] - ETA: 0s - loss: 0.1254 - accuracy: 0.9487\n",
            "Epoch 00265: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1240 - accuracy: 0.9494 - val_loss: 0.0784 - val_accuracy: 0.9637\n",
            "Epoch 266/1056\n",
            "783/821 [===========================>..] - ETA: 0s - loss: 0.1259 - accuracy: 0.9496\n",
            "Epoch 00266: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1243 - accuracy: 0.9503 - val_loss: 0.0784 - val_accuracy: 0.9637\n",
            "Epoch 267/1056\n",
            "779/821 [===========================>..] - ETA: 0s - loss: 0.1259 - accuracy: 0.9512\n",
            "Epoch 00267: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1243 - accuracy: 0.9509 - val_loss: 0.0781 - val_accuracy: 0.9637\n",
            "Epoch 268/1056\n",
            "785/821 [===========================>..] - ETA: 0s - loss: 0.1249 - accuracy: 0.9510\n",
            "Epoch 00268: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1237 - accuracy: 0.9509 - val_loss: 0.0782 - val_accuracy: 0.9637\n",
            "Epoch 269/1056\n",
            "793/821 [===========================>..] - ETA: 0s - loss: 0.1265 - accuracy: 0.9492\n",
            "Epoch 00269: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1240 - accuracy: 0.9500 - val_loss: 0.0790 - val_accuracy: 0.9637\n",
            "Epoch 270/1056\n",
            "790/821 [===========================>..] - ETA: 0s - loss: 0.1247 - accuracy: 0.9516\n",
            "Epoch 00270: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1242 - accuracy: 0.9512 - val_loss: 0.0793 - val_accuracy: 0.9637\n",
            "Epoch 271/1056\n",
            "788/821 [===========================>..] - ETA: 0s - loss: 0.1250 - accuracy: 0.9502\n",
            "Epoch 00271: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1244 - accuracy: 0.9503 - val_loss: 0.0794 - val_accuracy: 0.9644\n",
            "Epoch 272/1056\n",
            "809/821 [============================>.] - ETA: 0s - loss: 0.1246 - accuracy: 0.9521\n",
            "Epoch 00272: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1243 - accuracy: 0.9521 - val_loss: 0.0785 - val_accuracy: 0.9659\n",
            "Epoch 273/1056\n",
            "803/821 [============================>.] - ETA: 0s - loss: 0.1144 - accuracy: 0.9493\n",
            "Epoch 00273: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1251 - accuracy: 0.9497 - val_loss: 0.0786 - val_accuracy: 0.9651\n",
            "Epoch 274/1056\n",
            "777/821 [===========================>..] - ETA: 0s - loss: 0.1270 - accuracy: 0.9501\n",
            "Epoch 00274: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1251 - accuracy: 0.9503 - val_loss: 0.0790 - val_accuracy: 0.9651\n",
            "Epoch 275/1056\n",
            "776/821 [===========================>..] - ETA: 0s - loss: 0.1268 - accuracy: 0.9501\n",
            "Epoch 00275: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1252 - accuracy: 0.9503 - val_loss: 0.0794 - val_accuracy: 0.9637\n",
            "Epoch 276/1056\n",
            "795/821 [============================>.] - ETA: 0s - loss: 0.1270 - accuracy: 0.9500\n",
            "Epoch 00276: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1254 - accuracy: 0.9503 - val_loss: 0.0802 - val_accuracy: 0.9637\n",
            "Epoch 277/1056\n",
            "796/821 [============================>.] - ETA: 0s - loss: 0.1256 - accuracy: 0.9501\n",
            "Epoch 00277: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1255 - accuracy: 0.9500 - val_loss: 0.0794 - val_accuracy: 0.9637\n",
            "Epoch 278/1056\n",
            "794/821 [============================>.] - ETA: 0s - loss: 0.1255 - accuracy: 0.9499\n",
            "Epoch 00278: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1257 - accuracy: 0.9494 - val_loss: 0.0794 - val_accuracy: 0.9637\n",
            "Epoch 279/1056\n",
            "797/821 [============================>.] - ETA: 0s - loss: 0.1256 - accuracy: 0.9492\n",
            "Epoch 00279: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1252 - accuracy: 0.9491 - val_loss: 0.0798 - val_accuracy: 0.9644\n",
            "Epoch 280/1056\n",
            "792/821 [===========================>..] - ETA: 0s - loss: 0.1268 - accuracy: 0.9511\n",
            "Epoch 00280: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1255 - accuracy: 0.9512 - val_loss: 0.0799 - val_accuracy: 0.9630\n",
            "Epoch 281/1056\n",
            "799/821 [============================>.] - ETA: 0s - loss: 0.1260 - accuracy: 0.9503\n",
            "Epoch 00281: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1255 - accuracy: 0.9506 - val_loss: 0.0794 - val_accuracy: 0.9637\n",
            "Epoch 282/1056\n",
            "818/821 [============================>.] - ETA: 0s - loss: 0.1254 - accuracy: 0.9499\n",
            "Epoch 00282: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1253 - accuracy: 0.9497 - val_loss: 0.0799 - val_accuracy: 0.9630\n",
            "Epoch 283/1056\n",
            "788/821 [===========================>..] - ETA: 0s - loss: 0.1277 - accuracy: 0.9505\n",
            "Epoch 00283: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1253 - accuracy: 0.9509 - val_loss: 0.0801 - val_accuracy: 0.9630\n",
            "Epoch 284/1056\n",
            "780/821 [===========================>..] - ETA: 0s - loss: 0.1269 - accuracy: 0.9506\n",
            "Epoch 00284: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1256 - accuracy: 0.9506 - val_loss: 0.0795 - val_accuracy: 0.9637\n",
            "Epoch 285/1056\n",
            "778/821 [===========================>..] - ETA: 0s - loss: 0.1274 - accuracy: 0.9499\n",
            "Epoch 00285: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1257 - accuracy: 0.9506 - val_loss: 0.0798 - val_accuracy: 0.9623\n",
            "Epoch 286/1056\n",
            "796/821 [============================>.] - ETA: 0s - loss: 0.1264 - accuracy: 0.9497\n",
            "Epoch 00286: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1256 - accuracy: 0.9503 - val_loss: 0.0795 - val_accuracy: 0.9630\n",
            "Epoch 287/1056\n",
            "790/821 [===========================>..] - ETA: 0s - loss: 0.1247 - accuracy: 0.9500\n",
            "Epoch 00287: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1258 - accuracy: 0.9497 - val_loss: 0.0795 - val_accuracy: 0.9637\n",
            "Epoch 288/1056\n",
            "794/821 [============================>.] - ETA: 0s - loss: 0.1275 - accuracy: 0.9499\n",
            "Epoch 00288: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1260 - accuracy: 0.9503 - val_loss: 0.0792 - val_accuracy: 0.9637\n",
            "Epoch 289/1056\n",
            "781/821 [===========================>..] - ETA: 0s - loss: 0.1280 - accuracy: 0.9504\n",
            "Epoch 00289: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1263 - accuracy: 0.9503 - val_loss: 0.0806 - val_accuracy: 0.9630\n",
            "Epoch 290/1056\n",
            "794/821 [============================>.] - ETA: 0s - loss: 0.1242 - accuracy: 0.9515\n",
            "Epoch 00290: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1263 - accuracy: 0.9503 - val_loss: 0.0807 - val_accuracy: 0.9630\n",
            "Epoch 291/1056\n",
            "776/821 [===========================>..] - ETA: 0s - loss: 0.1306 - accuracy: 0.9485\n",
            "Epoch 00291: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1261 - accuracy: 0.9500 - val_loss: 0.0804 - val_accuracy: 0.9630\n",
            "Epoch 292/1056\n",
            "808/821 [============================>.] - ETA: 0s - loss: 0.1266 - accuracy: 0.9502\n",
            "Epoch 00292: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1261 - accuracy: 0.9500 - val_loss: 0.0802 - val_accuracy: 0.9623\n",
            "Epoch 293/1056\n",
            "786/821 [===========================>..] - ETA: 0s - loss: 0.1299 - accuracy: 0.9491\n",
            "Epoch 00293: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1263 - accuracy: 0.9503 - val_loss: 0.0801 - val_accuracy: 0.9623\n",
            "Epoch 294/1056\n",
            "797/821 [============================>.] - ETA: 0s - loss: 0.1271 - accuracy: 0.9504\n",
            "Epoch 00294: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1264 - accuracy: 0.9509 - val_loss: 0.0812 - val_accuracy: 0.9623\n",
            "Epoch 295/1056\n",
            "787/821 [===========================>..] - ETA: 0s - loss: 0.1273 - accuracy: 0.9504\n",
            "Epoch 00295: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1272 - accuracy: 0.9500 - val_loss: 0.0816 - val_accuracy: 0.9616\n",
            "Epoch 296/1056\n",
            "783/821 [===========================>..] - ETA: 0s - loss: 0.1164 - accuracy: 0.9492\n",
            "Epoch 00296: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1269 - accuracy: 0.9497 - val_loss: 0.0811 - val_accuracy: 0.9623\n",
            "Epoch 297/1056\n",
            "795/821 [============================>.] - ETA: 0s - loss: 0.1268 - accuracy: 0.9503\n",
            "Epoch 00297: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1268 - accuracy: 0.9497 - val_loss: 0.0799 - val_accuracy: 0.9630\n",
            "Epoch 298/1056\n",
            "784/821 [===========================>..] - ETA: 0s - loss: 0.1160 - accuracy: 0.9499\n",
            "Epoch 00298: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1268 - accuracy: 0.9497 - val_loss: 0.0800 - val_accuracy: 0.9623\n",
            "Epoch 299/1056\n",
            "797/821 [============================>.] - ETA: 0s - loss: 0.1289 - accuracy: 0.9492\n",
            "Epoch 00299: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1267 - accuracy: 0.9500 - val_loss: 0.0807 - val_accuracy: 0.9623\n",
            "Epoch 300/1056\n",
            "810/821 [============================>.] - ETA: 0s - loss: 0.1262 - accuracy: 0.9497\n",
            "Epoch 00300: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1270 - accuracy: 0.9494 - val_loss: 0.0804 - val_accuracy: 0.9630\n",
            "Epoch 301/1056\n",
            "802/821 [============================>.] - ETA: 0s - loss: 0.1276 - accuracy: 0.9492\n",
            "Epoch 00301: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1266 - accuracy: 0.9491 - val_loss: 0.0802 - val_accuracy: 0.9630\n",
            "Epoch 302/1056\n",
            "788/821 [===========================>..] - ETA: 0s - loss: 0.1274 - accuracy: 0.9508\n",
            "Epoch 00302: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1267 - accuracy: 0.9500 - val_loss: 0.0793 - val_accuracy: 0.9630\n",
            "Epoch 303/1056\n",
            "789/821 [===========================>..] - ETA: 0s - loss: 0.1243 - accuracy: 0.9512\n",
            "Epoch 00303: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1269 - accuracy: 0.9500 - val_loss: 0.0802 - val_accuracy: 0.9630\n",
            "Epoch 304/1056\n",
            "813/821 [============================>.] - ETA: 0s - loss: 0.1268 - accuracy: 0.9496\n",
            "Epoch 00304: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1269 - accuracy: 0.9494 - val_loss: 0.0803 - val_accuracy: 0.9637\n",
            "Epoch 305/1056\n",
            "790/821 [===========================>..] - ETA: 0s - loss: 0.1285 - accuracy: 0.9503\n",
            "Epoch 00305: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1270 - accuracy: 0.9500 - val_loss: 0.0809 - val_accuracy: 0.9623\n",
            "Epoch 306/1056\n",
            "781/821 [===========================>..] - ETA: 0s - loss: 0.1284 - accuracy: 0.9501\n",
            "Epoch 00306: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1271 - accuracy: 0.9500 - val_loss: 0.0800 - val_accuracy: 0.9616\n",
            "Epoch 307/1056\n",
            "803/821 [============================>.] - ETA: 0s - loss: 0.1283 - accuracy: 0.9496\n",
            "Epoch 00307: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1270 - accuracy: 0.9500 - val_loss: 0.0800 - val_accuracy: 0.9609\n",
            "Epoch 308/1056\n",
            "802/821 [============================>.] - ETA: 0s - loss: 0.1273 - accuracy: 0.9504\n",
            "Epoch 00308: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1270 - accuracy: 0.9500 - val_loss: 0.0801 - val_accuracy: 0.9637\n",
            "Epoch 309/1056\n",
            "782/821 [===========================>..] - ETA: 0s - loss: 0.1281 - accuracy: 0.9495\n",
            "Epoch 00309: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1270 - accuracy: 0.9494 - val_loss: 0.0805 - val_accuracy: 0.9623\n",
            "Epoch 310/1056\n",
            "783/821 [===========================>..] - ETA: 0s - loss: 0.1253 - accuracy: 0.9508\n",
            "Epoch 00310: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1270 - accuracy: 0.9494 - val_loss: 0.0804 - val_accuracy: 0.9637\n",
            "Epoch 311/1056\n",
            "778/821 [===========================>..] - ETA: 0s - loss: 0.1244 - accuracy: 0.9521\n",
            "Epoch 00311: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1273 - accuracy: 0.9506 - val_loss: 0.0804 - val_accuracy: 0.9637\n",
            "Epoch 312/1056\n",
            "797/821 [============================>.] - ETA: 0s - loss: 0.1289 - accuracy: 0.9501\n",
            "Epoch 00312: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1272 - accuracy: 0.9506 - val_loss: 0.0808 - val_accuracy: 0.9630\n",
            "Epoch 313/1056\n",
            "793/821 [===========================>..] - ETA: 0s - loss: 0.1274 - accuracy: 0.9505\n",
            "Epoch 00313: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1273 - accuracy: 0.9506 - val_loss: 0.0799 - val_accuracy: 0.9630\n",
            "Epoch 314/1056\n",
            "801/821 [============================>.] - ETA: 0s - loss: 0.1274 - accuracy: 0.9513\n",
            "Epoch 00314: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1275 - accuracy: 0.9506 - val_loss: 0.0797 - val_accuracy: 0.9623\n",
            "Epoch 315/1056\n",
            "794/821 [============================>.] - ETA: 0s - loss: 0.1267 - accuracy: 0.9515\n",
            "Epoch 00315: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1271 - accuracy: 0.9506 - val_loss: 0.0808 - val_accuracy: 0.9623\n",
            "Epoch 316/1056\n",
            "798/821 [============================>.] - ETA: 0s - loss: 0.1280 - accuracy: 0.9511\n",
            "Epoch 00316: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1279 - accuracy: 0.9509 - val_loss: 0.0803 - val_accuracy: 0.9623\n",
            "Epoch 317/1056\n",
            "790/821 [===========================>..] - ETA: 0s - loss: 0.1299 - accuracy: 0.9503\n",
            "Epoch 00317: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1274 - accuracy: 0.9506 - val_loss: 0.0808 - val_accuracy: 0.9616\n",
            "Epoch 318/1056\n",
            "779/821 [===========================>..] - ETA: 0s - loss: 0.1256 - accuracy: 0.9509\n",
            "Epoch 00318: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1279 - accuracy: 0.9497 - val_loss: 0.0807 - val_accuracy: 0.9609\n",
            "Epoch 319/1056\n",
            "780/821 [===========================>..] - ETA: 0s - loss: 0.1305 - accuracy: 0.9490\n",
            "Epoch 00319: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1279 - accuracy: 0.9497 - val_loss: 0.0817 - val_accuracy: 0.9630\n",
            "Epoch 320/1056\n",
            "782/821 [===========================>..] - ETA: 0s - loss: 0.1290 - accuracy: 0.9501\n",
            "Epoch 00320: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1280 - accuracy: 0.9500 - val_loss: 0.0812 - val_accuracy: 0.9644\n",
            "Epoch 321/1056\n",
            "785/821 [===========================>..] - ETA: 0s - loss: 0.1310 - accuracy: 0.9500\n",
            "Epoch 00321: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1280 - accuracy: 0.9506 - val_loss: 0.0808 - val_accuracy: 0.9644\n",
            "Epoch 322/1056\n",
            "775/821 [===========================>..] - ETA: 0s - loss: 0.1278 - accuracy: 0.9510\n",
            "Epoch 00322: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1281 - accuracy: 0.9500 - val_loss: 0.0805 - val_accuracy: 0.9637\n",
            "Epoch 323/1056\n",
            "818/821 [============================>.] - ETA: 0s - loss: 0.1279 - accuracy: 0.9502\n",
            "Epoch 00323: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1277 - accuracy: 0.9503 - val_loss: 0.0803 - val_accuracy: 0.9630\n",
            "Epoch 324/1056\n",
            "821/821 [==============================] - ETA: 0s - loss: 0.1277 - accuracy: 0.9494\n",
            "Epoch 00324: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1277 - accuracy: 0.9494 - val_loss: 0.0798 - val_accuracy: 0.9630\n",
            "Epoch 325/1056\n",
            "816/821 [============================>.] - ETA: 0s - loss: 0.1283 - accuracy: 0.9494\n",
            "Epoch 00325: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1277 - accuracy: 0.9497 - val_loss: 0.0799 - val_accuracy: 0.9630\n",
            "Epoch 326/1056\n",
            "817/821 [============================>.] - ETA: 0s - loss: 0.1279 - accuracy: 0.9495\n",
            "Epoch 00326: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1277 - accuracy: 0.9494 - val_loss: 0.0804 - val_accuracy: 0.9623\n",
            "Epoch 327/1056\n",
            "780/821 [===========================>..] - ETA: 0s - loss: 0.1281 - accuracy: 0.9519\n",
            "Epoch 00327: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1277 - accuracy: 0.9503 - val_loss: 0.0819 - val_accuracy: 0.9637\n",
            "Epoch 328/1056\n",
            "774/821 [===========================>..] - ETA: 0s - loss: 0.1310 - accuracy: 0.9493\n",
            "Epoch 00328: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1284 - accuracy: 0.9500 - val_loss: 0.0813 - val_accuracy: 0.9637\n",
            "Epoch 329/1056\n",
            "785/821 [===========================>..] - ETA: 0s - loss: 0.1308 - accuracy: 0.9494\n",
            "Epoch 00329: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1286 - accuracy: 0.9497 - val_loss: 0.0815 - val_accuracy: 0.9630\n",
            "Epoch 330/1056\n",
            "780/821 [===========================>..] - ETA: 0s - loss: 0.1274 - accuracy: 0.9500\n",
            "Epoch 00330: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1287 - accuracy: 0.9500 - val_loss: 0.0817 - val_accuracy: 0.9623\n",
            "Epoch 331/1056\n",
            "782/821 [===========================>..] - ETA: 0s - loss: 0.1294 - accuracy: 0.9495\n",
            "Epoch 00331: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1290 - accuracy: 0.9497 - val_loss: 0.0811 - val_accuracy: 0.9616\n",
            "Epoch 332/1056\n",
            "791/821 [===========================>..] - ETA: 0s - loss: 0.1283 - accuracy: 0.9513\n",
            "Epoch 00332: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1288 - accuracy: 0.9497 - val_loss: 0.0814 - val_accuracy: 0.9623\n",
            "Epoch 333/1056\n",
            "776/821 [===========================>..] - ETA: 0s - loss: 0.1158 - accuracy: 0.9497\n",
            "Epoch 00333: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1289 - accuracy: 0.9491 - val_loss: 0.0817 - val_accuracy: 0.9623\n",
            "Epoch 334/1056\n",
            "818/821 [============================>.] - ETA: 0s - loss: 0.1292 - accuracy: 0.9502\n",
            "Epoch 00334: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1290 - accuracy: 0.9500 - val_loss: 0.0820 - val_accuracy: 0.9623\n",
            "Epoch 335/1056\n",
            "777/821 [===========================>..] - ETA: 0s - loss: 0.1309 - accuracy: 0.9514\n",
            "Epoch 00335: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1291 - accuracy: 0.9512 - val_loss: 0.0819 - val_accuracy: 0.9637\n",
            "Epoch 336/1056\n",
            "783/821 [===========================>..] - ETA: 0s - loss: 0.1290 - accuracy: 0.9515\n",
            "Epoch 00336: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1292 - accuracy: 0.9500 - val_loss: 0.0814 - val_accuracy: 0.9637\n",
            "Epoch 337/1056\n",
            "782/821 [===========================>..] - ETA: 0s - loss: 0.1289 - accuracy: 0.9504\n",
            "Epoch 00337: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1287 - accuracy: 0.9497 - val_loss: 0.0808 - val_accuracy: 0.9609\n",
            "Epoch 338/1056\n",
            "778/821 [===========================>..] - ETA: 0s - loss: 0.1312 - accuracy: 0.9476\n",
            "Epoch 00338: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1286 - accuracy: 0.9488 - val_loss: 0.0808 - val_accuracy: 0.9623\n",
            "Epoch 339/1056\n",
            "820/821 [============================>.] - ETA: 0s - loss: 0.1285 - accuracy: 0.9497\n",
            "Epoch 00339: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1286 - accuracy: 0.9497 - val_loss: 0.0805 - val_accuracy: 0.9623\n",
            "Epoch 340/1056\n",
            "773/821 [===========================>..] - ETA: 0s - loss: 0.1334 - accuracy: 0.9479\n",
            "Epoch 00340: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1287 - accuracy: 0.9497 - val_loss: 0.0809 - val_accuracy: 0.9616\n",
            "Epoch 341/1056\n",
            "786/821 [===========================>..] - ETA: 0s - loss: 0.1318 - accuracy: 0.9494\n",
            "Epoch 00341: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1287 - accuracy: 0.9503 - val_loss: 0.0800 - val_accuracy: 0.9609\n",
            "Epoch 342/1056\n",
            "777/821 [===========================>..] - ETA: 0s - loss: 0.1305 - accuracy: 0.9492\n",
            "Epoch 00342: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1288 - accuracy: 0.9497 - val_loss: 0.0805 - val_accuracy: 0.9609\n",
            "Epoch 343/1056\n",
            "785/821 [===========================>..] - ETA: 0s - loss: 0.1314 - accuracy: 0.9494\n",
            "Epoch 00343: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1290 - accuracy: 0.9500 - val_loss: 0.0809 - val_accuracy: 0.9609\n",
            "Epoch 344/1056\n",
            "821/821 [==============================] - ETA: 0s - loss: 0.1292 - accuracy: 0.9503\n",
            "Epoch 00344: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1292 - accuracy: 0.9503 - val_loss: 0.0810 - val_accuracy: 0.9602\n",
            "Epoch 345/1056\n",
            "788/821 [===========================>..] - ETA: 0s - loss: 0.1282 - accuracy: 0.9492\n",
            "Epoch 00345: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1292 - accuracy: 0.9500 - val_loss: 0.0808 - val_accuracy: 0.9602\n",
            "Epoch 346/1056\n",
            "787/821 [===========================>..] - ETA: 0s - loss: 0.1287 - accuracy: 0.9514\n",
            "Epoch 00346: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1292 - accuracy: 0.9506 - val_loss: 0.0812 - val_accuracy: 0.9616\n",
            "Epoch 347/1056\n",
            "783/821 [===========================>..] - ETA: 0s - loss: 0.1326 - accuracy: 0.9496\n",
            "Epoch 00347: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1294 - accuracy: 0.9503 - val_loss: 0.0814 - val_accuracy: 0.9616\n",
            "Epoch 348/1056\n",
            "785/821 [===========================>..] - ETA: 0s - loss: 0.1316 - accuracy: 0.9494\n",
            "Epoch 00348: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1291 - accuracy: 0.9503 - val_loss: 0.0819 - val_accuracy: 0.9630\n",
            "Epoch 349/1056\n",
            "785/821 [===========================>..] - ETA: 0s - loss: 0.1042 - accuracy: 0.9506\n",
            "Epoch 00349: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1299 - accuracy: 0.9506 - val_loss: 0.0819 - val_accuracy: 0.9623\n",
            "Epoch 350/1056\n",
            "793/821 [===========================>..] - ETA: 0s - loss: 0.1315 - accuracy: 0.9505\n",
            "Epoch 00350: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1298 - accuracy: 0.9503 - val_loss: 0.0833 - val_accuracy: 0.9630\n",
            "Epoch 351/1056\n",
            "774/821 [===========================>..] - ETA: 0s - loss: 0.1326 - accuracy: 0.9499\n",
            "Epoch 00351: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1303 - accuracy: 0.9500 - val_loss: 0.0825 - val_accuracy: 0.9630\n",
            "Epoch 352/1056\n",
            "778/821 [===========================>..] - ETA: 0s - loss: 0.1294 - accuracy: 0.9502\n",
            "Epoch 00352: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1302 - accuracy: 0.9497 - val_loss: 0.0826 - val_accuracy: 0.9623\n",
            "Epoch 353/1056\n",
            "773/821 [===========================>..] - ETA: 0s - loss: 0.1306 - accuracy: 0.9505\n",
            "Epoch 00353: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1301 - accuracy: 0.9491 - val_loss: 0.0817 - val_accuracy: 0.9623\n",
            "Epoch 354/1056\n",
            "821/821 [==============================] - ETA: 0s - loss: 0.1302 - accuracy: 0.9494\n",
            "Epoch 00354: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1302 - accuracy: 0.9494 - val_loss: 0.0821 - val_accuracy: 0.9630\n",
            "Epoch 355/1056\n",
            "776/821 [===========================>..] - ETA: 0s - loss: 0.1332 - accuracy: 0.9488\n",
            "Epoch 00355: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1302 - accuracy: 0.9497 - val_loss: 0.0820 - val_accuracy: 0.9630\n",
            "Epoch 356/1056\n",
            "806/821 [============================>.] - ETA: 0s - loss: 0.1315 - accuracy: 0.9485\n",
            "Epoch 00356: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1303 - accuracy: 0.9488 - val_loss: 0.0819 - val_accuracy: 0.9644\n",
            "Epoch 357/1056\n",
            "798/821 [============================>.] - ETA: 0s - loss: 0.1300 - accuracy: 0.9502\n",
            "Epoch 00357: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1301 - accuracy: 0.9494 - val_loss: 0.0816 - val_accuracy: 0.9637\n",
            "Epoch 358/1056\n",
            "787/821 [===========================>..] - ETA: 0s - loss: 0.1323 - accuracy: 0.9492\n",
            "Epoch 00358: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1302 - accuracy: 0.9500 - val_loss: 0.0815 - val_accuracy: 0.9644\n",
            "Epoch 359/1056\n",
            "779/821 [===========================>..] - ETA: 0s - loss: 0.1302 - accuracy: 0.9519\n",
            "Epoch 00359: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1303 - accuracy: 0.9503 - val_loss: 0.0817 - val_accuracy: 0.9623\n",
            "Epoch 360/1056\n",
            "775/821 [===========================>..] - ETA: 0s - loss: 0.1334 - accuracy: 0.9487\n",
            "Epoch 00360: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1304 - accuracy: 0.9494 - val_loss: 0.0813 - val_accuracy: 0.9644\n",
            "Epoch 361/1056\n",
            "783/821 [===========================>..] - ETA: 0s - loss: 0.1311 - accuracy: 0.9492\n",
            "Epoch 00361: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1303 - accuracy: 0.9488 - val_loss: 0.0812 - val_accuracy: 0.9644\n",
            "Epoch 362/1056\n",
            "781/821 [===========================>..] - ETA: 0s - loss: 0.1330 - accuracy: 0.9488\n",
            "Epoch 00362: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1305 - accuracy: 0.9494 - val_loss: 0.0815 - val_accuracy: 0.9644\n",
            "Epoch 363/1056\n",
            "799/821 [============================>.] - ETA: 0s - loss: 0.1314 - accuracy: 0.9484\n",
            "Epoch 00363: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1303 - accuracy: 0.9488 - val_loss: 0.0822 - val_accuracy: 0.9630\n",
            "Epoch 364/1056\n",
            "784/821 [===========================>..] - ETA: 0s - loss: 0.1189 - accuracy: 0.9506\n",
            "Epoch 00364: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1304 - accuracy: 0.9509 - val_loss: 0.0815 - val_accuracy: 0.9616\n",
            "Epoch 365/1056\n",
            "815/821 [============================>.] - ETA: 0s - loss: 0.1307 - accuracy: 0.9506\n",
            "Epoch 00365: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1303 - accuracy: 0.9506 - val_loss: 0.0814 - val_accuracy: 0.9616\n",
            "Epoch 366/1056\n",
            "812/821 [============================>.] - ETA: 0s - loss: 0.1308 - accuracy: 0.9510\n",
            "Epoch 00366: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1304 - accuracy: 0.9512 - val_loss: 0.0814 - val_accuracy: 0.9616\n",
            "Epoch 367/1056\n",
            "808/821 [============================>.] - ETA: 0s - loss: 0.1316 - accuracy: 0.9499\n",
            "Epoch 00367: val_loss did not improve from 0.07655\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.1304 - accuracy: 0.9503 - val_loss: 0.0815 - val_accuracy: 0.9609\n",
            "Epoch 00367: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xwCKoMYkp7i",
        "colab_type": "text"
      },
      "source": [
        "Single Layer evaluation of Validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jh9Q44nAVGF7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "f1542141-b34e-4bc8-a2bd-145a80f4d244"
      },
      "source": [
        "model.load_weights('Reduced_Nasa_asteroid.hdf5')\n",
        "prediction = model.predict(XVALID)\n",
        "accuracy = accuracy_score(YVALID, prediction.round())\n",
        "precision = precision_score(YVALID, prediction.round())\n",
        "recall = recall_score(YVALID, prediction.round())\n",
        "f1score = f1_score(YVALID, prediction.round())\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
        "print(\"Precision: %.2f%%\" % (precision * 100.0))\n",
        "print(\"Recall: %.2f%%\" % (recall * 100.0))\n",
        "print(\"F1-score: %.2f\" % (f1score*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 96.51%\n",
            "Precision: 89.13%\n",
            "Recall: 89.52%\n",
            "F1-score: 89.32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_EDfYqvxCQd",
        "colab_type": "text"
      },
      "source": [
        "Learning Curve:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAR6oE246u1P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 624
        },
        "outputId": "62b9b651-126a-4068-e0dd-2bd72b3fb2b8"
      },
      "source": [
        "print(history.params)\n",
        "plt.plot(history.history['loss']) \n",
        "plt.plot(history.history['val_loss']) \n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['Training data', 'Validation data'], loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'verbose': 1, 'epochs': 1056, 'steps': 821}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDjP9To4k7Wt",
        "colab_type": "text"
      },
      "source": [
        "Evaluation on Training data set.(Form Single Layer)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y57bHxisk5Lg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "fdf581c4-7ce4-428f-c0ad-685be080faa9"
      },
      "source": [
        "prediction=model.predict(XTRAIN)\n",
        "accuracy = accuracy_score(YTRAIN, prediction.round())\n",
        "precision = precision_score(YTRAIN, prediction.round())\n",
        "recall = recall_score(YTRAIN, prediction.round())\n",
        "f1score = f1_score(YTRAIN, prediction.round())\n",
        "print(\"Training Set Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
        "print(\"Precision: %.2f%%\" % (precision * 100.0))\n",
        "print(\"Recall: %.2f%%\" % (recall * 100.0))\n",
        "print(\"F1-score: %.2f\" % (f1score*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Set Accuracy: 95.15%\n",
            "Precision: 84.82%\n",
            "Recall: 84.98%\n",
            "F1-score: 84.90\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__rk0WvszGq-",
        "colab_type": "text"
      },
      "source": [
        "## Neural Network Model with Sigmoid Activation(Multilayer) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xqdLBZfzFEl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "callback_a = ModelCheckpoint(filepath = 'Reduced_Nasa_asteroid.hdf5', monitor='val_loss', save_best_only = True, save_weights_only = True, verbose = 1)\n",
        "callback_b = EarlyStopping(monitor='val_loss', mode='min', patience=200, verbose=1)\n",
        "model = Sequential()\n",
        "model.add(Dense(24, input_dim = len(XTRAIN[0, :]), activation='sigmoid'))\n",
        "model.add(Dense(12, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYyhx6Oe3BnP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bb61f02e-7dc6-4339-eb84-b3be5ea91350"
      },
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer = 'rmsprop', metrics=['accuracy'])\n",
        "history = model.fit(XTRAIN, YTRAIN, validation_data=(XVALID, YVALID), epochs=1056, batch_size=4, callbacks = [callback_a, callback_b])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1056\n",
            "811/821 [============================>.] - ETA: 0s - loss: 0.3327 - accuracy: 0.8416\n",
            "Epoch 00001: val_loss improved from inf to 0.26326, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.3328 - accuracy: 0.8415 - val_loss: 0.2633 - val_accuracy: 0.8442\n",
            "Epoch 2/1056\n",
            "801/821 [============================>.] - ETA: 0s - loss: 0.2158 - accuracy: 0.8933\n",
            "Epoch 00002: val_loss improved from 0.26326 to 0.19158, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.2153 - accuracy: 0.8948 - val_loss: 0.1916 - val_accuracy: 0.9310\n",
            "Epoch 3/1056\n",
            "814/821 [============================>.] - ETA: 0s - loss: 0.1477 - accuracy: 0.9459\n",
            "Epoch 00003: val_loss improved from 0.19158 to 0.14208, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.1470 - accuracy: 0.9464 - val_loss: 0.1421 - val_accuracy: 0.9438\n",
            "Epoch 4/1056\n",
            "782/821 [===========================>..] - ETA: 0s - loss: 0.1122 - accuracy: 0.9568\n",
            "Epoch 00004: val_loss improved from 0.14208 to 0.10641, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.1123 - accuracy: 0.9558 - val_loss: 0.1064 - val_accuracy: 0.9573\n",
            "Epoch 5/1056\n",
            "793/821 [===========================>..] - ETA: 0s - loss: 0.0960 - accuracy: 0.9596\n",
            "Epoch 00005: val_loss improved from 0.10641 to 0.09412, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0963 - accuracy: 0.9592 - val_loss: 0.0941 - val_accuracy: 0.9595\n",
            "Epoch 6/1056\n",
            "793/821 [===========================>..] - ETA: 0s - loss: 0.0883 - accuracy: 0.9625\n",
            "Epoch 00006: val_loss improved from 0.09412 to 0.08697, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0864 - accuracy: 0.9634 - val_loss: 0.0870 - val_accuracy: 0.9616\n",
            "Epoch 7/1056\n",
            "796/821 [============================>.] - ETA: 0s - loss: 0.0810 - accuracy: 0.9667\n",
            "Epoch 00007: val_loss improved from 0.08697 to 0.07952, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0799 - accuracy: 0.9674 - val_loss: 0.0795 - val_accuracy: 0.9687\n",
            "Epoch 8/1056\n",
            "789/821 [===========================>..] - ETA: 0s - loss: 0.0748 - accuracy: 0.9696\n",
            "Epoch 00008: val_loss improved from 0.07952 to 0.07834, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0758 - accuracy: 0.9689 - val_loss: 0.0783 - val_accuracy: 0.9673\n",
            "Epoch 9/1056\n",
            "779/821 [===========================>..] - ETA: 0s - loss: 0.0688 - accuracy: 0.9721\n",
            "Epoch 00009: val_loss did not improve from 0.07834\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0727 - accuracy: 0.9714 - val_loss: 0.0844 - val_accuracy: 0.9680\n",
            "Epoch 10/1056\n",
            "818/821 [============================>.] - ETA: 0s - loss: 0.0692 - accuracy: 0.9728\n",
            "Epoch 00010: val_loss improved from 0.07834 to 0.07579, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0699 - accuracy: 0.9723 - val_loss: 0.0758 - val_accuracy: 0.9651\n",
            "Epoch 11/1056\n",
            "790/821 [===========================>..] - ETA: 0s - loss: 0.0670 - accuracy: 0.9734\n",
            "Epoch 00011: val_loss did not improve from 0.07579\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0684 - accuracy: 0.9723 - val_loss: 0.0796 - val_accuracy: 0.9708\n",
            "Epoch 12/1056\n",
            "800/821 [============================>.] - ETA: 0s - loss: 0.0675 - accuracy: 0.9756\n",
            "Epoch 00012: val_loss improved from 0.07579 to 0.07034, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0672 - accuracy: 0.9759 - val_loss: 0.0703 - val_accuracy: 0.9737\n",
            "Epoch 13/1056\n",
            "811/821 [============================>.] - ETA: 0s - loss: 0.0621 - accuracy: 0.9750\n",
            "Epoch 00013: val_loss improved from 0.07034 to 0.07006, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0623 - accuracy: 0.9747 - val_loss: 0.0701 - val_accuracy: 0.9716\n",
            "Epoch 14/1056\n",
            "803/821 [============================>.] - ETA: 0s - loss: 0.0621 - accuracy: 0.9735\n",
            "Epoch 00014: val_loss improved from 0.07006 to 0.06993, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0622 - accuracy: 0.9735 - val_loss: 0.0699 - val_accuracy: 0.9737\n",
            "Epoch 15/1056\n",
            "820/821 [============================>.] - ETA: 0s - loss: 0.0614 - accuracy: 0.9780\n",
            "Epoch 00015: val_loss improved from 0.06993 to 0.06725, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0614 - accuracy: 0.9781 - val_loss: 0.0672 - val_accuracy: 0.9744\n",
            "Epoch 16/1056\n",
            "793/821 [===========================>..] - ETA: 0s - loss: 0.0577 - accuracy: 0.9792\n",
            "Epoch 00016: val_loss did not improve from 0.06725\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0571 - accuracy: 0.9793 - val_loss: 0.0673 - val_accuracy: 0.9787\n",
            "Epoch 17/1056\n",
            "790/821 [===========================>..] - ETA: 0s - loss: 0.0549 - accuracy: 0.9785\n",
            "Epoch 00017: val_loss did not improve from 0.06725\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0562 - accuracy: 0.9781 - val_loss: 0.0681 - val_accuracy: 0.9751\n",
            "Epoch 18/1056\n",
            "788/821 [===========================>..] - ETA: 0s - loss: 0.0540 - accuracy: 0.9810\n",
            "Epoch 00018: val_loss improved from 0.06725 to 0.06256, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0550 - accuracy: 0.9805 - val_loss: 0.0626 - val_accuracy: 0.9815\n",
            "Epoch 19/1056\n",
            "811/821 [============================>.] - ETA: 0s - loss: 0.0538 - accuracy: 0.9818\n",
            "Epoch 00019: val_loss did not improve from 0.06256\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0547 - accuracy: 0.9814 - val_loss: 0.0657 - val_accuracy: 0.9815\n",
            "Epoch 20/1056\n",
            "803/821 [============================>.] - ETA: 0s - loss: 0.0526 - accuracy: 0.9810\n",
            "Epoch 00020: val_loss improved from 0.06256 to 0.05935, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0520 - accuracy: 0.9814 - val_loss: 0.0593 - val_accuracy: 0.9844\n",
            "Epoch 21/1056\n",
            "809/821 [============================>.] - ETA: 0s - loss: 0.0506 - accuracy: 0.9836\n",
            "Epoch 00021: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0504 - accuracy: 0.9835 - val_loss: 0.0617 - val_accuracy: 0.9844\n",
            "Epoch 22/1056\n",
            "801/821 [============================>.] - ETA: 0s - loss: 0.0476 - accuracy: 0.9825\n",
            "Epoch 00022: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0490 - accuracy: 0.9823 - val_loss: 0.0629 - val_accuracy: 0.9858\n",
            "Epoch 23/1056\n",
            "806/821 [============================>.] - ETA: 0s - loss: 0.0484 - accuracy: 0.9829\n",
            "Epoch 00023: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0483 - accuracy: 0.9829 - val_loss: 0.0670 - val_accuracy: 0.9787\n",
            "Epoch 24/1056\n",
            "820/821 [============================>.] - ETA: 0s - loss: 0.0495 - accuracy: 0.9832\n",
            "Epoch 00024: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0494 - accuracy: 0.9832 - val_loss: 0.0674 - val_accuracy: 0.9822\n",
            "Epoch 25/1056\n",
            "817/821 [============================>.] - ETA: 0s - loss: 0.0490 - accuracy: 0.9850\n",
            "Epoch 00025: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0489 - accuracy: 0.9851 - val_loss: 0.0682 - val_accuracy: 0.9808\n",
            "Epoch 26/1056\n",
            "786/821 [===========================>..] - ETA: 0s - loss: 0.0471 - accuracy: 0.9851\n",
            "Epoch 00026: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0483 - accuracy: 0.9848 - val_loss: 0.0692 - val_accuracy: 0.9758\n",
            "Epoch 27/1056\n",
            "797/821 [============================>.] - ETA: 0s - loss: 0.0481 - accuracy: 0.9856\n",
            "Epoch 00027: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0472 - accuracy: 0.9860 - val_loss: 0.0681 - val_accuracy: 0.9822\n",
            "Epoch 28/1056\n",
            "800/821 [============================>.] - ETA: 0s - loss: 0.0475 - accuracy: 0.9847\n",
            "Epoch 00028: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0467 - accuracy: 0.9848 - val_loss: 0.0701 - val_accuracy: 0.9822\n",
            "Epoch 29/1056\n",
            "787/821 [===========================>..] - ETA: 0s - loss: 0.0442 - accuracy: 0.9892\n",
            "Epoch 00029: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0441 - accuracy: 0.9890 - val_loss: 0.0686 - val_accuracy: 0.9829\n",
            "Epoch 30/1056\n",
            "793/821 [===========================>..] - ETA: 0s - loss: 0.0442 - accuracy: 0.9883\n",
            "Epoch 00030: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0434 - accuracy: 0.9884 - val_loss: 0.0707 - val_accuracy: 0.9865\n",
            "Epoch 31/1056\n",
            "802/821 [============================>.] - ETA: 0s - loss: 0.0459 - accuracy: 0.9866\n",
            "Epoch 00031: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0449 - accuracy: 0.9869 - val_loss: 0.0647 - val_accuracy: 0.9829\n",
            "Epoch 32/1056\n",
            "808/821 [============================>.] - ETA: 0s - loss: 0.0449 - accuracy: 0.9889\n",
            "Epoch 00032: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0446 - accuracy: 0.9887 - val_loss: 0.0673 - val_accuracy: 0.9829\n",
            "Epoch 33/1056\n",
            "799/821 [============================>.] - ETA: 0s - loss: 0.0434 - accuracy: 0.9894\n",
            "Epoch 00033: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0429 - accuracy: 0.9893 - val_loss: 0.0724 - val_accuracy: 0.9844\n",
            "Epoch 34/1056\n",
            "801/821 [============================>.] - ETA: 0s - loss: 0.0432 - accuracy: 0.9900\n",
            "Epoch 00034: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0424 - accuracy: 0.9902 - val_loss: 0.0769 - val_accuracy: 0.9794\n",
            "Epoch 35/1056\n",
            "802/821 [============================>.] - ETA: 0s - loss: 0.0397 - accuracy: 0.9913\n",
            "Epoch 00035: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0391 - accuracy: 0.9915 - val_loss: 0.0668 - val_accuracy: 0.9851\n",
            "Epoch 36/1056\n",
            "805/821 [============================>.] - ETA: 0s - loss: 0.0418 - accuracy: 0.9885\n",
            "Epoch 00036: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0414 - accuracy: 0.9887 - val_loss: 0.0710 - val_accuracy: 0.9858\n",
            "Epoch 37/1056\n",
            "795/821 [============================>.] - ETA: 0s - loss: 0.0407 - accuracy: 0.9896\n",
            "Epoch 00037: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0400 - accuracy: 0.9896 - val_loss: 0.0678 - val_accuracy: 0.9844\n",
            "Epoch 38/1056\n",
            "807/821 [============================>.] - ETA: 0s - loss: 0.0398 - accuracy: 0.9910\n",
            "Epoch 00038: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0392 - accuracy: 0.9912 - val_loss: 0.0716 - val_accuracy: 0.9844\n",
            "Epoch 39/1056\n",
            "805/821 [============================>.] - ETA: 0s - loss: 0.0320 - accuracy: 0.9891\n",
            "Epoch 00039: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0406 - accuracy: 0.9890 - val_loss: 0.0700 - val_accuracy: 0.9836\n",
            "Epoch 40/1056\n",
            "798/821 [============================>.] - ETA: 0s - loss: 0.0393 - accuracy: 0.9915\n",
            "Epoch 00040: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0393 - accuracy: 0.9912 - val_loss: 0.0691 - val_accuracy: 0.9844\n",
            "Epoch 41/1056\n",
            "783/821 [===========================>..] - ETA: 0s - loss: 0.0406 - accuracy: 0.9907\n",
            "Epoch 00041: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0396 - accuracy: 0.9909 - val_loss: 0.0758 - val_accuracy: 0.9844\n",
            "Epoch 42/1056\n",
            "818/821 [============================>.] - ETA: 0s - loss: 0.0403 - accuracy: 0.9908\n",
            "Epoch 00042: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0402 - accuracy: 0.9909 - val_loss: 0.0712 - val_accuracy: 0.9865\n",
            "Epoch 43/1056\n",
            "797/821 [============================>.] - ETA: 0s - loss: 0.0400 - accuracy: 0.9912\n",
            "Epoch 00043: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0399 - accuracy: 0.9909 - val_loss: 0.0686 - val_accuracy: 0.9844\n",
            "Epoch 44/1056\n",
            "799/821 [============================>.] - ETA: 0s - loss: 0.0391 - accuracy: 0.9925\n",
            "Epoch 00044: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0397 - accuracy: 0.9918 - val_loss: 0.0733 - val_accuracy: 0.9844\n",
            "Epoch 45/1056\n",
            "790/821 [===========================>..] - ETA: 0s - loss: 0.0387 - accuracy: 0.9918\n",
            "Epoch 00045: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0390 - accuracy: 0.9918 - val_loss: 0.0793 - val_accuracy: 0.9851\n",
            "Epoch 46/1056\n",
            "816/821 [============================>.] - ETA: 0s - loss: 0.0404 - accuracy: 0.9908\n",
            "Epoch 00046: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0402 - accuracy: 0.9909 - val_loss: 0.0783 - val_accuracy: 0.9801\n",
            "Epoch 47/1056\n",
            "786/821 [===========================>..] - ETA: 0s - loss: 0.0402 - accuracy: 0.9917\n",
            "Epoch 00047: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0394 - accuracy: 0.9915 - val_loss: 0.0704 - val_accuracy: 0.9865\n",
            "Epoch 48/1056\n",
            "821/821 [==============================] - ETA: 0s - loss: 0.0378 - accuracy: 0.9906\n",
            "Epoch 00048: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0378 - accuracy: 0.9906 - val_loss: 0.0712 - val_accuracy: 0.9851\n",
            "Epoch 49/1056\n",
            "786/821 [===========================>..] - ETA: 0s - loss: 0.0393 - accuracy: 0.9911\n",
            "Epoch 00049: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0407 - accuracy: 0.9909 - val_loss: 0.0741 - val_accuracy: 0.9844\n",
            "Epoch 50/1056\n",
            "786/821 [===========================>..] - ETA: 0s - loss: 0.0391 - accuracy: 0.9927\n",
            "Epoch 00050: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0386 - accuracy: 0.9927 - val_loss: 0.0747 - val_accuracy: 0.9865\n",
            "Epoch 51/1056\n",
            "792/821 [===========================>..] - ETA: 0s - loss: 0.0396 - accuracy: 0.9927\n",
            "Epoch 00051: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0389 - accuracy: 0.9927 - val_loss: 0.0792 - val_accuracy: 0.9865\n",
            "Epoch 52/1056\n",
            "781/821 [===========================>..] - ETA: 0s - loss: 0.0377 - accuracy: 0.9930\n",
            "Epoch 00052: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0389 - accuracy: 0.9927 - val_loss: 0.0805 - val_accuracy: 0.9844\n",
            "Epoch 53/1056\n",
            "799/821 [============================>.] - ETA: 0s - loss: 0.0401 - accuracy: 0.9912\n",
            "Epoch 00053: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0392 - accuracy: 0.9915 - val_loss: 0.0806 - val_accuracy: 0.9844\n",
            "Epoch 54/1056\n",
            "789/821 [===========================>..] - ETA: 0s - loss: 0.0416 - accuracy: 0.9905\n",
            "Epoch 00054: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0403 - accuracy: 0.9906 - val_loss: 0.0773 - val_accuracy: 0.9858\n",
            "Epoch 55/1056\n",
            "791/821 [===========================>..] - ETA: 0s - loss: 0.0387 - accuracy: 0.9927\n",
            "Epoch 00055: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0384 - accuracy: 0.9927 - val_loss: 0.0737 - val_accuracy: 0.9851\n",
            "Epoch 56/1056\n",
            "791/821 [===========================>..] - ETA: 0s - loss: 0.0358 - accuracy: 0.9924\n",
            "Epoch 00056: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0380 - accuracy: 0.9921 - val_loss: 0.0827 - val_accuracy: 0.9844\n",
            "Epoch 57/1056\n",
            "815/821 [============================>.] - ETA: 0s - loss: 0.0404 - accuracy: 0.9917\n",
            "Epoch 00057: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0404 - accuracy: 0.9915 - val_loss: 0.0852 - val_accuracy: 0.9822\n",
            "Epoch 58/1056\n",
            "817/821 [============================>.] - ETA: 0s - loss: 0.0388 - accuracy: 0.9920\n",
            "Epoch 00058: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0387 - accuracy: 0.9921 - val_loss: 0.0790 - val_accuracy: 0.9836\n",
            "Epoch 59/1056\n",
            "816/821 [============================>.] - ETA: 0s - loss: 0.0378 - accuracy: 0.9936\n",
            "Epoch 00059: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0376 - accuracy: 0.9936 - val_loss: 0.0835 - val_accuracy: 0.9801\n",
            "Epoch 60/1056\n",
            "812/821 [============================>.] - ETA: 0s - loss: 0.0403 - accuracy: 0.9929\n",
            "Epoch 00060: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0399 - accuracy: 0.9930 - val_loss: 0.0853 - val_accuracy: 0.9851\n",
            "Epoch 61/1056\n",
            "786/821 [===========================>..] - ETA: 0s - loss: 0.0408 - accuracy: 0.9933\n",
            "Epoch 00061: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0395 - accuracy: 0.9933 - val_loss: 0.0784 - val_accuracy: 0.9865\n",
            "Epoch 62/1056\n",
            "811/821 [============================>.] - ETA: 0s - loss: 0.0355 - accuracy: 0.9941\n",
            "Epoch 00062: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0371 - accuracy: 0.9939 - val_loss: 0.0766 - val_accuracy: 0.9829\n",
            "Epoch 63/1056\n",
            "797/821 [============================>.] - ETA: 0s - loss: 0.0380 - accuracy: 0.9928\n",
            "Epoch 00063: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0375 - accuracy: 0.9924 - val_loss: 0.0777 - val_accuracy: 0.9858\n",
            "Epoch 64/1056\n",
            "783/821 [===========================>..] - ETA: 0s - loss: 0.0403 - accuracy: 0.9923\n",
            "Epoch 00064: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0389 - accuracy: 0.9924 - val_loss: 0.0838 - val_accuracy: 0.9844\n",
            "Epoch 65/1056\n",
            "782/821 [===========================>..] - ETA: 0s - loss: 0.0415 - accuracy: 0.9910\n",
            "Epoch 00065: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0399 - accuracy: 0.9915 - val_loss: 0.0852 - val_accuracy: 0.9858\n",
            "Epoch 66/1056\n",
            "794/821 [============================>.] - ETA: 0s - loss: 0.0337 - accuracy: 0.9924\n",
            "Epoch 00066: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0378 - accuracy: 0.9921 - val_loss: 0.0829 - val_accuracy: 0.9844\n",
            "Epoch 67/1056\n",
            "795/821 [============================>.] - ETA: 0s - loss: 0.0371 - accuracy: 0.9937\n",
            "Epoch 00067: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0366 - accuracy: 0.9936 - val_loss: 0.0875 - val_accuracy: 0.9829\n",
            "Epoch 68/1056\n",
            "786/821 [===========================>..] - ETA: 0s - loss: 0.0389 - accuracy: 0.9940\n",
            "Epoch 00068: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0403 - accuracy: 0.9936 - val_loss: 0.0848 - val_accuracy: 0.9829\n",
            "Epoch 69/1056\n",
            "781/821 [===========================>..] - ETA: 0s - loss: 0.0399 - accuracy: 0.9920\n",
            "Epoch 00069: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0422 - accuracy: 0.9921 - val_loss: 0.0865 - val_accuracy: 0.9851\n",
            "Epoch 70/1056\n",
            "804/821 [============================>.] - ETA: 0s - loss: 0.0389 - accuracy: 0.9938\n",
            "Epoch 00070: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0382 - accuracy: 0.9939 - val_loss: 0.0843 - val_accuracy: 0.9865\n",
            "Epoch 71/1056\n",
            "821/821 [==============================] - ETA: 0s - loss: 0.0369 - accuracy: 0.9930\n",
            "Epoch 00071: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0369 - accuracy: 0.9930 - val_loss: 0.0900 - val_accuracy: 0.9836\n",
            "Epoch 72/1056\n",
            "787/821 [===========================>..] - ETA: 0s - loss: 0.0411 - accuracy: 0.9927\n",
            "Epoch 00072: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0396 - accuracy: 0.9930 - val_loss: 0.0805 - val_accuracy: 0.9858\n",
            "Epoch 73/1056\n",
            "794/821 [============================>.] - ETA: 0s - loss: 0.0406 - accuracy: 0.9918\n",
            "Epoch 00073: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0396 - accuracy: 0.9921 - val_loss: 0.0812 - val_accuracy: 0.9858\n",
            "Epoch 74/1056\n",
            "781/821 [===========================>..] - ETA: 0s - loss: 0.0412 - accuracy: 0.9920\n",
            "Epoch 00074: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0398 - accuracy: 0.9921 - val_loss: 0.0817 - val_accuracy: 0.9865\n",
            "Epoch 75/1056\n",
            "793/821 [===========================>..] - ETA: 0s - loss: 0.0393 - accuracy: 0.9924\n",
            "Epoch 00075: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0394 - accuracy: 0.9924 - val_loss: 0.0867 - val_accuracy: 0.9851\n",
            "Epoch 76/1056\n",
            "784/821 [===========================>..] - ETA: 0s - loss: 0.0402 - accuracy: 0.9923\n",
            "Epoch 00076: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0388 - accuracy: 0.9927 - val_loss: 0.0899 - val_accuracy: 0.9836\n",
            "Epoch 77/1056\n",
            "799/821 [============================>.] - ETA: 0s - loss: 0.0418 - accuracy: 0.9931\n",
            "Epoch 00077: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0410 - accuracy: 0.9933 - val_loss: 0.0880 - val_accuracy: 0.9836\n",
            "Epoch 78/1056\n",
            "792/821 [===========================>..] - ETA: 0s - loss: 0.0393 - accuracy: 0.9915\n",
            "Epoch 00078: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0382 - accuracy: 0.9918 - val_loss: 0.0925 - val_accuracy: 0.9836\n",
            "Epoch 79/1056\n",
            "803/821 [============================>.] - ETA: 0s - loss: 0.0421 - accuracy: 0.9935\n",
            "Epoch 00079: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0413 - accuracy: 0.9936 - val_loss: 0.0878 - val_accuracy: 0.9858\n",
            "Epoch 80/1056\n",
            "793/821 [===========================>..] - ETA: 0s - loss: 0.0421 - accuracy: 0.9921\n",
            "Epoch 00080: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0407 - accuracy: 0.9924 - val_loss: 0.0904 - val_accuracy: 0.9836\n",
            "Epoch 81/1056\n",
            "790/821 [===========================>..] - ETA: 0s - loss: 0.0413 - accuracy: 0.9940\n",
            "Epoch 00081: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0412 - accuracy: 0.9936 - val_loss: 0.0860 - val_accuracy: 0.9851\n",
            "Epoch 82/1056\n",
            "789/821 [===========================>..] - ETA: 0s - loss: 0.0443 - accuracy: 0.9921\n",
            "Epoch 00082: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0427 - accuracy: 0.9924 - val_loss: 0.0889 - val_accuracy: 0.9851\n",
            "Epoch 83/1056\n",
            "813/821 [============================>.] - ETA: 0s - loss: 0.0436 - accuracy: 0.9929\n",
            "Epoch 00083: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0433 - accuracy: 0.9930 - val_loss: 0.0938 - val_accuracy: 0.9844\n",
            "Epoch 84/1056\n",
            "791/821 [===========================>..] - ETA: 0s - loss: 0.0432 - accuracy: 0.9927\n",
            "Epoch 00084: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0417 - accuracy: 0.9930 - val_loss: 0.0922 - val_accuracy: 0.9836\n",
            "Epoch 85/1056\n",
            "820/821 [============================>.] - ETA: 0s - loss: 0.0411 - accuracy: 0.9927\n",
            "Epoch 00085: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0411 - accuracy: 0.9927 - val_loss: 0.0880 - val_accuracy: 0.9829\n",
            "Epoch 86/1056\n",
            "782/821 [===========================>..] - ETA: 0s - loss: 0.0407 - accuracy: 0.9936\n",
            "Epoch 00086: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0415 - accuracy: 0.9933 - val_loss: 0.0926 - val_accuracy: 0.9858\n",
            "Epoch 87/1056\n",
            "783/821 [===========================>..] - ETA: 0s - loss: 0.0453 - accuracy: 0.9923\n",
            "Epoch 00087: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0434 - accuracy: 0.9927 - val_loss: 0.0941 - val_accuracy: 0.9851\n",
            "Epoch 88/1056\n",
            "813/821 [============================>.] - ETA: 0s - loss: 0.0411 - accuracy: 0.9929\n",
            "Epoch 00088: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0409 - accuracy: 0.9930 - val_loss: 0.0936 - val_accuracy: 0.9836\n",
            "Epoch 89/1056\n",
            "789/821 [===========================>..] - ETA: 0s - loss: 0.0435 - accuracy: 0.9933\n",
            "Epoch 00089: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0423 - accuracy: 0.9933 - val_loss: 0.0937 - val_accuracy: 0.9829\n",
            "Epoch 90/1056\n",
            "796/821 [============================>.] - ETA: 0s - loss: 0.0391 - accuracy: 0.9931\n",
            "Epoch 00090: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0400 - accuracy: 0.9927 - val_loss: 0.0862 - val_accuracy: 0.9815\n",
            "Epoch 91/1056\n",
            "783/821 [===========================>..] - ETA: 0s - loss: 0.0411 - accuracy: 0.9920\n",
            "Epoch 00091: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0393 - accuracy: 0.9924 - val_loss: 0.0926 - val_accuracy: 0.9829\n",
            "Epoch 92/1056\n",
            "802/821 [============================>.] - ETA: 0s - loss: 0.0397 - accuracy: 0.9919\n",
            "Epoch 00092: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0391 - accuracy: 0.9918 - val_loss: 0.0978 - val_accuracy: 0.9829\n",
            "Epoch 93/1056\n",
            "803/821 [============================>.] - ETA: 0s - loss: 0.0441 - accuracy: 0.9922\n",
            "Epoch 00093: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0432 - accuracy: 0.9924 - val_loss: 0.1096 - val_accuracy: 0.9801\n",
            "Epoch 94/1056\n",
            "800/821 [============================>.] - ETA: 0s - loss: 0.0425 - accuracy: 0.9934\n",
            "Epoch 00094: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0416 - accuracy: 0.9936 - val_loss: 0.1021 - val_accuracy: 0.9815\n",
            "Epoch 95/1056\n",
            "798/821 [============================>.] - ETA: 0s - loss: 0.0412 - accuracy: 0.9934\n",
            "Epoch 00095: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0401 - accuracy: 0.9936 - val_loss: 0.0983 - val_accuracy: 0.9808\n",
            "Epoch 96/1056\n",
            "796/821 [============================>.] - ETA: 0s - loss: 0.0441 - accuracy: 0.9937\n",
            "Epoch 00096: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0428 - accuracy: 0.9939 - val_loss: 0.1019 - val_accuracy: 0.9815\n",
            "Epoch 97/1056\n",
            "796/821 [============================>.] - ETA: 0s - loss: 0.0440 - accuracy: 0.9925\n",
            "Epoch 00097: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0428 - accuracy: 0.9927 - val_loss: 0.1092 - val_accuracy: 0.9822\n",
            "Epoch 98/1056\n",
            "782/821 [===========================>..] - ETA: 0s - loss: 0.0451 - accuracy: 0.9933\n",
            "Epoch 00098: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0446 - accuracy: 0.9933 - val_loss: 0.0975 - val_accuracy: 0.9836\n",
            "Epoch 99/1056\n",
            "795/821 [============================>.] - ETA: 0s - loss: 0.0432 - accuracy: 0.9925\n",
            "Epoch 00099: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0419 - accuracy: 0.9927 - val_loss: 0.1111 - val_accuracy: 0.9808\n",
            "Epoch 100/1056\n",
            "782/821 [===========================>..] - ETA: 0s - loss: 0.0418 - accuracy: 0.9930\n",
            "Epoch 00100: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0413 - accuracy: 0.9930 - val_loss: 0.1005 - val_accuracy: 0.9822\n",
            "Epoch 101/1056\n",
            "794/821 [============================>.] - ETA: 0s - loss: 0.0281 - accuracy: 0.9928\n",
            "Epoch 00101: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0426 - accuracy: 0.9921 - val_loss: 0.1046 - val_accuracy: 0.9808\n",
            "Epoch 102/1056\n",
            "789/821 [===========================>..] - ETA: 0s - loss: 0.0438 - accuracy: 0.9930\n",
            "Epoch 00102: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0427 - accuracy: 0.9930 - val_loss: 0.1024 - val_accuracy: 0.9815\n",
            "Epoch 103/1056\n",
            "788/821 [===========================>..] - ETA: 0s - loss: 0.0427 - accuracy: 0.9930\n",
            "Epoch 00103: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0410 - accuracy: 0.9933 - val_loss: 0.1069 - val_accuracy: 0.9808\n",
            "Epoch 104/1056\n",
            "802/821 [============================>.] - ETA: 0s - loss: 0.0438 - accuracy: 0.9938\n",
            "Epoch 00104: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0439 - accuracy: 0.9936 - val_loss: 0.1048 - val_accuracy: 0.9836\n",
            "Epoch 105/1056\n",
            "802/821 [============================>.] - ETA: 0s - loss: 0.0385 - accuracy: 0.9941\n",
            "Epoch 00105: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0411 - accuracy: 0.9936 - val_loss: 0.1052 - val_accuracy: 0.9829\n",
            "Epoch 106/1056\n",
            "806/821 [============================>.] - ETA: 0s - loss: 0.0440 - accuracy: 0.9929\n",
            "Epoch 00106: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0436 - accuracy: 0.9927 - val_loss: 0.1278 - val_accuracy: 0.9822\n",
            "Epoch 107/1056\n",
            "789/821 [===========================>..] - ETA: 0s - loss: 0.0466 - accuracy: 0.9927\n",
            "Epoch 00107: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0450 - accuracy: 0.9930 - val_loss: 0.1091 - val_accuracy: 0.9801\n",
            "Epoch 108/1056\n",
            "796/821 [============================>.] - ETA: 0s - loss: 0.0456 - accuracy: 0.9943\n",
            "Epoch 00108: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0464 - accuracy: 0.9939 - val_loss: 0.1107 - val_accuracy: 0.9829\n",
            "Epoch 109/1056\n",
            "821/821 [==============================] - ETA: 0s - loss: 0.0459 - accuracy: 0.9939\n",
            "Epoch 00109: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0459 - accuracy: 0.9939 - val_loss: 0.1230 - val_accuracy: 0.9822\n",
            "Epoch 110/1056\n",
            "789/821 [===========================>..] - ETA: 0s - loss: 0.0458 - accuracy: 0.9927\n",
            "Epoch 00110: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0463 - accuracy: 0.9924 - val_loss: 0.1146 - val_accuracy: 0.9815\n",
            "Epoch 111/1056\n",
            "795/821 [============================>.] - ETA: 0s - loss: 0.0438 - accuracy: 0.9937\n",
            "Epoch 00111: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0425 - accuracy: 0.9939 - val_loss: 0.1351 - val_accuracy: 0.9822\n",
            "Epoch 112/1056\n",
            "797/821 [============================>.] - ETA: 0s - loss: 0.0474 - accuracy: 0.9937\n",
            "Epoch 00112: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0461 - accuracy: 0.9939 - val_loss: 0.1227 - val_accuracy: 0.9787\n",
            "Epoch 113/1056\n",
            "792/821 [===========================>..] - ETA: 0s - loss: 0.0462 - accuracy: 0.9937\n",
            "Epoch 00113: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0446 - accuracy: 0.9939 - val_loss: 0.1188 - val_accuracy: 0.9801\n",
            "Epoch 114/1056\n",
            "817/821 [============================>.] - ETA: 0s - loss: 0.0448 - accuracy: 0.9936\n",
            "Epoch 00114: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0464 - accuracy: 0.9933 - val_loss: 0.1153 - val_accuracy: 0.9794\n",
            "Epoch 115/1056\n",
            "791/821 [===========================>..] - ETA: 0s - loss: 0.0447 - accuracy: 0.9934\n",
            "Epoch 00115: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0438 - accuracy: 0.9933 - val_loss: 0.1179 - val_accuracy: 0.9808\n",
            "Epoch 116/1056\n",
            "791/821 [===========================>..] - ETA: 0s - loss: 0.0426 - accuracy: 0.9943\n",
            "Epoch 00116: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0435 - accuracy: 0.9939 - val_loss: 0.1163 - val_accuracy: 0.9815\n",
            "Epoch 117/1056\n",
            "797/821 [============================>.] - ETA: 0s - loss: 0.0377 - accuracy: 0.9937\n",
            "Epoch 00117: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0423 - accuracy: 0.9936 - val_loss: 0.1210 - val_accuracy: 0.9787\n",
            "Epoch 118/1056\n",
            "811/821 [============================>.] - ETA: 0s - loss: 0.0439 - accuracy: 0.9945\n",
            "Epoch 00118: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0435 - accuracy: 0.9945 - val_loss: 0.1220 - val_accuracy: 0.9801\n",
            "Epoch 119/1056\n",
            "813/821 [============================>.] - ETA: 0s - loss: 0.0413 - accuracy: 0.9938\n",
            "Epoch 00119: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0409 - accuracy: 0.9939 - val_loss: 0.1220 - val_accuracy: 0.9794\n",
            "Epoch 120/1056\n",
            "792/821 [===========================>..] - ETA: 0s - loss: 0.0486 - accuracy: 0.9931\n",
            "Epoch 00120: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0469 - accuracy: 0.9933 - val_loss: 0.1243 - val_accuracy: 0.9808\n",
            "Epoch 121/1056\n",
            "821/821 [==============================] - ETA: 0s - loss: 0.0468 - accuracy: 0.9933\n",
            "Epoch 00121: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0468 - accuracy: 0.9933 - val_loss: 0.1377 - val_accuracy: 0.9794\n",
            "Epoch 122/1056\n",
            "807/821 [============================>.] - ETA: 0s - loss: 0.0466 - accuracy: 0.9935\n",
            "Epoch 00122: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0465 - accuracy: 0.9933 - val_loss: 0.1508 - val_accuracy: 0.9794\n",
            "Epoch 123/1056\n",
            "783/821 [===========================>..] - ETA: 0s - loss: 0.0319 - accuracy: 0.9939\n",
            "Epoch 00123: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0457 - accuracy: 0.9939 - val_loss: 0.1240 - val_accuracy: 0.9801\n",
            "Epoch 124/1056\n",
            "792/821 [===========================>..] - ETA: 0s - loss: 0.0441 - accuracy: 0.9940\n",
            "Epoch 00124: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0437 - accuracy: 0.9939 - val_loss: 0.1374 - val_accuracy: 0.9822\n",
            "Epoch 125/1056\n",
            "790/821 [===========================>..] - ETA: 0s - loss: 0.0474 - accuracy: 0.9930\n",
            "Epoch 00125: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0457 - accuracy: 0.9933 - val_loss: 0.1479 - val_accuracy: 0.9815\n",
            "Epoch 126/1056\n",
            "797/821 [============================>.] - ETA: 0s - loss: 0.0456 - accuracy: 0.9940\n",
            "Epoch 00126: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0445 - accuracy: 0.9942 - val_loss: 0.1267 - val_accuracy: 0.9765\n",
            "Epoch 127/1056\n",
            "779/821 [===========================>..] - ETA: 0s - loss: 0.0468 - accuracy: 0.9933\n",
            "Epoch 00127: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0450 - accuracy: 0.9933 - val_loss: 0.1410 - val_accuracy: 0.9794\n",
            "Epoch 128/1056\n",
            "779/821 [===========================>..] - ETA: 0s - loss: 0.0415 - accuracy: 0.9939\n",
            "Epoch 00128: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0395 - accuracy: 0.9942 - val_loss: 0.1347 - val_accuracy: 0.9801\n",
            "Epoch 129/1056\n",
            "819/821 [============================>.] - ETA: 0s - loss: 0.0423 - accuracy: 0.9933\n",
            "Epoch 00129: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0422 - accuracy: 0.9933 - val_loss: 0.1389 - val_accuracy: 0.9815\n",
            "Epoch 130/1056\n",
            "789/821 [===========================>..] - ETA: 0s - loss: 0.0474 - accuracy: 0.9930\n",
            "Epoch 00130: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0457 - accuracy: 0.9933 - val_loss: 0.1323 - val_accuracy: 0.9794\n",
            "Epoch 131/1056\n",
            "819/821 [============================>.] - ETA: 0s - loss: 0.0434 - accuracy: 0.9936\n",
            "Epoch 00131: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0434 - accuracy: 0.9936 - val_loss: 0.1427 - val_accuracy: 0.9836\n",
            "Epoch 132/1056\n",
            "787/821 [===========================>..] - ETA: 0s - loss: 0.0482 - accuracy: 0.9933\n",
            "Epoch 00132: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0462 - accuracy: 0.9936 - val_loss: 0.1356 - val_accuracy: 0.9829\n",
            "Epoch 133/1056\n",
            "790/821 [===========================>..] - ETA: 0s - loss: 0.0464 - accuracy: 0.9924\n",
            "Epoch 00133: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0448 - accuracy: 0.9927 - val_loss: 0.1496 - val_accuracy: 0.9787\n",
            "Epoch 134/1056\n",
            "821/821 [==============================] - ETA: 0s - loss: 0.0445 - accuracy: 0.9942\n",
            "Epoch 00134: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0445 - accuracy: 0.9942 - val_loss: 0.1370 - val_accuracy: 0.9808\n",
            "Epoch 135/1056\n",
            "782/821 [===========================>..] - ETA: 0s - loss: 0.0438 - accuracy: 0.9936\n",
            "Epoch 00135: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0420 - accuracy: 0.9939 - val_loss: 0.1290 - val_accuracy: 0.9801\n",
            "Epoch 136/1056\n",
            "789/821 [===========================>..] - ETA: 0s - loss: 0.0453 - accuracy: 0.9946\n",
            "Epoch 00136: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0436 - accuracy: 0.9948 - val_loss: 0.1385 - val_accuracy: 0.9765\n",
            "Epoch 137/1056\n",
            "780/821 [===========================>..] - ETA: 0s - loss: 0.0415 - accuracy: 0.9946\n",
            "Epoch 00137: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0418 - accuracy: 0.9936 - val_loss: 0.1417 - val_accuracy: 0.9787\n",
            "Epoch 138/1056\n",
            "816/821 [============================>.] - ETA: 0s - loss: 0.0450 - accuracy: 0.9942\n",
            "Epoch 00138: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0448 - accuracy: 0.9942 - val_loss: 0.1489 - val_accuracy: 0.9772\n",
            "Epoch 139/1056\n",
            "785/821 [===========================>..] - ETA: 0s - loss: 0.0387 - accuracy: 0.9943\n",
            "Epoch 00139: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0381 - accuracy: 0.9939 - val_loss: 0.1474 - val_accuracy: 0.9758\n",
            "Epoch 140/1056\n",
            "819/821 [============================>.] - ETA: 0s - loss: 0.0406 - accuracy: 0.9942\n",
            "Epoch 00140: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0405 - accuracy: 0.9942 - val_loss: 0.1430 - val_accuracy: 0.9780\n",
            "Epoch 141/1056\n",
            "819/821 [============================>.] - ETA: 0s - loss: 0.0412 - accuracy: 0.9939\n",
            "Epoch 00141: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0411 - accuracy: 0.9939 - val_loss: 0.1356 - val_accuracy: 0.9794\n",
            "Epoch 142/1056\n",
            "789/821 [===========================>..] - ETA: 0s - loss: 0.0356 - accuracy: 0.9946\n",
            "Epoch 00142: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0343 - accuracy: 0.9948 - val_loss: 0.1534 - val_accuracy: 0.9787\n",
            "Epoch 143/1056\n",
            "789/821 [===========================>..] - ETA: 0s - loss: 0.0420 - accuracy: 0.9937\n",
            "Epoch 00143: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0409 - accuracy: 0.9936 - val_loss: 0.1511 - val_accuracy: 0.9794\n",
            "Epoch 144/1056\n",
            "795/821 [============================>.] - ETA: 0s - loss: 0.0436 - accuracy: 0.9943\n",
            "Epoch 00144: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0423 - accuracy: 0.9945 - val_loss: 0.1423 - val_accuracy: 0.9780\n",
            "Epoch 145/1056\n",
            "787/821 [===========================>..] - ETA: 0s - loss: 0.0415 - accuracy: 0.9946\n",
            "Epoch 00145: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0412 - accuracy: 0.9945 - val_loss: 0.1407 - val_accuracy: 0.9794\n",
            "Epoch 146/1056\n",
            "820/821 [============================>.] - ETA: 0s - loss: 0.0414 - accuracy: 0.9948\n",
            "Epoch 00146: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0414 - accuracy: 0.9948 - val_loss: 0.1672 - val_accuracy: 0.9787\n",
            "Epoch 147/1056\n",
            "787/821 [===========================>..] - ETA: 0s - loss: 0.0417 - accuracy: 0.9946\n",
            "Epoch 00147: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0421 - accuracy: 0.9942 - val_loss: 0.1508 - val_accuracy: 0.9801\n",
            "Epoch 148/1056\n",
            "797/821 [============================>.] - ETA: 0s - loss: 0.0416 - accuracy: 0.9944\n",
            "Epoch 00148: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0409 - accuracy: 0.9942 - val_loss: 0.1578 - val_accuracy: 0.9808\n",
            "Epoch 149/1056\n",
            "794/821 [============================>.] - ETA: 0s - loss: 0.0443 - accuracy: 0.9953\n",
            "Epoch 00149: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0434 - accuracy: 0.9951 - val_loss: 0.1500 - val_accuracy: 0.9794\n",
            "Epoch 150/1056\n",
            "788/821 [===========================>..] - ETA: 0s - loss: 0.0432 - accuracy: 0.9946\n",
            "Epoch 00150: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0424 - accuracy: 0.9945 - val_loss: 0.1569 - val_accuracy: 0.9787\n",
            "Epoch 151/1056\n",
            "820/821 [============================>.] - ETA: 0s - loss: 0.0397 - accuracy: 0.9933\n",
            "Epoch 00151: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0397 - accuracy: 0.9933 - val_loss: 0.1632 - val_accuracy: 0.9780\n",
            "Epoch 152/1056\n",
            "790/821 [===========================>..] - ETA: 0s - loss: 0.0442 - accuracy: 0.9937\n",
            "Epoch 00152: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0426 - accuracy: 0.9939 - val_loss: 0.1515 - val_accuracy: 0.9801\n",
            "Epoch 153/1056\n",
            "790/821 [===========================>..] - ETA: 0s - loss: 0.0448 - accuracy: 0.9940\n",
            "Epoch 00153: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0432 - accuracy: 0.9942 - val_loss: 0.1598 - val_accuracy: 0.9780\n",
            "Epoch 154/1056\n",
            "797/821 [============================>.] - ETA: 0s - loss: 0.0434 - accuracy: 0.9950\n",
            "Epoch 00154: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0428 - accuracy: 0.9948 - val_loss: 0.1492 - val_accuracy: 0.9780\n",
            "Epoch 155/1056\n",
            "793/821 [===========================>..] - ETA: 0s - loss: 0.0451 - accuracy: 0.9937\n",
            "Epoch 00155: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0436 - accuracy: 0.9939 - val_loss: 0.1622 - val_accuracy: 0.9772\n",
            "Epoch 156/1056\n",
            "819/821 [============================>.] - ETA: 0s - loss: 0.0424 - accuracy: 0.9942\n",
            "Epoch 00156: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0423 - accuracy: 0.9942 - val_loss: 0.1624 - val_accuracy: 0.9801\n",
            "Epoch 157/1056\n",
            "793/821 [===========================>..] - ETA: 0s - loss: 0.0422 - accuracy: 0.9946\n",
            "Epoch 00157: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0408 - accuracy: 0.9948 - val_loss: 0.1625 - val_accuracy: 0.9772\n",
            "Epoch 158/1056\n",
            "792/821 [===========================>..] - ETA: 0s - loss: 0.0362 - accuracy: 0.9946\n",
            "Epoch 00158: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0418 - accuracy: 0.9942 - val_loss: 0.1549 - val_accuracy: 0.9751\n",
            "Epoch 159/1056\n",
            "781/821 [===========================>..] - ETA: 0s - loss: 0.0438 - accuracy: 0.9946\n",
            "Epoch 00159: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0418 - accuracy: 0.9948 - val_loss: 0.1682 - val_accuracy: 0.9787\n",
            "Epoch 160/1056\n",
            "790/821 [===========================>..] - ETA: 0s - loss: 0.0409 - accuracy: 0.9940\n",
            "Epoch 00160: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0395 - accuracy: 0.9942 - val_loss: 0.1577 - val_accuracy: 0.9765\n",
            "Epoch 161/1056\n",
            "792/821 [===========================>..] - ETA: 0s - loss: 0.0433 - accuracy: 0.9940\n",
            "Epoch 00161: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0418 - accuracy: 0.9942 - val_loss: 0.1629 - val_accuracy: 0.9765\n",
            "Epoch 162/1056\n",
            "819/821 [============================>.] - ETA: 0s - loss: 0.0385 - accuracy: 0.9942\n",
            "Epoch 00162: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0384 - accuracy: 0.9942 - val_loss: 0.1575 - val_accuracy: 0.9772\n",
            "Epoch 163/1056\n",
            "802/821 [============================>.] - ETA: 0s - loss: 0.0379 - accuracy: 0.9947\n",
            "Epoch 00163: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 2s 2ms/step - loss: 0.0390 - accuracy: 0.9945 - val_loss: 0.1599 - val_accuracy: 0.9772\n",
            "Epoch 164/1056\n",
            "793/821 [===========================>..] - ETA: 0s - loss: 0.0412 - accuracy: 0.9937\n",
            "Epoch 00164: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0399 - accuracy: 0.9939 - val_loss: 0.1587 - val_accuracy: 0.9772\n",
            "Epoch 165/1056\n",
            "796/821 [============================>.] - ETA: 0s - loss: 0.0395 - accuracy: 0.9950\n",
            "Epoch 00165: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0384 - accuracy: 0.9951 - val_loss: 0.1479 - val_accuracy: 0.9780\n",
            "Epoch 166/1056\n",
            "796/821 [============================>.] - ETA: 0s - loss: 0.0411 - accuracy: 0.9943\n",
            "Epoch 00166: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0399 - accuracy: 0.9945 - val_loss: 0.1524 - val_accuracy: 0.9808\n",
            "Epoch 167/1056\n",
            "792/821 [===========================>..] - ETA: 0s - loss: 0.0392 - accuracy: 0.9953\n",
            "Epoch 00167: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0381 - accuracy: 0.9951 - val_loss: 0.1560 - val_accuracy: 0.9780\n",
            "Epoch 168/1056\n",
            "785/821 [===========================>..] - ETA: 0s - loss: 0.0385 - accuracy: 0.9930\n",
            "Epoch 00168: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0375 - accuracy: 0.9927 - val_loss: 0.1655 - val_accuracy: 0.9780\n",
            "Epoch 169/1056\n",
            "786/821 [===========================>..] - ETA: 0s - loss: 0.0233 - accuracy: 0.9952\n",
            "Epoch 00169: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0350 - accuracy: 0.9951 - val_loss: 0.1499 - val_accuracy: 0.9780\n",
            "Epoch 170/1056\n",
            "787/821 [===========================>..] - ETA: 0s - loss: 0.0387 - accuracy: 0.9946\n",
            "Epoch 00170: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0382 - accuracy: 0.9945 - val_loss: 0.1753 - val_accuracy: 0.9794\n",
            "Epoch 171/1056\n",
            "809/821 [============================>.] - ETA: 0s - loss: 0.0400 - accuracy: 0.9951\n",
            "Epoch 00171: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0394 - accuracy: 0.9951 - val_loss: 0.1742 - val_accuracy: 0.9780\n",
            "Epoch 172/1056\n",
            "816/821 [============================>.] - ETA: 0s - loss: 0.0401 - accuracy: 0.9945\n",
            "Epoch 00172: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0398 - accuracy: 0.9945 - val_loss: 0.1666 - val_accuracy: 0.9787\n",
            "Epoch 173/1056\n",
            "783/821 [===========================>..] - ETA: 0s - loss: 0.0415 - accuracy: 0.9946\n",
            "Epoch 00173: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0396 - accuracy: 0.9948 - val_loss: 0.1670 - val_accuracy: 0.9780\n",
            "Epoch 174/1056\n",
            "821/821 [==============================] - ETA: 0s - loss: 0.0381 - accuracy: 0.9945\n",
            "Epoch 00174: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0381 - accuracy: 0.9945 - val_loss: 0.1594 - val_accuracy: 0.9772\n",
            "Epoch 175/1056\n",
            "820/821 [============================>.] - ETA: 0s - loss: 0.0404 - accuracy: 0.9939\n",
            "Epoch 00175: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0404 - accuracy: 0.9939 - val_loss: 0.1642 - val_accuracy: 0.9780\n",
            "Epoch 176/1056\n",
            "808/821 [============================>.] - ETA: 0s - loss: 0.0388 - accuracy: 0.9957\n",
            "Epoch 00176: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0383 - accuracy: 0.9957 - val_loss: 0.1638 - val_accuracy: 0.9787\n",
            "Epoch 177/1056\n",
            "796/821 [============================>.] - ETA: 0s - loss: 0.0381 - accuracy: 0.9937\n",
            "Epoch 00177: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0370 - accuracy: 0.9939 - val_loss: 0.1588 - val_accuracy: 0.9787\n",
            "Epoch 178/1056\n",
            "818/821 [============================>.] - ETA: 0s - loss: 0.0390 - accuracy: 0.9945\n",
            "Epoch 00178: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0389 - accuracy: 0.9945 - val_loss: 0.1581 - val_accuracy: 0.9772\n",
            "Epoch 179/1056\n",
            "790/821 [===========================>..] - ETA: 0s - loss: 0.0389 - accuracy: 0.9940\n",
            "Epoch 00179: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0374 - accuracy: 0.9942 - val_loss: 0.1722 - val_accuracy: 0.9772\n",
            "Epoch 180/1056\n",
            "798/821 [============================>.] - ETA: 0s - loss: 0.0375 - accuracy: 0.9947\n",
            "Epoch 00180: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0368 - accuracy: 0.9945 - val_loss: 0.1666 - val_accuracy: 0.9780\n",
            "Epoch 181/1056\n",
            "816/821 [============================>.] - ETA: 0s - loss: 0.0359 - accuracy: 0.9948\n",
            "Epoch 00181: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0357 - accuracy: 0.9948 - val_loss: 0.1600 - val_accuracy: 0.9772\n",
            "Epoch 182/1056\n",
            "793/821 [===========================>..] - ETA: 0s - loss: 0.0382 - accuracy: 0.9940\n",
            "Epoch 00182: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0371 - accuracy: 0.9942 - val_loss: 0.1614 - val_accuracy: 0.9794\n",
            "Epoch 183/1056\n",
            "791/821 [===========================>..] - ETA: 0s - loss: 0.0331 - accuracy: 0.9962\n",
            "Epoch 00183: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0357 - accuracy: 0.9954 - val_loss: 0.1472 - val_accuracy: 0.9780\n",
            "Epoch 184/1056\n",
            "821/821 [==============================] - ETA: 0s - loss: 0.0338 - accuracy: 0.9957\n",
            "Epoch 00184: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0338 - accuracy: 0.9957 - val_loss: 0.1496 - val_accuracy: 0.9787\n",
            "Epoch 185/1056\n",
            "812/821 [============================>.] - ETA: 0s - loss: 0.0368 - accuracy: 0.9954\n",
            "Epoch 00185: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0364 - accuracy: 0.9954 - val_loss: 0.1597 - val_accuracy: 0.9801\n",
            "Epoch 186/1056\n",
            "798/821 [============================>.] - ETA: 0s - loss: 0.0360 - accuracy: 0.9959\n",
            "Epoch 00186: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0350 - accuracy: 0.9960 - val_loss: 0.1493 - val_accuracy: 0.9787\n",
            "Epoch 187/1056\n",
            "798/821 [============================>.] - ETA: 0s - loss: 0.0367 - accuracy: 0.9950\n",
            "Epoch 00187: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0357 - accuracy: 0.9951 - val_loss: 0.1647 - val_accuracy: 0.9801\n",
            "Epoch 188/1056\n",
            "809/821 [============================>.] - ETA: 0s - loss: 0.0335 - accuracy: 0.9944\n",
            "Epoch 00188: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0331 - accuracy: 0.9945 - val_loss: 0.1608 - val_accuracy: 0.9794\n",
            "Epoch 189/1056\n",
            "811/821 [============================>.] - ETA: 0s - loss: 0.0341 - accuracy: 0.9960\n",
            "Epoch 00189: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0338 - accuracy: 0.9960 - val_loss: 0.1731 - val_accuracy: 0.9787\n",
            "Epoch 190/1056\n",
            "780/821 [===========================>..] - ETA: 0s - loss: 0.0390 - accuracy: 0.9952\n",
            "Epoch 00190: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0371 - accuracy: 0.9954 - val_loss: 0.1697 - val_accuracy: 0.9772\n",
            "Epoch 191/1056\n",
            "794/821 [============================>.] - ETA: 0s - loss: 0.0352 - accuracy: 0.9959\n",
            "Epoch 00191: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0367 - accuracy: 0.9957 - val_loss: 0.1886 - val_accuracy: 0.9794\n",
            "Epoch 192/1056\n",
            "815/821 [============================>.] - ETA: 0s - loss: 0.0364 - accuracy: 0.9957\n",
            "Epoch 00192: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0362 - accuracy: 0.9957 - val_loss: 0.1733 - val_accuracy: 0.9794\n",
            "Epoch 193/1056\n",
            "791/821 [===========================>..] - ETA: 0s - loss: 0.0356 - accuracy: 0.9959\n",
            "Epoch 00193: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0357 - accuracy: 0.9957 - val_loss: 0.1635 - val_accuracy: 0.9758\n",
            "Epoch 194/1056\n",
            "781/821 [===========================>..] - ETA: 0s - loss: 0.0370 - accuracy: 0.9955\n",
            "Epoch 00194: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0353 - accuracy: 0.9957 - val_loss: 0.1718 - val_accuracy: 0.9780\n",
            "Epoch 195/1056\n",
            "784/821 [===========================>..] - ETA: 0s - loss: 0.0374 - accuracy: 0.9952\n",
            "Epoch 00195: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0357 - accuracy: 0.9954 - val_loss: 0.1591 - val_accuracy: 0.9794\n",
            "Epoch 196/1056\n",
            "811/821 [============================>.] - ETA: 0s - loss: 0.0337 - accuracy: 0.9960\n",
            "Epoch 00196: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0333 - accuracy: 0.9960 - val_loss: 0.2031 - val_accuracy: 0.9772\n",
            "Epoch 197/1056\n",
            "799/821 [============================>.] - ETA: 0s - loss: 0.0367 - accuracy: 0.9962\n",
            "Epoch 00197: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0357 - accuracy: 0.9963 - val_loss: 0.1732 - val_accuracy: 0.9801\n",
            "Epoch 198/1056\n",
            "817/821 [============================>.] - ETA: 0s - loss: 0.0356 - accuracy: 0.9957\n",
            "Epoch 00198: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0355 - accuracy: 0.9957 - val_loss: 0.1652 - val_accuracy: 0.9787\n",
            "Epoch 199/1056\n",
            "782/821 [===========================>..] - ETA: 0s - loss: 0.0332 - accuracy: 0.9955\n",
            "Epoch 00199: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0329 - accuracy: 0.9954 - val_loss: 0.1508 - val_accuracy: 0.9780\n",
            "Epoch 200/1056\n",
            "804/821 [============================>.] - ETA: 0s - loss: 0.0342 - accuracy: 0.9953\n",
            "Epoch 00200: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0336 - accuracy: 0.9954 - val_loss: 0.1576 - val_accuracy: 0.9765\n",
            "Epoch 201/1056\n",
            "808/821 [============================>.] - ETA: 0s - loss: 0.0360 - accuracy: 0.9960\n",
            "Epoch 00201: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0354 - accuracy: 0.9960 - val_loss: 0.1646 - val_accuracy: 0.9794\n",
            "Epoch 202/1056\n",
            "800/821 [============================>.] - ETA: 0s - loss: 0.0382 - accuracy: 0.9953\n",
            "Epoch 00202: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0372 - accuracy: 0.9954 - val_loss: 0.1691 - val_accuracy: 0.9787\n",
            "Epoch 203/1056\n",
            "795/821 [============================>.] - ETA: 0s - loss: 0.0385 - accuracy: 0.9947\n",
            "Epoch 00203: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0373 - accuracy: 0.9948 - val_loss: 0.1807 - val_accuracy: 0.9787\n",
            "Epoch 204/1056\n",
            "808/821 [============================>.] - ETA: 0s - loss: 0.0399 - accuracy: 0.9950\n",
            "Epoch 00204: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0406 - accuracy: 0.9948 - val_loss: 0.1710 - val_accuracy: 0.9780\n",
            "Epoch 205/1056\n",
            "794/821 [============================>.] - ETA: 0s - loss: 0.0403 - accuracy: 0.9950\n",
            "Epoch 00205: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0390 - accuracy: 0.9951 - val_loss: 0.1710 - val_accuracy: 0.9787\n",
            "Epoch 206/1056\n",
            "813/821 [============================>.] - ETA: 0s - loss: 0.0407 - accuracy: 0.9954\n",
            "Epoch 00206: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0404 - accuracy: 0.9954 - val_loss: 0.1714 - val_accuracy: 0.9772\n",
            "Epoch 207/1056\n",
            "819/821 [============================>.] - ETA: 0s - loss: 0.0384 - accuracy: 0.9957\n",
            "Epoch 00207: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0384 - accuracy: 0.9957 - val_loss: 0.1720 - val_accuracy: 0.9801\n",
            "Epoch 208/1056\n",
            "791/821 [===========================>..] - ETA: 0s - loss: 0.0383 - accuracy: 0.9959\n",
            "Epoch 00208: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0385 - accuracy: 0.9957 - val_loss: 0.1869 - val_accuracy: 0.9787\n",
            "Epoch 209/1056\n",
            "799/821 [============================>.] - ETA: 0s - loss: 0.0407 - accuracy: 0.9962\n",
            "Epoch 00209: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0399 - accuracy: 0.9960 - val_loss: 0.1742 - val_accuracy: 0.9780\n",
            "Epoch 210/1056\n",
            "813/821 [============================>.] - ETA: 0s - loss: 0.0393 - accuracy: 0.9951\n",
            "Epoch 00210: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0390 - accuracy: 0.9951 - val_loss: 0.1797 - val_accuracy: 0.9772\n",
            "Epoch 211/1056\n",
            "806/821 [============================>.] - ETA: 0s - loss: 0.0403 - accuracy: 0.9963\n",
            "Epoch 00211: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0413 - accuracy: 0.9957 - val_loss: 0.1784 - val_accuracy: 0.9772\n",
            "Epoch 212/1056\n",
            "788/821 [===========================>..] - ETA: 0s - loss: 0.0412 - accuracy: 0.9952\n",
            "Epoch 00212: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0413 - accuracy: 0.9951 - val_loss: 0.1834 - val_accuracy: 0.9780\n",
            "Epoch 213/1056\n",
            "792/821 [===========================>..] - ETA: 0s - loss: 0.0407 - accuracy: 0.9953\n",
            "Epoch 00213: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0400 - accuracy: 0.9951 - val_loss: 0.1849 - val_accuracy: 0.9787\n",
            "Epoch 214/1056\n",
            "789/821 [===========================>..] - ETA: 0s - loss: 0.0420 - accuracy: 0.9946\n",
            "Epoch 00214: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0405 - accuracy: 0.9948 - val_loss: 0.1772 - val_accuracy: 0.9772\n",
            "Epoch 215/1056\n",
            "798/821 [============================>.] - ETA: 0s - loss: 0.0408 - accuracy: 0.9950\n",
            "Epoch 00215: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0397 - accuracy: 0.9951 - val_loss: 0.1843 - val_accuracy: 0.9772\n",
            "Epoch 216/1056\n",
            "809/821 [============================>.] - ETA: 0s - loss: 0.0413 - accuracy: 0.9941\n",
            "Epoch 00216: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0408 - accuracy: 0.9942 - val_loss: 0.1711 - val_accuracy: 0.9794\n",
            "Epoch 217/1056\n",
            "793/821 [===========================>..] - ETA: 0s - loss: 0.0383 - accuracy: 0.9953\n",
            "Epoch 00217: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0370 - accuracy: 0.9954 - val_loss: 0.1986 - val_accuracy: 0.9787\n",
            "Epoch 218/1056\n",
            "812/821 [============================>.] - ETA: 0s - loss: 0.0405 - accuracy: 0.9957\n",
            "Epoch 00218: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0401 - accuracy: 0.9957 - val_loss: 0.1836 - val_accuracy: 0.9801\n",
            "Epoch 219/1056\n",
            "786/821 [===========================>..] - ETA: 0s - loss: 0.0430 - accuracy: 0.9959\n",
            "Epoch 00219: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0412 - accuracy: 0.9960 - val_loss: 0.1884 - val_accuracy: 0.9787\n",
            "Epoch 220/1056\n",
            "789/821 [===========================>..] - ETA: 0s - loss: 0.0383 - accuracy: 0.9946\n",
            "Epoch 00220: val_loss did not improve from 0.05935\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0386 - accuracy: 0.9945 - val_loss: 0.1809 - val_accuracy: 0.9780\n",
            "Epoch 00220: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68YHH9PEdPyq",
        "colab_type": "text"
      },
      "source": [
        "Evaluation on Validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVSIK9bQuXBo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpG_Srxay7Uq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "7873b271-b238-45e5-b1a6-2dda29b4e5f1"
      },
      "source": [
        "model.load_weights('Reduced_Nasa_asteroid.hdf5')\n",
        "prediction = model.predict(XVALID)\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "accuracy = accuracy_score(YVALID, prediction.round())\n",
        "precision = precision_score(YVALID, prediction.round())\n",
        "recall = recall_score(YVALID, prediction.round())\n",
        "f1score = f1_score(YVALID, prediction.round())\n",
        "print(\"Multilayer NN Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
        "print(\"Precision: %.2f%%\" % (precision * 100.0))\n",
        "print(\"Recall: %.2f%%\" % (recall * 100.0))\n",
        "print(\"F1-score: %.2f\" % (f1score*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Multilayer NN Accuracy: 98.44%\n",
            "Precision: 96.51%\n",
            "Recall: 94.04%\n",
            "F1-score: 95.26\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoPnYN-1tSM_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 623
        },
        "outputId": "647c4f9b-fd8a-48c0-fdd5-cd8880d38a02"
      },
      "source": [
        "print(history.params)\n",
        "plt.plot(history.history['loss']) \n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['Training data', 'Validation data'], loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'verbose': 1, 'epochs': 1056, 'steps': 821}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t69GMw-adXV5",
        "colab_type": "text"
      },
      "source": [
        "evaluation on Training model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkevEIOSddL3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "0020301e-6e13-4148-b891-1971246b4a89"
      },
      "source": [
        "prediction=model.predict(XTRAIN)\n",
        "accuracy = accuracy_score(YTRAIN, prediction.round())\n",
        "precision = precision_score(YTRAIN, prediction.round())\n",
        "recall = recall_score(YTRAIN, prediction.round())\n",
        "f1score = f1_score(YTRAIN, prediction.round())\n",
        "print(\"Training Set Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
        "print(\"Precision: %.2f%%\" % (precision * 100.0))\n",
        "print(\"Recall: %.2f%%\" % (recall * 100.0))\n",
        "print(\"F1-score: %.2f\" % (f1score))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Set Accuracy: 98.63%\n",
            "Precision: 97.41%\n",
            "Recall: 93.85%\n",
            "F1-score: 0.96\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmLtnKpCKg_c",
        "colab_type": "text"
      },
      "source": [
        "##3 Layers - with increased number of neurons and linear activation function in last layer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wEVn8pJK6kS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3f6857b5-8688-42ae-ad12-53cfeb4bf512"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(64, input_dim = len(XTRAIN[0, :]), activation='sigmoid'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer = 'rmsprop', metrics=['accuracy'])\n",
        "history = model.fit(XTRAIN, YTRAIN, validation_data=(XVALID, YVALID),epochs = 1056, verbose = 1 )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1056\n",
            "103/103 [==============================] - 0s 3ms/step - loss: 0.3756 - accuracy: 0.8372 - val_loss: 0.2940 - val_accuracy: 0.8599\n",
            "Epoch 2/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.2812 - accuracy: 0.8714 - val_loss: 0.2409 - val_accuracy: 0.8947\n",
            "Epoch 3/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.2265 - accuracy: 0.9037 - val_loss: 0.1994 - val_accuracy: 0.9168\n",
            "Epoch 4/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.1851 - accuracy: 0.9220 - val_loss: 0.1761 - val_accuracy: 0.9275\n",
            "Epoch 5/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.1540 - accuracy: 0.9418 - val_loss: 0.1800 - val_accuracy: 0.9239\n",
            "Epoch 6/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.1346 - accuracy: 0.9485 - val_loss: 0.1311 - val_accuracy: 0.9431\n",
            "Epoch 7/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.1201 - accuracy: 0.9540 - val_loss: 0.1184 - val_accuracy: 0.9403\n",
            "Epoch 8/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.1113 - accuracy: 0.9543 - val_loss: 0.1124 - val_accuracy: 0.9459\n",
            "Epoch 9/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.1049 - accuracy: 0.9585 - val_loss: 0.1056 - val_accuracy: 0.9538\n",
            "Epoch 10/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.1014 - accuracy: 0.9607 - val_loss: 0.1107 - val_accuracy: 0.9431\n",
            "Epoch 11/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0964 - accuracy: 0.9601 - val_loss: 0.0992 - val_accuracy: 0.9580\n",
            "Epoch 12/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0938 - accuracy: 0.9607 - val_loss: 0.0968 - val_accuracy: 0.9538\n",
            "Epoch 13/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0929 - accuracy: 0.9622 - val_loss: 0.1091 - val_accuracy: 0.9424\n",
            "Epoch 14/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0894 - accuracy: 0.9640 - val_loss: 0.1013 - val_accuracy: 0.9481\n",
            "Epoch 15/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0877 - accuracy: 0.9601 - val_loss: 0.0975 - val_accuracy: 0.9516\n",
            "Epoch 16/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0842 - accuracy: 0.9653 - val_loss: 0.0877 - val_accuracy: 0.9616\n",
            "Epoch 17/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0814 - accuracy: 0.9677 - val_loss: 0.0945 - val_accuracy: 0.9495\n",
            "Epoch 18/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0787 - accuracy: 0.9692 - val_loss: 0.0820 - val_accuracy: 0.9623\n",
            "Epoch 19/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0772 - accuracy: 0.9683 - val_loss: 0.1052 - val_accuracy: 0.9481\n",
            "Epoch 20/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0750 - accuracy: 0.9692 - val_loss: 0.0846 - val_accuracy: 0.9587\n",
            "Epoch 21/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0734 - accuracy: 0.9665 - val_loss: 0.0806 - val_accuracy: 0.9666\n",
            "Epoch 22/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0704 - accuracy: 0.9704 - val_loss: 0.0803 - val_accuracy: 0.9630\n",
            "Epoch 23/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0677 - accuracy: 0.9689 - val_loss: 0.0806 - val_accuracy: 0.9716\n",
            "Epoch 24/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0666 - accuracy: 0.9698 - val_loss: 0.0757 - val_accuracy: 0.9673\n",
            "Epoch 25/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0651 - accuracy: 0.9726 - val_loss: 0.0775 - val_accuracy: 0.9637\n",
            "Epoch 26/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0630 - accuracy: 0.9747 - val_loss: 0.0664 - val_accuracy: 0.9751\n",
            "Epoch 27/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0610 - accuracy: 0.9747 - val_loss: 0.0680 - val_accuracy: 0.9708\n",
            "Epoch 28/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0581 - accuracy: 0.9759 - val_loss: 0.0927 - val_accuracy: 0.9566\n",
            "Epoch 29/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0583 - accuracy: 0.9756 - val_loss: 0.0661 - val_accuracy: 0.9708\n",
            "Epoch 30/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0549 - accuracy: 0.9774 - val_loss: 0.0642 - val_accuracy: 0.9801\n",
            "Epoch 31/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0531 - accuracy: 0.9787 - val_loss: 0.0635 - val_accuracy: 0.9716\n",
            "Epoch 32/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0511 - accuracy: 0.9774 - val_loss: 0.0620 - val_accuracy: 0.9794\n",
            "Epoch 33/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0492 - accuracy: 0.9811 - val_loss: 0.0614 - val_accuracy: 0.9716\n",
            "Epoch 34/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0475 - accuracy: 0.9799 - val_loss: 0.0559 - val_accuracy: 0.9794\n",
            "Epoch 35/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0455 - accuracy: 0.9829 - val_loss: 0.0660 - val_accuracy: 0.9708\n",
            "Epoch 36/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0450 - accuracy: 0.9826 - val_loss: 0.0571 - val_accuracy: 0.9751\n",
            "Epoch 37/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0424 - accuracy: 0.9848 - val_loss: 0.0531 - val_accuracy: 0.9815\n",
            "Epoch 38/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0422 - accuracy: 0.9838 - val_loss: 0.0561 - val_accuracy: 0.9758\n",
            "Epoch 39/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0403 - accuracy: 0.9860 - val_loss: 0.0562 - val_accuracy: 0.9737\n",
            "Epoch 40/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0393 - accuracy: 0.9857 - val_loss: 0.0495 - val_accuracy: 0.9829\n",
            "Epoch 41/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0377 - accuracy: 0.9857 - val_loss: 0.0454 - val_accuracy: 0.9858\n",
            "Epoch 42/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0353 - accuracy: 0.9884 - val_loss: 0.0510 - val_accuracy: 0.9808\n",
            "Epoch 43/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0351 - accuracy: 0.9896 - val_loss: 0.0462 - val_accuracy: 0.9865\n",
            "Epoch 44/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0356 - accuracy: 0.9857 - val_loss: 0.0453 - val_accuracy: 0.9829\n",
            "Epoch 45/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0333 - accuracy: 0.9878 - val_loss: 0.0425 - val_accuracy: 0.9858\n",
            "Epoch 46/1056\n",
            "103/103 [==============================] - 0s 4ms/step - loss: 0.0325 - accuracy: 0.9890 - val_loss: 0.0541 - val_accuracy: 0.9780\n",
            "Epoch 47/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0317 - accuracy: 0.9893 - val_loss: 0.0415 - val_accuracy: 0.9872\n",
            "Epoch 48/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0309 - accuracy: 0.9896 - val_loss: 0.0409 - val_accuracy: 0.9858\n",
            "Epoch 49/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0294 - accuracy: 0.9909 - val_loss: 0.0396 - val_accuracy: 0.9851\n",
            "Epoch 50/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0288 - accuracy: 0.9915 - val_loss: 0.0382 - val_accuracy: 0.9865\n",
            "Epoch 51/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0283 - accuracy: 0.9906 - val_loss: 0.0387 - val_accuracy: 0.9858\n",
            "Epoch 52/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0286 - accuracy: 0.9902 - val_loss: 0.0364 - val_accuracy: 0.9893\n",
            "Epoch 53/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0267 - accuracy: 0.9912 - val_loss: 0.0400 - val_accuracy: 0.9865\n",
            "Epoch 54/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0255 - accuracy: 0.9899 - val_loss: 0.0392 - val_accuracy: 0.9829\n",
            "Epoch 55/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0258 - accuracy: 0.9915 - val_loss: 0.0368 - val_accuracy: 0.9879\n",
            "Epoch 56/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0261 - accuracy: 0.9912 - val_loss: 0.0372 - val_accuracy: 0.9851\n",
            "Epoch 57/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0239 - accuracy: 0.9921 - val_loss: 0.0351 - val_accuracy: 0.9851\n",
            "Epoch 58/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0239 - accuracy: 0.9918 - val_loss: 0.0338 - val_accuracy: 0.9900\n",
            "Epoch 59/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0235 - accuracy: 0.9927 - val_loss: 0.0323 - val_accuracy: 0.9915\n",
            "Epoch 60/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0223 - accuracy: 0.9933 - val_loss: 0.0483 - val_accuracy: 0.9822\n",
            "Epoch 61/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0225 - accuracy: 0.9933 - val_loss: 0.0353 - val_accuracy: 0.9865\n",
            "Epoch 62/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0217 - accuracy: 0.9927 - val_loss: 0.0364 - val_accuracy: 0.9844\n",
            "Epoch 63/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0217 - accuracy: 0.9921 - val_loss: 0.0337 - val_accuracy: 0.9858\n",
            "Epoch 64/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0214 - accuracy: 0.9927 - val_loss: 0.0320 - val_accuracy: 0.9879\n",
            "Epoch 65/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0196 - accuracy: 0.9930 - val_loss: 0.0310 - val_accuracy: 0.9900\n",
            "Epoch 66/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0192 - accuracy: 0.9936 - val_loss: 0.0335 - val_accuracy: 0.9872\n",
            "Epoch 67/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0201 - accuracy: 0.9930 - val_loss: 0.0316 - val_accuracy: 0.9879\n",
            "Epoch 68/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0195 - accuracy: 0.9933 - val_loss: 0.0318 - val_accuracy: 0.9865\n",
            "Epoch 69/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0186 - accuracy: 0.9939 - val_loss: 0.0331 - val_accuracy: 0.9872\n",
            "Epoch 70/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0193 - accuracy: 0.9942 - val_loss: 0.0313 - val_accuracy: 0.9893\n",
            "Epoch 71/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0189 - accuracy: 0.9927 - val_loss: 0.0313 - val_accuracy: 0.9893\n",
            "Epoch 72/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0181 - accuracy: 0.9939 - val_loss: 0.0321 - val_accuracy: 0.9858\n",
            "Epoch 73/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0177 - accuracy: 0.9954 - val_loss: 0.0310 - val_accuracy: 0.9872\n",
            "Epoch 74/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0179 - accuracy: 0.9939 - val_loss: 0.0300 - val_accuracy: 0.9872\n",
            "Epoch 75/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0176 - accuracy: 0.9942 - val_loss: 0.0282 - val_accuracy: 0.9886\n",
            "Epoch 76/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0172 - accuracy: 0.9951 - val_loss: 0.0308 - val_accuracy: 0.9872\n",
            "Epoch 77/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0168 - accuracy: 0.9942 - val_loss: 0.0308 - val_accuracy: 0.9886\n",
            "Epoch 78/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0169 - accuracy: 0.9933 - val_loss: 0.0342 - val_accuracy: 0.9836\n",
            "Epoch 79/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0158 - accuracy: 0.9945 - val_loss: 0.0264 - val_accuracy: 0.9915\n",
            "Epoch 80/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0153 - accuracy: 0.9948 - val_loss: 0.0269 - val_accuracy: 0.9922\n",
            "Epoch 81/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0159 - accuracy: 0.9948 - val_loss: 0.0377 - val_accuracy: 0.9844\n",
            "Epoch 82/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0156 - accuracy: 0.9939 - val_loss: 0.0272 - val_accuracy: 0.9886\n",
            "Epoch 83/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0157 - accuracy: 0.9957 - val_loss: 0.0277 - val_accuracy: 0.9900\n",
            "Epoch 84/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0147 - accuracy: 0.9951 - val_loss: 0.0320 - val_accuracy: 0.9879\n",
            "Epoch 85/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0152 - accuracy: 0.9951 - val_loss: 0.0304 - val_accuracy: 0.9879\n",
            "Epoch 86/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0145 - accuracy: 0.9951 - val_loss: 0.0256 - val_accuracy: 0.9929\n",
            "Epoch 87/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0144 - accuracy: 0.9957 - val_loss: 0.0285 - val_accuracy: 0.9893\n",
            "Epoch 88/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0146 - accuracy: 0.9939 - val_loss: 0.0275 - val_accuracy: 0.9900\n",
            "Epoch 89/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0139 - accuracy: 0.9951 - val_loss: 0.0284 - val_accuracy: 0.9900\n",
            "Epoch 90/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0143 - accuracy: 0.9942 - val_loss: 0.0345 - val_accuracy: 0.9865\n",
            "Epoch 91/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0126 - accuracy: 0.9966 - val_loss: 0.0354 - val_accuracy: 0.9822\n",
            "Epoch 92/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0140 - accuracy: 0.9954 - val_loss: 0.0327 - val_accuracy: 0.9893\n",
            "Epoch 93/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0128 - accuracy: 0.9951 - val_loss: 0.0275 - val_accuracy: 0.9900\n",
            "Epoch 94/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0130 - accuracy: 0.9960 - val_loss: 0.0261 - val_accuracy: 0.9922\n",
            "Epoch 95/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0130 - accuracy: 0.9948 - val_loss: 0.0258 - val_accuracy: 0.9929\n",
            "Epoch 96/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0130 - accuracy: 0.9951 - val_loss: 0.0271 - val_accuracy: 0.9900\n",
            "Epoch 97/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0132 - accuracy: 0.9954 - val_loss: 0.0281 - val_accuracy: 0.9900\n",
            "Epoch 98/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0131 - accuracy: 0.9960 - val_loss: 0.0312 - val_accuracy: 0.9893\n",
            "Epoch 99/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0125 - accuracy: 0.9951 - val_loss: 0.0294 - val_accuracy: 0.9872\n",
            "Epoch 100/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0121 - accuracy: 0.9957 - val_loss: 0.0257 - val_accuracy: 0.9922\n",
            "Epoch 101/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0120 - accuracy: 0.9939 - val_loss: 0.0275 - val_accuracy: 0.9886\n",
            "Epoch 102/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0120 - accuracy: 0.9960 - val_loss: 0.0304 - val_accuracy: 0.9886\n",
            "Epoch 103/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0115 - accuracy: 0.9970 - val_loss: 0.0261 - val_accuracy: 0.9915\n",
            "Epoch 104/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0118 - accuracy: 0.9963 - val_loss: 0.0281 - val_accuracy: 0.9908\n",
            "Epoch 105/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0111 - accuracy: 0.9973 - val_loss: 0.0264 - val_accuracy: 0.9900\n",
            "Epoch 106/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0115 - accuracy: 0.9957 - val_loss: 0.0250 - val_accuracy: 0.9915\n",
            "Epoch 107/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0111 - accuracy: 0.9973 - val_loss: 0.0286 - val_accuracy: 0.9893\n",
            "Epoch 108/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0113 - accuracy: 0.9970 - val_loss: 0.0321 - val_accuracy: 0.9879\n",
            "Epoch 109/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0113 - accuracy: 0.9966 - val_loss: 0.0279 - val_accuracy: 0.9900\n",
            "Epoch 110/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0108 - accuracy: 0.9973 - val_loss: 0.0393 - val_accuracy: 0.9851\n",
            "Epoch 111/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0108 - accuracy: 0.9954 - val_loss: 0.0277 - val_accuracy: 0.9908\n",
            "Epoch 112/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0105 - accuracy: 0.9966 - val_loss: 0.0264 - val_accuracy: 0.9893\n",
            "Epoch 113/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0095 - accuracy: 0.9976 - val_loss: 0.0254 - val_accuracy: 0.9893\n",
            "Epoch 114/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0111 - accuracy: 0.9963 - val_loss: 0.0247 - val_accuracy: 0.9908\n",
            "Epoch 115/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0100 - accuracy: 0.9979 - val_loss: 0.0295 - val_accuracy: 0.9915\n",
            "Epoch 116/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0099 - accuracy: 0.9960 - val_loss: 0.0269 - val_accuracy: 0.9908\n",
            "Epoch 117/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0106 - accuracy: 0.9963 - val_loss: 0.0279 - val_accuracy: 0.9886\n",
            "Epoch 118/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0096 - accuracy: 0.9976 - val_loss: 0.0278 - val_accuracy: 0.9908\n",
            "Epoch 119/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0094 - accuracy: 0.9963 - val_loss: 0.0281 - val_accuracy: 0.9900\n",
            "Epoch 120/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0092 - accuracy: 0.9973 - val_loss: 0.0276 - val_accuracy: 0.9915\n",
            "Epoch 121/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0092 - accuracy: 0.9976 - val_loss: 0.0277 - val_accuracy: 0.9908\n",
            "Epoch 122/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0094 - accuracy: 0.9963 - val_loss: 0.0295 - val_accuracy: 0.9893\n",
            "Epoch 123/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0091 - accuracy: 0.9976 - val_loss: 0.0310 - val_accuracy: 0.9893\n",
            "Epoch 124/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0090 - accuracy: 0.9979 - val_loss: 0.0288 - val_accuracy: 0.9886\n",
            "Epoch 125/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0088 - accuracy: 0.9957 - val_loss: 0.0304 - val_accuracy: 0.9900\n",
            "Epoch 126/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0084 - accuracy: 0.9973 - val_loss: 0.0271 - val_accuracy: 0.9908\n",
            "Epoch 127/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0088 - accuracy: 0.9970 - val_loss: 0.0256 - val_accuracy: 0.9915\n",
            "Epoch 128/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0082 - accuracy: 0.9973 - val_loss: 0.0295 - val_accuracy: 0.9872\n",
            "Epoch 129/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0080 - accuracy: 0.9970 - val_loss: 0.0277 - val_accuracy: 0.9908\n",
            "Epoch 130/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0071 - accuracy: 0.9985 - val_loss: 0.0298 - val_accuracy: 0.9900\n",
            "Epoch 131/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0084 - accuracy: 0.9960 - val_loss: 0.0276 - val_accuracy: 0.9900\n",
            "Epoch 132/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0080 - accuracy: 0.9970 - val_loss: 0.0288 - val_accuracy: 0.9879\n",
            "Epoch 133/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0074 - accuracy: 0.9985 - val_loss: 0.0277 - val_accuracy: 0.9900\n",
            "Epoch 134/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0078 - accuracy: 0.9970 - val_loss: 0.0314 - val_accuracy: 0.9872\n",
            "Epoch 135/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0073 - accuracy: 0.9982 - val_loss: 0.0377 - val_accuracy: 0.9851\n",
            "Epoch 136/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0079 - accuracy: 0.9976 - val_loss: 0.0262 - val_accuracy: 0.9908\n",
            "Epoch 137/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0073 - accuracy: 0.9982 - val_loss: 0.0311 - val_accuracy: 0.9879\n",
            "Epoch 138/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0070 - accuracy: 0.9973 - val_loss: 0.0274 - val_accuracy: 0.9915\n",
            "Epoch 139/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0073 - accuracy: 0.9976 - val_loss: 0.0280 - val_accuracy: 0.9915\n",
            "Epoch 140/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0065 - accuracy: 0.9982 - val_loss: 0.0351 - val_accuracy: 0.9851\n",
            "Epoch 141/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0075 - accuracy: 0.9979 - val_loss: 0.0281 - val_accuracy: 0.9908\n",
            "Epoch 142/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0072 - accuracy: 0.9976 - val_loss: 0.0287 - val_accuracy: 0.9908\n",
            "Epoch 143/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0066 - accuracy: 0.9982 - val_loss: 0.0297 - val_accuracy: 0.9908\n",
            "Epoch 144/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0065 - accuracy: 0.9979 - val_loss: 0.0296 - val_accuracy: 0.9915\n",
            "Epoch 145/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0066 - accuracy: 0.9976 - val_loss: 0.0375 - val_accuracy: 0.9865\n",
            "Epoch 146/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0067 - accuracy: 0.9970 - val_loss: 0.0287 - val_accuracy: 0.9915\n",
            "Epoch 147/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0066 - accuracy: 0.9985 - val_loss: 0.0288 - val_accuracy: 0.9922\n",
            "Epoch 148/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0064 - accuracy: 0.9988 - val_loss: 0.0302 - val_accuracy: 0.9915\n",
            "Epoch 149/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0064 - accuracy: 0.9982 - val_loss: 0.0339 - val_accuracy: 0.9879\n",
            "Epoch 150/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0067 - accuracy: 0.9979 - val_loss: 0.0299 - val_accuracy: 0.9893\n",
            "Epoch 151/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0065 - accuracy: 0.9979 - val_loss: 0.0295 - val_accuracy: 0.9893\n",
            "Epoch 152/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0060 - accuracy: 0.9982 - val_loss: 0.0274 - val_accuracy: 0.9908\n",
            "Epoch 153/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0064 - accuracy: 0.9982 - val_loss: 0.0301 - val_accuracy: 0.9908\n",
            "Epoch 154/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0052 - accuracy: 0.9991 - val_loss: 0.0294 - val_accuracy: 0.9900\n",
            "Epoch 155/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0061 - accuracy: 0.9976 - val_loss: 0.0284 - val_accuracy: 0.9929\n",
            "Epoch 156/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0058 - accuracy: 0.9985 - val_loss: 0.0296 - val_accuracy: 0.9915\n",
            "Epoch 157/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0056 - accuracy: 0.9979 - val_loss: 0.0336 - val_accuracy: 0.9879\n",
            "Epoch 158/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0056 - accuracy: 0.9982 - val_loss: 0.0318 - val_accuracy: 0.9893\n",
            "Epoch 159/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0047 - accuracy: 0.9988 - val_loss: 0.0328 - val_accuracy: 0.9872\n",
            "Epoch 160/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0054 - accuracy: 0.9985 - val_loss: 0.0288 - val_accuracy: 0.9908\n",
            "Epoch 161/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0052 - accuracy: 0.9988 - val_loss: 0.0258 - val_accuracy: 0.9922\n",
            "Epoch 162/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0050 - accuracy: 0.9988 - val_loss: 0.0280 - val_accuracy: 0.9908\n",
            "Epoch 163/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0050 - accuracy: 0.9988 - val_loss: 0.0280 - val_accuracy: 0.9908\n",
            "Epoch 164/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0054 - accuracy: 0.9982 - val_loss: 0.0305 - val_accuracy: 0.9900\n",
            "Epoch 165/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0055 - accuracy: 0.9985 - val_loss: 0.0323 - val_accuracy: 0.9893\n",
            "Epoch 166/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0039 - accuracy: 0.9997 - val_loss: 0.0351 - val_accuracy: 0.9872\n",
            "Epoch 167/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0049 - accuracy: 0.9982 - val_loss: 0.0328 - val_accuracy: 0.9893\n",
            "Epoch 168/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0054 - accuracy: 0.9985 - val_loss: 0.0343 - val_accuracy: 0.9886\n",
            "Epoch 169/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0045 - accuracy: 0.9994 - val_loss: 0.0356 - val_accuracy: 0.9879\n",
            "Epoch 170/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0046 - accuracy: 0.9991 - val_loss: 0.0329 - val_accuracy: 0.9893\n",
            "Epoch 171/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0043 - accuracy: 0.9991 - val_loss: 0.0310 - val_accuracy: 0.9908\n",
            "Epoch 172/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0043 - accuracy: 0.9985 - val_loss: 0.0342 - val_accuracy: 0.9872\n",
            "Epoch 173/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0041 - accuracy: 0.9985 - val_loss: 0.0336 - val_accuracy: 0.9886\n",
            "Epoch 174/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0044 - accuracy: 0.9982 - val_loss: 0.0320 - val_accuracy: 0.9879\n",
            "Epoch 175/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0048 - accuracy: 0.9985 - val_loss: 0.0352 - val_accuracy: 0.9879\n",
            "Epoch 176/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0042 - accuracy: 0.9985 - val_loss: 0.0402 - val_accuracy: 0.9865\n",
            "Epoch 177/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0043 - accuracy: 0.9994 - val_loss: 0.0312 - val_accuracy: 0.9908\n",
            "Epoch 178/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0044 - accuracy: 0.9991 - val_loss: 0.0330 - val_accuracy: 0.9893\n",
            "Epoch 179/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0039 - accuracy: 0.9994 - val_loss: 0.0295 - val_accuracy: 0.9908\n",
            "Epoch 180/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0043 - accuracy: 0.9985 - val_loss: 0.0314 - val_accuracy: 0.9915\n",
            "Epoch 181/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0036 - accuracy: 0.9991 - val_loss: 0.0282 - val_accuracy: 0.9915\n",
            "Epoch 182/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0040 - accuracy: 0.9988 - val_loss: 0.0325 - val_accuracy: 0.9893\n",
            "Epoch 183/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0038 - accuracy: 0.9994 - val_loss: 0.0371 - val_accuracy: 0.9886\n",
            "Epoch 184/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0041 - accuracy: 0.9991 - val_loss: 0.0368 - val_accuracy: 0.9872\n",
            "Epoch 185/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0040 - accuracy: 0.9985 - val_loss: 0.0336 - val_accuracy: 0.9886\n",
            "Epoch 186/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0039 - accuracy: 0.9985 - val_loss: 0.0353 - val_accuracy: 0.9879\n",
            "Epoch 187/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0034 - accuracy: 0.9991 - val_loss: 0.0350 - val_accuracy: 0.9893\n",
            "Epoch 188/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0039 - accuracy: 0.9988 - val_loss: 0.0337 - val_accuracy: 0.9893\n",
            "Epoch 189/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0031 - accuracy: 0.9991 - val_loss: 0.0427 - val_accuracy: 0.9851\n",
            "Epoch 190/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0039 - accuracy: 0.9991 - val_loss: 0.0404 - val_accuracy: 0.9872\n",
            "Epoch 191/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0038 - accuracy: 0.9991 - val_loss: 0.0357 - val_accuracy: 0.9886\n",
            "Epoch 192/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0032 - accuracy: 0.9991 - val_loss: 0.0339 - val_accuracy: 0.9886\n",
            "Epoch 193/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0031 - accuracy: 0.9994 - val_loss: 0.0331 - val_accuracy: 0.9886\n",
            "Epoch 194/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0031 - accuracy: 0.9994 - val_loss: 0.0336 - val_accuracy: 0.9886\n",
            "Epoch 195/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0037 - accuracy: 0.9988 - val_loss: 0.0347 - val_accuracy: 0.9886\n",
            "Epoch 196/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0033 - accuracy: 0.9994 - val_loss: 0.0379 - val_accuracy: 0.9886\n",
            "Epoch 197/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0030 - accuracy: 0.9997 - val_loss: 0.0341 - val_accuracy: 0.9900\n",
            "Epoch 198/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0030 - accuracy: 0.9997 - val_loss: 0.0382 - val_accuracy: 0.9886\n",
            "Epoch 199/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.0425 - val_accuracy: 0.9865\n",
            "Epoch 200/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0359 - val_accuracy: 0.9879\n",
            "Epoch 201/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0027 - accuracy: 0.9997 - val_loss: 0.0353 - val_accuracy: 0.9900\n",
            "Epoch 202/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0030 - accuracy: 0.9994 - val_loss: 0.0392 - val_accuracy: 0.9858\n",
            "Epoch 203/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0025 - accuracy: 0.9997 - val_loss: 0.0335 - val_accuracy: 0.9915\n",
            "Epoch 204/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0025 - accuracy: 0.9994 - val_loss: 0.0323 - val_accuracy: 0.9900\n",
            "Epoch 205/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0030 - accuracy: 0.9994 - val_loss: 0.0345 - val_accuracy: 0.9893\n",
            "Epoch 206/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0026 - accuracy: 0.9991 - val_loss: 0.0385 - val_accuracy: 0.9865\n",
            "Epoch 207/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.0416 - val_accuracy: 0.9865\n",
            "Epoch 208/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0369 - val_accuracy: 0.9879\n",
            "Epoch 209/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0400 - val_accuracy: 0.9865\n",
            "Epoch 210/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0029 - accuracy: 0.9997 - val_loss: 0.0381 - val_accuracy: 0.9879\n",
            "Epoch 211/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0022 - accuracy: 0.9997 - val_loss: 0.0339 - val_accuracy: 0.9915\n",
            "Epoch 212/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0025 - accuracy: 0.9997 - val_loss: 0.0353 - val_accuracy: 0.9900\n",
            "Epoch 213/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0023 - accuracy: 0.9994 - val_loss: 0.0392 - val_accuracy: 0.9879\n",
            "Epoch 214/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.0386 - val_accuracy: 0.9893\n",
            "Epoch 215/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0025 - accuracy: 0.9991 - val_loss: 0.0387 - val_accuracy: 0.9872\n",
            "Epoch 216/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0027 - accuracy: 0.9991 - val_loss: 0.0463 - val_accuracy: 0.9865\n",
            "Epoch 217/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0022 - accuracy: 0.9997 - val_loss: 0.0365 - val_accuracy: 0.9872\n",
            "Epoch 218/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0025 - accuracy: 0.9994 - val_loss: 0.0405 - val_accuracy: 0.9886\n",
            "Epoch 219/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0023 - accuracy: 0.9997 - val_loss: 0.0407 - val_accuracy: 0.9858\n",
            "Epoch 220/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0023 - accuracy: 0.9997 - val_loss: 0.0326 - val_accuracy: 0.9908\n",
            "Epoch 221/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0022 - accuracy: 0.9994 - val_loss: 0.0351 - val_accuracy: 0.9886\n",
            "Epoch 222/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0024 - accuracy: 0.9988 - val_loss: 0.0367 - val_accuracy: 0.9893\n",
            "Epoch 223/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 0.0414 - val_accuracy: 0.9886\n",
            "Epoch 224/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0020 - accuracy: 0.9991 - val_loss: 0.0380 - val_accuracy: 0.9879\n",
            "Epoch 225/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0023 - accuracy: 0.9994 - val_loss: 0.0380 - val_accuracy: 0.9900\n",
            "Epoch 226/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0020 - accuracy: 0.9994 - val_loss: 0.0403 - val_accuracy: 0.9872\n",
            "Epoch 227/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0023 - accuracy: 0.9994 - val_loss: 0.0391 - val_accuracy: 0.9893\n",
            "Epoch 228/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0399 - val_accuracy: 0.9879\n",
            "Epoch 229/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0019 - accuracy: 0.9997 - val_loss: 0.0398 - val_accuracy: 0.9872\n",
            "Epoch 230/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0403 - val_accuracy: 0.9879\n",
            "Epoch 231/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0540 - val_accuracy: 0.9851\n",
            "Epoch 232/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0470 - val_accuracy: 0.9865\n",
            "Epoch 233/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0020 - accuracy: 0.9994 - val_loss: 0.0388 - val_accuracy: 0.9893\n",
            "Epoch 234/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 0.0402 - val_accuracy: 0.9900\n",
            "Epoch 235/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0510 - val_accuracy: 0.9865\n",
            "Epoch 236/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 0.0412 - val_accuracy: 0.9893\n",
            "Epoch 237/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 0.0392 - val_accuracy: 0.9893\n",
            "Epoch 238/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 0.0372 - val_accuracy: 0.9886\n",
            "Epoch 239/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 0.0403 - val_accuracy: 0.9886\n",
            "Epoch 240/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 0.0409 - val_accuracy: 0.9865\n",
            "Epoch 241/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0416 - val_accuracy: 0.9879\n",
            "Epoch 242/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0424 - val_accuracy: 0.9872\n",
            "Epoch 243/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 0.0424 - val_accuracy: 0.9900\n",
            "Epoch 244/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 0.0409 - val_accuracy: 0.9886\n",
            "Epoch 245/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.0410 - val_accuracy: 0.9886\n",
            "Epoch 246/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0013 - accuracy: 0.9997 - val_loss: 0.0458 - val_accuracy: 0.9865\n",
            "Epoch 247/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 0.0400 - val_accuracy: 0.9893\n",
            "Epoch 248/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.0393 - val_accuracy: 0.9893\n",
            "Epoch 249/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 0.0400 - val_accuracy: 0.9893\n",
            "Epoch 250/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0013 - accuracy: 0.9994 - val_loss: 0.0477 - val_accuracy: 0.9879\n",
            "Epoch 251/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 0.0400 - val_accuracy: 0.9879\n",
            "Epoch 252/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0447 - val_accuracy: 0.9879\n",
            "Epoch 253/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 0.0423 - val_accuracy: 0.9879\n",
            "Epoch 254/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 0.0427 - val_accuracy: 0.9893\n",
            "Epoch 255/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0011 - accuracy: 0.9997 - val_loss: 0.0437 - val_accuracy: 0.9879\n",
            "Epoch 256/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0012 - accuracy: 0.9997 - val_loss: 0.0420 - val_accuracy: 0.9893\n",
            "Epoch 257/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0010 - accuracy: 0.9997 - val_loss: 0.0441 - val_accuracy: 0.9908\n",
            "Epoch 258/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0502 - val_accuracy: 0.9879\n",
            "Epoch 259/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0012 - accuracy: 0.9997 - val_loss: 0.0416 - val_accuracy: 0.9900\n",
            "Epoch 260/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0012 - accuracy: 0.9997 - val_loss: 0.0421 - val_accuracy: 0.9872\n",
            "Epoch 261/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 0.0439 - val_accuracy: 0.9886\n",
            "Epoch 262/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0436 - val_accuracy: 0.9893\n",
            "Epoch 263/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.6193e-04 - accuracy: 0.9997 - val_loss: 0.0470 - val_accuracy: 0.9872\n",
            "Epoch 264/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0013 - accuracy: 0.9997 - val_loss: 0.0453 - val_accuracy: 0.9879\n",
            "Epoch 265/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0011 - accuracy: 0.9997 - val_loss: 0.0421 - val_accuracy: 0.9900\n",
            "Epoch 266/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0011 - accuracy: 0.9997 - val_loss: 0.0417 - val_accuracy: 0.9893\n",
            "Epoch 267/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.7221e-04 - accuracy: 1.0000 - val_loss: 0.0513 - val_accuracy: 0.9893\n",
            "Epoch 268/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.1236e-04 - accuracy: 1.0000 - val_loss: 0.0458 - val_accuracy: 0.9886\n",
            "Epoch 269/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.5984e-04 - accuracy: 1.0000 - val_loss: 0.0487 - val_accuracy: 0.9879\n",
            "Epoch 270/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0011 - accuracy: 0.9997 - val_loss: 0.0476 - val_accuracy: 0.9879\n",
            "Epoch 271/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.9886e-04 - accuracy: 1.0000 - val_loss: 0.0484 - val_accuracy: 0.9879\n",
            "Epoch 272/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.8809e-04 - accuracy: 1.0000 - val_loss: 0.0442 - val_accuracy: 0.9893\n",
            "Epoch 273/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.9409e-04 - accuracy: 1.0000 - val_loss: 0.0473 - val_accuracy: 0.9886\n",
            "Epoch 274/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0010 - accuracy: 0.9997 - val_loss: 0.0453 - val_accuracy: 0.9879\n",
            "Epoch 275/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.8491e-04 - accuracy: 0.9997 - val_loss: 0.0450 - val_accuracy: 0.9886\n",
            "Epoch 276/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.9462e-04 - accuracy: 1.0000 - val_loss: 0.0477 - val_accuracy: 0.9872\n",
            "Epoch 277/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.7373e-04 - accuracy: 0.9997 - val_loss: 0.0464 - val_accuracy: 0.9872\n",
            "Epoch 278/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.6952e-04 - accuracy: 1.0000 - val_loss: 0.0505 - val_accuracy: 0.9879\n",
            "Epoch 279/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.3690e-04 - accuracy: 0.9997 - val_loss: 0.0482 - val_accuracy: 0.9886\n",
            "Epoch 280/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0011 - accuracy: 0.9997 - val_loss: 0.0491 - val_accuracy: 0.9872\n",
            "Epoch 281/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.2306e-04 - accuracy: 0.9997 - val_loss: 0.0485 - val_accuracy: 0.9872\n",
            "Epoch 282/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.1211e-04 - accuracy: 0.9997 - val_loss: 0.0508 - val_accuracy: 0.9865\n",
            "Epoch 283/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.9493e-04 - accuracy: 1.0000 - val_loss: 0.0503 - val_accuracy: 0.9879\n",
            "Epoch 284/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.9465e-04 - accuracy: 1.0000 - val_loss: 0.0515 - val_accuracy: 0.9879\n",
            "Epoch 285/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.3386e-04 - accuracy: 0.9997 - val_loss: 0.0457 - val_accuracy: 0.9879\n",
            "Epoch 286/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.8537e-04 - accuracy: 1.0000 - val_loss: 0.0574 - val_accuracy: 0.9879\n",
            "Epoch 287/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.9065e-04 - accuracy: 1.0000 - val_loss: 0.0473 - val_accuracy: 0.9865\n",
            "Epoch 288/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.5730e-04 - accuracy: 1.0000 - val_loss: 0.0496 - val_accuracy: 0.9879\n",
            "Epoch 289/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.4045e-04 - accuracy: 1.0000 - val_loss: 0.0505 - val_accuracy: 0.9872\n",
            "Epoch 290/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.6477e-04 - accuracy: 1.0000 - val_loss: 0.0519 - val_accuracy: 0.9865\n",
            "Epoch 291/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.9765e-04 - accuracy: 0.9997 - val_loss: 0.0529 - val_accuracy: 0.9872\n",
            "Epoch 292/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.0180e-04 - accuracy: 0.9997 - val_loss: 0.0542 - val_accuracy: 0.9858\n",
            "Epoch 293/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.0031e-04 - accuracy: 1.0000 - val_loss: 0.0498 - val_accuracy: 0.9858\n",
            "Epoch 294/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.0568e-04 - accuracy: 1.0000 - val_loss: 0.0506 - val_accuracy: 0.9858\n",
            "Epoch 295/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.2274e-04 - accuracy: 1.0000 - val_loss: 0.0511 - val_accuracy: 0.9865\n",
            "Epoch 296/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.8858e-04 - accuracy: 1.0000 - val_loss: 0.0522 - val_accuracy: 0.9872\n",
            "Epoch 297/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.0828e-04 - accuracy: 1.0000 - val_loss: 0.0533 - val_accuracy: 0.9865\n",
            "Epoch 298/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.9476e-04 - accuracy: 1.0000 - val_loss: 0.0511 - val_accuracy: 0.9865\n",
            "Epoch 299/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.8841e-04 - accuracy: 0.9997 - val_loss: 0.0538 - val_accuracy: 0.9865\n",
            "Epoch 300/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.3169e-04 - accuracy: 1.0000 - val_loss: 0.0521 - val_accuracy: 0.9872\n",
            "Epoch 301/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.5210e-04 - accuracy: 0.9997 - val_loss: 0.0483 - val_accuracy: 0.9872\n",
            "Epoch 302/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.3962e-04 - accuracy: 1.0000 - val_loss: 0.0481 - val_accuracy: 0.9893\n",
            "Epoch 303/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.4809e-04 - accuracy: 0.9997 - val_loss: 0.0485 - val_accuracy: 0.9872\n",
            "Epoch 304/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.7273e-04 - accuracy: 1.0000 - val_loss: 0.0488 - val_accuracy: 0.9872\n",
            "Epoch 305/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.2766e-04 - accuracy: 1.0000 - val_loss: 0.0558 - val_accuracy: 0.9900\n",
            "Epoch 306/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.5851e-04 - accuracy: 1.0000 - val_loss: 0.0621 - val_accuracy: 0.9865\n",
            "Epoch 307/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.2447e-04 - accuracy: 1.0000 - val_loss: 0.0517 - val_accuracy: 0.9872\n",
            "Epoch 308/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.6802e-04 - accuracy: 1.0000 - val_loss: 0.0544 - val_accuracy: 0.9865\n",
            "Epoch 309/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.0303e-04 - accuracy: 0.9997 - val_loss: 0.0576 - val_accuracy: 0.9865\n",
            "Epoch 310/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.4791e-04 - accuracy: 1.0000 - val_loss: 0.0551 - val_accuracy: 0.9879\n",
            "Epoch 311/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.4172e-04 - accuracy: 1.0000 - val_loss: 0.0574 - val_accuracy: 0.9872\n",
            "Epoch 312/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.1229e-04 - accuracy: 1.0000 - val_loss: 0.0545 - val_accuracy: 0.9879\n",
            "Epoch 313/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.1937e-04 - accuracy: 1.0000 - val_loss: 0.0527 - val_accuracy: 0.9879\n",
            "Epoch 314/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.5691e-04 - accuracy: 1.0000 - val_loss: 0.0546 - val_accuracy: 0.9865\n",
            "Epoch 315/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.6520e-04 - accuracy: 1.0000 - val_loss: 0.0533 - val_accuracy: 0.9879\n",
            "Epoch 316/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.0051e-04 - accuracy: 1.0000 - val_loss: 0.0538 - val_accuracy: 0.9886\n",
            "Epoch 317/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.5760e-04 - accuracy: 1.0000 - val_loss: 0.0608 - val_accuracy: 0.9865\n",
            "Epoch 318/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.5636e-04 - accuracy: 0.9997 - val_loss: 0.0600 - val_accuracy: 0.9872\n",
            "Epoch 319/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.5497e-04 - accuracy: 1.0000 - val_loss: 0.0539 - val_accuracy: 0.9879\n",
            "Epoch 320/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.1807e-04 - accuracy: 1.0000 - val_loss: 0.0567 - val_accuracy: 0.9865\n",
            "Epoch 321/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.0374e-04 - accuracy: 1.0000 - val_loss: 0.0521 - val_accuracy: 0.9879\n",
            "Epoch 322/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.0617e-04 - accuracy: 1.0000 - val_loss: 0.0542 - val_accuracy: 0.9893\n",
            "Epoch 323/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.3141e-04 - accuracy: 0.9997 - val_loss: 0.0544 - val_accuracy: 0.9879\n",
            "Epoch 324/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.2241e-04 - accuracy: 1.0000 - val_loss: 0.0654 - val_accuracy: 0.9858\n",
            "Epoch 325/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.9008e-04 - accuracy: 1.0000 - val_loss: 0.0574 - val_accuracy: 0.9865\n",
            "Epoch 326/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.3726e-04 - accuracy: 1.0000 - val_loss: 0.0615 - val_accuracy: 0.9886\n",
            "Epoch 327/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.4359e-04 - accuracy: 1.0000 - val_loss: 0.0577 - val_accuracy: 0.9872\n",
            "Epoch 328/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.5824e-04 - accuracy: 1.0000 - val_loss: 0.0590 - val_accuracy: 0.9872\n",
            "Epoch 329/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.9586e-04 - accuracy: 0.9997 - val_loss: 0.0580 - val_accuracy: 0.9872\n",
            "Epoch 330/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.0564e-04 - accuracy: 1.0000 - val_loss: 0.0603 - val_accuracy: 0.9879\n",
            "Epoch 331/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.0814e-04 - accuracy: 1.0000 - val_loss: 0.0598 - val_accuracy: 0.9851\n",
            "Epoch 332/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.5951e-04 - accuracy: 1.0000 - val_loss: 0.0593 - val_accuracy: 0.9872\n",
            "Epoch 333/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.7484e-04 - accuracy: 1.0000 - val_loss: 0.0634 - val_accuracy: 0.9872\n",
            "Epoch 334/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.1258e-04 - accuracy: 1.0000 - val_loss: 0.0610 - val_accuracy: 0.9879\n",
            "Epoch 335/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.7789e-04 - accuracy: 1.0000 - val_loss: 0.0622 - val_accuracy: 0.9851\n",
            "Epoch 336/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.4880e-04 - accuracy: 1.0000 - val_loss: 0.0632 - val_accuracy: 0.9858\n",
            "Epoch 337/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.3657e-04 - accuracy: 1.0000 - val_loss: 0.0663 - val_accuracy: 0.9872\n",
            "Epoch 338/1056\n",
            "103/103 [==============================] - 0s 3ms/step - loss: 1.9648e-04 - accuracy: 1.0000 - val_loss: 0.0594 - val_accuracy: 0.9879\n",
            "Epoch 339/1056\n",
            "103/103 [==============================] - 0s 3ms/step - loss: 2.3399e-04 - accuracy: 1.0000 - val_loss: 0.0618 - val_accuracy: 0.9879\n",
            "Epoch 340/1056\n",
            "103/103 [==============================] - 0s 3ms/step - loss: 1.3805e-04 - accuracy: 1.0000 - val_loss: 0.0592 - val_accuracy: 0.9879\n",
            "Epoch 341/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.2140e-04 - accuracy: 1.0000 - val_loss: 0.0633 - val_accuracy: 0.9879\n",
            "Epoch 342/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.6943e-04 - accuracy: 1.0000 - val_loss: 0.0626 - val_accuracy: 0.9872\n",
            "Epoch 343/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.5157e-04 - accuracy: 1.0000 - val_loss: 0.0632 - val_accuracy: 0.9879\n",
            "Epoch 344/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.4140e-04 - accuracy: 1.0000 - val_loss: 0.0624 - val_accuracy: 0.9872\n",
            "Epoch 345/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.1315e-04 - accuracy: 1.0000 - val_loss: 0.0639 - val_accuracy: 0.9872\n",
            "Epoch 346/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.2660e-04 - accuracy: 1.0000 - val_loss: 0.0678 - val_accuracy: 0.9865\n",
            "Epoch 347/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.9859e-04 - accuracy: 1.0000 - val_loss: 0.0625 - val_accuracy: 0.9872\n",
            "Epoch 348/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.1659e-04 - accuracy: 1.0000 - val_loss: 0.0631 - val_accuracy: 0.9865\n",
            "Epoch 349/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.2766e-04 - accuracy: 1.0000 - val_loss: 0.0630 - val_accuracy: 0.9865\n",
            "Epoch 350/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.5065e-04 - accuracy: 1.0000 - val_loss: 0.0654 - val_accuracy: 0.9858\n",
            "Epoch 351/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.3189e-04 - accuracy: 0.9997 - val_loss: 0.0680 - val_accuracy: 0.9865\n",
            "Epoch 352/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.0701e-05 - accuracy: 1.0000 - val_loss: 0.0753 - val_accuracy: 0.9851\n",
            "Epoch 353/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.2198e-04 - accuracy: 1.0000 - val_loss: 0.0706 - val_accuracy: 0.9858\n",
            "Epoch 354/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.4568e-04 - accuracy: 1.0000 - val_loss: 0.0646 - val_accuracy: 0.9872\n",
            "Epoch 355/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.5975e-04 - accuracy: 1.0000 - val_loss: 0.0646 - val_accuracy: 0.9872\n",
            "Epoch 356/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.1963e-04 - accuracy: 1.0000 - val_loss: 0.0691 - val_accuracy: 0.9879\n",
            "Epoch 357/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.0224e-04 - accuracy: 1.0000 - val_loss: 0.0642 - val_accuracy: 0.9865\n",
            "Epoch 358/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.5071e-04 - accuracy: 1.0000 - val_loss: 0.0675 - val_accuracy: 0.9865\n",
            "Epoch 359/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.1606e-04 - accuracy: 1.0000 - val_loss: 0.0821 - val_accuracy: 0.9829\n",
            "Epoch 360/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.7966e-04 - accuracy: 1.0000 - val_loss: 0.0677 - val_accuracy: 0.9865\n",
            "Epoch 361/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.2764e-05 - accuracy: 1.0000 - val_loss: 0.0734 - val_accuracy: 0.9865\n",
            "Epoch 362/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.1856e-04 - accuracy: 1.0000 - val_loss: 0.0632 - val_accuracy: 0.9851\n",
            "Epoch 363/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.0993e-04 - accuracy: 1.0000 - val_loss: 0.0694 - val_accuracy: 0.9865\n",
            "Epoch 364/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.5381e-05 - accuracy: 1.0000 - val_loss: 0.0713 - val_accuracy: 0.9858\n",
            "Epoch 365/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.2062e-04 - accuracy: 1.0000 - val_loss: 0.0732 - val_accuracy: 0.9858\n",
            "Epoch 366/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.3306e-04 - accuracy: 1.0000 - val_loss: 0.0752 - val_accuracy: 0.9865\n",
            "Epoch 367/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.2512e-04 - accuracy: 1.0000 - val_loss: 0.0676 - val_accuracy: 0.9879\n",
            "Epoch 368/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.1543e-05 - accuracy: 1.0000 - val_loss: 0.0717 - val_accuracy: 0.9872\n",
            "Epoch 369/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.2098e-04 - accuracy: 1.0000 - val_loss: 0.0693 - val_accuracy: 0.9865\n",
            "Epoch 370/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.7724e-05 - accuracy: 1.0000 - val_loss: 0.0758 - val_accuracy: 0.9858\n",
            "Epoch 371/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.5672e-05 - accuracy: 1.0000 - val_loss: 0.0705 - val_accuracy: 0.9865\n",
            "Epoch 372/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.7550e-05 - accuracy: 1.0000 - val_loss: 0.0705 - val_accuracy: 0.9865\n",
            "Epoch 373/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.9112e-05 - accuracy: 1.0000 - val_loss: 0.0655 - val_accuracy: 0.9879\n",
            "Epoch 374/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.6385e-04 - accuracy: 0.9997 - val_loss: 0.0697 - val_accuracy: 0.9872\n",
            "Epoch 375/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.4481e-04 - accuracy: 1.0000 - val_loss: 0.0718 - val_accuracy: 0.9872\n",
            "Epoch 376/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.1874e-05 - accuracy: 1.0000 - val_loss: 0.0691 - val_accuracy: 0.9872\n",
            "Epoch 377/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.7788e-05 - accuracy: 1.0000 - val_loss: 0.0742 - val_accuracy: 0.9865\n",
            "Epoch 378/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.4332e-04 - accuracy: 1.0000 - val_loss: 0.0749 - val_accuracy: 0.9865\n",
            "Epoch 379/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.1041e-05 - accuracy: 1.0000 - val_loss: 0.0689 - val_accuracy: 0.9872\n",
            "Epoch 380/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.0750e-05 - accuracy: 1.0000 - val_loss: 0.0722 - val_accuracy: 0.9879\n",
            "Epoch 381/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.5594e-05 - accuracy: 1.0000 - val_loss: 0.0751 - val_accuracy: 0.9865\n",
            "Epoch 382/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.7882e-04 - accuracy: 1.0000 - val_loss: 0.0778 - val_accuracy: 0.9851\n",
            "Epoch 383/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.1662e-05 - accuracy: 1.0000 - val_loss: 0.0747 - val_accuracy: 0.9865\n",
            "Epoch 384/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.8149e-04 - accuracy: 1.0000 - val_loss: 0.0702 - val_accuracy: 0.9872\n",
            "Epoch 385/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.1591e-05 - accuracy: 1.0000 - val_loss: 0.0732 - val_accuracy: 0.9872\n",
            "Epoch 386/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.2047e-05 - accuracy: 1.0000 - val_loss: 0.0637 - val_accuracy: 0.9879\n",
            "Epoch 387/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.3226e-05 - accuracy: 1.0000 - val_loss: 0.0724 - val_accuracy: 0.9865\n",
            "Epoch 388/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.8990e-05 - accuracy: 1.0000 - val_loss: 0.0680 - val_accuracy: 0.9865\n",
            "Epoch 389/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.1064e-04 - accuracy: 1.0000 - val_loss: 0.0737 - val_accuracy: 0.9872\n",
            "Epoch 390/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.3518e-05 - accuracy: 1.0000 - val_loss: 0.0743 - val_accuracy: 0.9851\n",
            "Epoch 391/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.8407e-05 - accuracy: 1.0000 - val_loss: 0.0756 - val_accuracy: 0.9872\n",
            "Epoch 392/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.3691e-05 - accuracy: 1.0000 - val_loss: 0.0731 - val_accuracy: 0.9865\n",
            "Epoch 393/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.0902e-05 - accuracy: 1.0000 - val_loss: 0.0723 - val_accuracy: 0.9858\n",
            "Epoch 394/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.8650e-05 - accuracy: 1.0000 - val_loss: 0.0807 - val_accuracy: 0.9865\n",
            "Epoch 395/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.5156e-05 - accuracy: 1.0000 - val_loss: 0.0726 - val_accuracy: 0.9879\n",
            "Epoch 396/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.9178e-05 - accuracy: 1.0000 - val_loss: 0.0788 - val_accuracy: 0.9858\n",
            "Epoch 397/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.0908e-05 - accuracy: 1.0000 - val_loss: 0.0735 - val_accuracy: 0.9872\n",
            "Epoch 398/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.6102e-05 - accuracy: 1.0000 - val_loss: 0.0748 - val_accuracy: 0.9858\n",
            "Epoch 399/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.7172e-05 - accuracy: 1.0000 - val_loss: 0.0777 - val_accuracy: 0.9865\n",
            "Epoch 400/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.5896e-05 - accuracy: 1.0000 - val_loss: 0.0808 - val_accuracy: 0.9865\n",
            "Epoch 401/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.2232e-05 - accuracy: 1.0000 - val_loss: 0.0786 - val_accuracy: 0.9865\n",
            "Epoch 402/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.2426e-05 - accuracy: 1.0000 - val_loss: 0.0717 - val_accuracy: 0.9851\n",
            "Epoch 403/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.7653e-05 - accuracy: 1.0000 - val_loss: 0.0808 - val_accuracy: 0.9872\n",
            "Epoch 404/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.1140e-05 - accuracy: 1.0000 - val_loss: 0.0880 - val_accuracy: 0.9858\n",
            "Epoch 405/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.7031e-05 - accuracy: 1.0000 - val_loss: 0.0766 - val_accuracy: 0.9858\n",
            "Epoch 406/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.5816e-05 - accuracy: 1.0000 - val_loss: 0.0762 - val_accuracy: 0.9865\n",
            "Epoch 407/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.1593e-05 - accuracy: 1.0000 - val_loss: 0.0896 - val_accuracy: 0.9872\n",
            "Epoch 408/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.9472e-05 - accuracy: 1.0000 - val_loss: 0.0789 - val_accuracy: 0.9865\n",
            "Epoch 409/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.0782e-05 - accuracy: 1.0000 - val_loss: 0.0798 - val_accuracy: 0.9865\n",
            "Epoch 410/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.0776e-05 - accuracy: 1.0000 - val_loss: 0.0798 - val_accuracy: 0.9851\n",
            "Epoch 411/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.4834e-05 - accuracy: 1.0000 - val_loss: 0.0782 - val_accuracy: 0.9858\n",
            "Epoch 412/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.6490e-05 - accuracy: 1.0000 - val_loss: 0.0790 - val_accuracy: 0.9872\n",
            "Epoch 413/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.6907e-05 - accuracy: 1.0000 - val_loss: 0.1054 - val_accuracy: 0.9865\n",
            "Epoch 414/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.7078e-05 - accuracy: 1.0000 - val_loss: 0.0823 - val_accuracy: 0.9858\n",
            "Epoch 415/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.3846e-05 - accuracy: 1.0000 - val_loss: 0.0815 - val_accuracy: 0.9858\n",
            "Epoch 416/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.6892e-05 - accuracy: 1.0000 - val_loss: 0.0825 - val_accuracy: 0.9858\n",
            "Epoch 417/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.1525e-05 - accuracy: 1.0000 - val_loss: 0.0751 - val_accuracy: 0.9879\n",
            "Epoch 418/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.4035e-05 - accuracy: 1.0000 - val_loss: 0.0825 - val_accuracy: 0.9872\n",
            "Epoch 419/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.5697e-05 - accuracy: 1.0000 - val_loss: 0.0854 - val_accuracy: 0.9858\n",
            "Epoch 420/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.6909e-05 - accuracy: 1.0000 - val_loss: 0.0879 - val_accuracy: 0.9851\n",
            "Epoch 421/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.4390e-05 - accuracy: 1.0000 - val_loss: 0.0860 - val_accuracy: 0.9872\n",
            "Epoch 422/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.0602e-05 - accuracy: 1.0000 - val_loss: 0.0877 - val_accuracy: 0.9865\n",
            "Epoch 423/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.2847e-05 - accuracy: 1.0000 - val_loss: 0.0861 - val_accuracy: 0.9865\n",
            "Epoch 424/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.0733e-05 - accuracy: 1.0000 - val_loss: 0.0853 - val_accuracy: 0.9872\n",
            "Epoch 425/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.7723e-05 - accuracy: 1.0000 - val_loss: 0.0876 - val_accuracy: 0.9872\n",
            "Epoch 426/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.8646e-06 - accuracy: 1.0000 - val_loss: 0.0790 - val_accuracy: 0.9886\n",
            "Epoch 427/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.3897e-06 - accuracy: 1.0000 - val_loss: 0.0885 - val_accuracy: 0.9872\n",
            "Epoch 428/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.7291e-05 - accuracy: 1.0000 - val_loss: 0.0814 - val_accuracy: 0.9872\n",
            "Epoch 429/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.0053e-05 - accuracy: 1.0000 - val_loss: 0.0855 - val_accuracy: 0.9858\n",
            "Epoch 430/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.3226e-05 - accuracy: 1.0000 - val_loss: 0.0788 - val_accuracy: 0.9872\n",
            "Epoch 431/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.7016e-06 - accuracy: 1.0000 - val_loss: 0.0916 - val_accuracy: 0.9844\n",
            "Epoch 432/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.5471e-05 - accuracy: 1.0000 - val_loss: 0.0855 - val_accuracy: 0.9872\n",
            "Epoch 433/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.3219e-05 - accuracy: 1.0000 - val_loss: 0.0838 - val_accuracy: 0.9858\n",
            "Epoch 434/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.4490e-05 - accuracy: 1.0000 - val_loss: 0.0895 - val_accuracy: 0.9872\n",
            "Epoch 435/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.5715e-06 - accuracy: 1.0000 - val_loss: 0.0812 - val_accuracy: 0.9872\n",
            "Epoch 436/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.5803e-06 - accuracy: 1.0000 - val_loss: 0.0976 - val_accuracy: 0.9844\n",
            "Epoch 437/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.8137e-05 - accuracy: 1.0000 - val_loss: 0.0851 - val_accuracy: 0.9865\n",
            "Epoch 438/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.5344e-05 - accuracy: 1.0000 - val_loss: 0.0970 - val_accuracy: 0.9844\n",
            "Epoch 439/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.0134e-06 - accuracy: 1.0000 - val_loss: 0.0908 - val_accuracy: 0.9872\n",
            "Epoch 440/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.1421e-05 - accuracy: 1.0000 - val_loss: 0.0954 - val_accuracy: 0.9858\n",
            "Epoch 441/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.5872e-05 - accuracy: 1.0000 - val_loss: 0.0897 - val_accuracy: 0.9865\n",
            "Epoch 442/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.5727e-05 - accuracy: 1.0000 - val_loss: 0.0969 - val_accuracy: 0.9865\n",
            "Epoch 443/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.6106e-05 - accuracy: 1.0000 - val_loss: 0.0832 - val_accuracy: 0.9872\n",
            "Epoch 444/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.3033e-05 - accuracy: 1.0000 - val_loss: 0.0929 - val_accuracy: 0.9858\n",
            "Epoch 445/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.1440e-05 - accuracy: 1.0000 - val_loss: 0.0919 - val_accuracy: 0.9865\n",
            "Epoch 446/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.3761e-05 - accuracy: 1.0000 - val_loss: 0.0971 - val_accuracy: 0.9844\n",
            "Epoch 447/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.5241e-05 - accuracy: 1.0000 - val_loss: 0.0909 - val_accuracy: 0.9851\n",
            "Epoch 448/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.0271e-06 - accuracy: 1.0000 - val_loss: 0.0935 - val_accuracy: 0.9872\n",
            "Epoch 449/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.5116e-05 - accuracy: 1.0000 - val_loss: 0.0938 - val_accuracy: 0.9851\n",
            "Epoch 450/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.5104e-06 - accuracy: 1.0000 - val_loss: 0.0895 - val_accuracy: 0.9865\n",
            "Epoch 451/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.3956e-06 - accuracy: 1.0000 - val_loss: 0.0920 - val_accuracy: 0.9872\n",
            "Epoch 452/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.5529e-06 - accuracy: 1.0000 - val_loss: 0.0892 - val_accuracy: 0.9858\n",
            "Epoch 453/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.8800e-06 - accuracy: 1.0000 - val_loss: 0.0905 - val_accuracy: 0.9865\n",
            "Epoch 454/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.2625e-05 - accuracy: 1.0000 - val_loss: 0.0952 - val_accuracy: 0.9851\n",
            "Epoch 455/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.5405e-06 - accuracy: 1.0000 - val_loss: 0.0908 - val_accuracy: 0.9865\n",
            "Epoch 456/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.6224e-05 - accuracy: 1.0000 - val_loss: 0.0932 - val_accuracy: 0.9858\n",
            "Epoch 457/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.7084e-06 - accuracy: 1.0000 - val_loss: 0.0940 - val_accuracy: 0.9858\n",
            "Epoch 458/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.0040e-05 - accuracy: 1.0000 - val_loss: 0.0933 - val_accuracy: 0.9865\n",
            "Epoch 459/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.5255e-06 - accuracy: 1.0000 - val_loss: 0.0965 - val_accuracy: 0.9858\n",
            "Epoch 460/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.8219e-06 - accuracy: 1.0000 - val_loss: 0.0933 - val_accuracy: 0.9858\n",
            "Epoch 461/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.1454e-06 - accuracy: 1.0000 - val_loss: 0.1085 - val_accuracy: 0.9858\n",
            "Epoch 462/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.0352e-05 - accuracy: 1.0000 - val_loss: 0.0949 - val_accuracy: 0.9851\n",
            "Epoch 463/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.6948e-06 - accuracy: 1.0000 - val_loss: 0.0958 - val_accuracy: 0.9836\n",
            "Epoch 464/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.7044e-05 - accuracy: 1.0000 - val_loss: 0.1091 - val_accuracy: 0.9844\n",
            "Epoch 465/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.3595e-06 - accuracy: 1.0000 - val_loss: 0.0956 - val_accuracy: 0.9872\n",
            "Epoch 466/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.0569e-05 - accuracy: 1.0000 - val_loss: 0.1068 - val_accuracy: 0.9851\n",
            "Epoch 467/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.3365e-05 - accuracy: 1.0000 - val_loss: 0.1125 - val_accuracy: 0.9829\n",
            "Epoch 468/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.8712e-06 - accuracy: 1.0000 - val_loss: 0.1015 - val_accuracy: 0.9858\n",
            "Epoch 469/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.0906e-06 - accuracy: 1.0000 - val_loss: 0.1007 - val_accuracy: 0.9858\n",
            "Epoch 470/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.6863e-06 - accuracy: 1.0000 - val_loss: 0.0989 - val_accuracy: 0.9865\n",
            "Epoch 471/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.0028e-05 - accuracy: 1.0000 - val_loss: 0.0993 - val_accuracy: 0.9851\n",
            "Epoch 472/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.4534e-06 - accuracy: 1.0000 - val_loss: 0.1074 - val_accuracy: 0.9858\n",
            "Epoch 473/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.0967e-05 - accuracy: 1.0000 - val_loss: 0.1049 - val_accuracy: 0.9858\n",
            "Epoch 474/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.5236e-06 - accuracy: 1.0000 - val_loss: 0.1033 - val_accuracy: 0.9851\n",
            "Epoch 475/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.1621e-06 - accuracy: 1.0000 - val_loss: 0.1119 - val_accuracy: 0.9844\n",
            "Epoch 476/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.1953e-05 - accuracy: 1.0000 - val_loss: 0.0969 - val_accuracy: 0.9872\n",
            "Epoch 477/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.4841e-05 - accuracy: 1.0000 - val_loss: 0.0985 - val_accuracy: 0.9858\n",
            "Epoch 478/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.9269e-05 - accuracy: 1.0000 - val_loss: 0.1054 - val_accuracy: 0.9851\n",
            "Epoch 479/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.6236e-06 - accuracy: 1.0000 - val_loss: 0.1062 - val_accuracy: 0.9851\n",
            "Epoch 480/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.3227e-06 - accuracy: 1.0000 - val_loss: 0.0957 - val_accuracy: 0.9872\n",
            "Epoch 481/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.1599e-06 - accuracy: 1.0000 - val_loss: 0.1040 - val_accuracy: 0.9865\n",
            "Epoch 482/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.9144e-06 - accuracy: 1.0000 - val_loss: 0.1048 - val_accuracy: 0.9858\n",
            "Epoch 483/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.0372e-06 - accuracy: 1.0000 - val_loss: 0.0986 - val_accuracy: 0.9865\n",
            "Epoch 484/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.6403e-06 - accuracy: 1.0000 - val_loss: 0.0948 - val_accuracy: 0.9872\n",
            "Epoch 485/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.3539e-05 - accuracy: 1.0000 - val_loss: 0.1035 - val_accuracy: 0.9872\n",
            "Epoch 486/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.7863e-06 - accuracy: 1.0000 - val_loss: 0.0995 - val_accuracy: 0.9865\n",
            "Epoch 487/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.7662e-06 - accuracy: 1.0000 - val_loss: 0.1034 - val_accuracy: 0.9858\n",
            "Epoch 488/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.7857e-06 - accuracy: 1.0000 - val_loss: 0.1050 - val_accuracy: 0.9851\n",
            "Epoch 489/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.5211e-06 - accuracy: 1.0000 - val_loss: 0.1062 - val_accuracy: 0.9865\n",
            "Epoch 490/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.9388e-06 - accuracy: 1.0000 - val_loss: 0.1034 - val_accuracy: 0.9865\n",
            "Epoch 491/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.4230e-06 - accuracy: 1.0000 - val_loss: 0.1048 - val_accuracy: 0.9858\n",
            "Epoch 492/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.8390e-06 - accuracy: 1.0000 - val_loss: 0.1064 - val_accuracy: 0.9858\n",
            "Epoch 493/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.9102e-06 - accuracy: 1.0000 - val_loss: 0.1198 - val_accuracy: 0.9851\n",
            "Epoch 494/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.1058e-06 - accuracy: 1.0000 - val_loss: 0.1126 - val_accuracy: 0.9836\n",
            "Epoch 495/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.2679e-06 - accuracy: 1.0000 - val_loss: 0.1119 - val_accuracy: 0.9851\n",
            "Epoch 496/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.7779e-07 - accuracy: 1.0000 - val_loss: 0.1126 - val_accuracy: 0.9851\n",
            "Epoch 497/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.4906e-06 - accuracy: 1.0000 - val_loss: 0.1038 - val_accuracy: 0.9851\n",
            "Epoch 498/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.5072e-06 - accuracy: 1.0000 - val_loss: 0.1090 - val_accuracy: 0.9844\n",
            "Epoch 499/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.5655e-06 - accuracy: 1.0000 - val_loss: 0.1118 - val_accuracy: 0.9829\n",
            "Epoch 500/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.5682e-07 - accuracy: 1.0000 - val_loss: 0.1144 - val_accuracy: 0.9858\n",
            "Epoch 501/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.1947e-06 - accuracy: 1.0000 - val_loss: 0.1137 - val_accuracy: 0.9858\n",
            "Epoch 502/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.4195e-06 - accuracy: 1.0000 - val_loss: 0.1055 - val_accuracy: 0.9844\n",
            "Epoch 503/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.7463e-07 - accuracy: 1.0000 - val_loss: 0.1080 - val_accuracy: 0.9851\n",
            "Epoch 504/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.0319e-06 - accuracy: 1.0000 - val_loss: 0.1059 - val_accuracy: 0.9865\n",
            "Epoch 505/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.2013e-06 - accuracy: 1.0000 - val_loss: 0.1293 - val_accuracy: 0.9851\n",
            "Epoch 506/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.5337e-06 - accuracy: 1.0000 - val_loss: 0.1096 - val_accuracy: 0.9865\n",
            "Epoch 507/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.4905e-06 - accuracy: 1.0000 - val_loss: 0.1215 - val_accuracy: 0.9844\n",
            "Epoch 508/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.0728e-06 - accuracy: 1.0000 - val_loss: 0.1118 - val_accuracy: 0.9851\n",
            "Epoch 509/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.4679e-07 - accuracy: 1.0000 - val_loss: 0.1125 - val_accuracy: 0.9851\n",
            "Epoch 510/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.3686e-06 - accuracy: 1.0000 - val_loss: 0.1103 - val_accuracy: 0.9858\n",
            "Epoch 511/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.3358e-06 - accuracy: 1.0000 - val_loss: 0.1159 - val_accuracy: 0.9858\n",
            "Epoch 512/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.4002e-06 - accuracy: 1.0000 - val_loss: 0.1086 - val_accuracy: 0.9844\n",
            "Epoch 513/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.7648e-07 - accuracy: 1.0000 - val_loss: 0.1172 - val_accuracy: 0.9844\n",
            "Epoch 514/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.7493e-06 - accuracy: 1.0000 - val_loss: 0.1221 - val_accuracy: 0.9858\n",
            "Epoch 515/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.4197e-06 - accuracy: 1.0000 - val_loss: 0.1166 - val_accuracy: 0.9851\n",
            "Epoch 516/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.3968e-07 - accuracy: 1.0000 - val_loss: 0.1156 - val_accuracy: 0.9851\n",
            "Epoch 517/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.7975e-06 - accuracy: 1.0000 - val_loss: 0.1234 - val_accuracy: 0.9836\n",
            "Epoch 518/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.6673e-07 - accuracy: 1.0000 - val_loss: 0.1091 - val_accuracy: 0.9844\n",
            "Epoch 519/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.0932e-06 - accuracy: 1.0000 - val_loss: 0.1219 - val_accuracy: 0.9844\n",
            "Epoch 520/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.3890e-06 - accuracy: 1.0000 - val_loss: 0.1246 - val_accuracy: 0.9844\n",
            "Epoch 521/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.1972e-07 - accuracy: 1.0000 - val_loss: 0.1169 - val_accuracy: 0.9836\n",
            "Epoch 522/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.5164e-06 - accuracy: 1.0000 - val_loss: 0.1169 - val_accuracy: 0.9851\n",
            "Epoch 523/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.9980e-07 - accuracy: 1.0000 - val_loss: 0.1148 - val_accuracy: 0.9844\n",
            "Epoch 524/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.2407e-06 - accuracy: 1.0000 - val_loss: 0.1406 - val_accuracy: 0.9836\n",
            "Epoch 525/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.2051e-07 - accuracy: 1.0000 - val_loss: 0.1187 - val_accuracy: 0.9844\n",
            "Epoch 526/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.4675e-07 - accuracy: 1.0000 - val_loss: 0.1298 - val_accuracy: 0.9836\n",
            "Epoch 527/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.9627e-06 - accuracy: 1.0000 - val_loss: 0.1195 - val_accuracy: 0.9844\n",
            "Epoch 528/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.1401e-07 - accuracy: 1.0000 - val_loss: 0.1165 - val_accuracy: 0.9865\n",
            "Epoch 529/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.7348e-06 - accuracy: 1.0000 - val_loss: 0.1261 - val_accuracy: 0.9844\n",
            "Epoch 530/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.3643e-07 - accuracy: 1.0000 - val_loss: 0.1382 - val_accuracy: 0.9836\n",
            "Epoch 531/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.9211e-07 - accuracy: 1.0000 - val_loss: 0.1155 - val_accuracy: 0.9858\n",
            "Epoch 532/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.2891e-07 - accuracy: 1.0000 - val_loss: 0.1303 - val_accuracy: 0.9836\n",
            "Epoch 533/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.9831e-07 - accuracy: 1.0000 - val_loss: 0.1383 - val_accuracy: 0.9844\n",
            "Epoch 534/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.4557e-07 - accuracy: 1.0000 - val_loss: 0.1173 - val_accuracy: 0.9851\n",
            "Epoch 535/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.2657e-07 - accuracy: 1.0000 - val_loss: 0.1290 - val_accuracy: 0.9844\n",
            "Epoch 536/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.1894e-07 - accuracy: 1.0000 - val_loss: 0.1331 - val_accuracy: 0.9836\n",
            "Epoch 537/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.5093e-08 - accuracy: 1.0000 - val_loss: 0.1291 - val_accuracy: 0.9836\n",
            "Epoch 538/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.4598e-07 - accuracy: 1.0000 - val_loss: 0.1272 - val_accuracy: 0.9858\n",
            "Epoch 539/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.4497e-07 - accuracy: 1.0000 - val_loss: 0.1339 - val_accuracy: 0.9836\n",
            "Epoch 540/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.0752e-07 - accuracy: 1.0000 - val_loss: 0.1218 - val_accuracy: 0.9858\n",
            "Epoch 541/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.6826e-07 - accuracy: 1.0000 - val_loss: 0.1295 - val_accuracy: 0.9851\n",
            "Epoch 542/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.8831e-06 - accuracy: 1.0000 - val_loss: 0.1267 - val_accuracy: 0.9844\n",
            "Epoch 543/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.3761e-06 - accuracy: 1.0000 - val_loss: 0.1260 - val_accuracy: 0.9858\n",
            "Epoch 544/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.2943e-07 - accuracy: 1.0000 - val_loss: 0.1255 - val_accuracy: 0.9851\n",
            "Epoch 545/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.3128e-07 - accuracy: 1.0000 - val_loss: 0.1306 - val_accuracy: 0.9844\n",
            "Epoch 546/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.4890e-07 - accuracy: 1.0000 - val_loss: 0.1243 - val_accuracy: 0.9851\n",
            "Epoch 547/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.8346e-07 - accuracy: 1.0000 - val_loss: 0.1275 - val_accuracy: 0.9858\n",
            "Epoch 548/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.5115e-07 - accuracy: 1.0000 - val_loss: 0.1412 - val_accuracy: 0.9836\n",
            "Epoch 549/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.2883e-07 - accuracy: 1.0000 - val_loss: 0.1223 - val_accuracy: 0.9851\n",
            "Epoch 550/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.5963e-07 - accuracy: 1.0000 - val_loss: 0.1245 - val_accuracy: 0.9865\n",
            "Epoch 551/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.3918e-07 - accuracy: 1.0000 - val_loss: 0.1268 - val_accuracy: 0.9851\n",
            "Epoch 552/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.2369e-07 - accuracy: 1.0000 - val_loss: 0.1233 - val_accuracy: 0.9858\n",
            "Epoch 553/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.3217e-07 - accuracy: 1.0000 - val_loss: 0.1328 - val_accuracy: 0.9844\n",
            "Epoch 554/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.7665e-07 - accuracy: 1.0000 - val_loss: 0.1286 - val_accuracy: 0.9844\n",
            "Epoch 555/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.0549e-08 - accuracy: 1.0000 - val_loss: 0.1277 - val_accuracy: 0.9844\n",
            "Epoch 556/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.0458e-07 - accuracy: 1.0000 - val_loss: 0.1356 - val_accuracy: 0.9836\n",
            "Epoch 557/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.6310e-07 - accuracy: 1.0000 - val_loss: 0.1365 - val_accuracy: 0.9851\n",
            "Epoch 558/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.1968e-07 - accuracy: 1.0000 - val_loss: 0.1394 - val_accuracy: 0.9836\n",
            "Epoch 559/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.4298e-07 - accuracy: 1.0000 - val_loss: 0.1308 - val_accuracy: 0.9851\n",
            "Epoch 560/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.3416e-07 - accuracy: 1.0000 - val_loss: 0.1402 - val_accuracy: 0.9815\n",
            "Epoch 561/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.1295e-06 - accuracy: 1.0000 - val_loss: 0.1337 - val_accuracy: 0.9836\n",
            "Epoch 562/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.9156e-07 - accuracy: 1.0000 - val_loss: 0.1281 - val_accuracy: 0.9836\n",
            "Epoch 563/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.7308e-07 - accuracy: 1.0000 - val_loss: 0.1295 - val_accuracy: 0.9858\n",
            "Epoch 564/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.3957e-07 - accuracy: 1.0000 - val_loss: 0.1345 - val_accuracy: 0.9844\n",
            "Epoch 565/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.1167e-07 - accuracy: 1.0000 - val_loss: 0.1340 - val_accuracy: 0.9851\n",
            "Epoch 566/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.2630e-07 - accuracy: 1.0000 - val_loss: 0.1325 - val_accuracy: 0.9836\n",
            "Epoch 567/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.1918e-08 - accuracy: 1.0000 - val_loss: 0.1360 - val_accuracy: 0.9844\n",
            "Epoch 568/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.1411e-08 - accuracy: 1.0000 - val_loss: 0.1287 - val_accuracy: 0.9858\n",
            "Epoch 569/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.7595e-07 - accuracy: 1.0000 - val_loss: 0.1396 - val_accuracy: 0.9836\n",
            "Epoch 570/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.1273e-07 - accuracy: 1.0000 - val_loss: 0.1362 - val_accuracy: 0.9829\n",
            "Epoch 571/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.3869e-08 - accuracy: 1.0000 - val_loss: 0.1398 - val_accuracy: 0.9836\n",
            "Epoch 572/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.9331e-07 - accuracy: 1.0000 - val_loss: 0.1298 - val_accuracy: 0.9844\n",
            "Epoch 573/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.3936e-08 - accuracy: 1.0000 - val_loss: 0.1329 - val_accuracy: 0.9858\n",
            "Epoch 574/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.3022e-08 - accuracy: 1.0000 - val_loss: 0.1284 - val_accuracy: 0.9851\n",
            "Epoch 575/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.6079e-08 - accuracy: 1.0000 - val_loss: 0.1330 - val_accuracy: 0.9851\n",
            "Epoch 576/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.9458e-08 - accuracy: 1.0000 - val_loss: 0.1332 - val_accuracy: 0.9865\n",
            "Epoch 577/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.3280e-08 - accuracy: 1.0000 - val_loss: 0.1323 - val_accuracy: 0.9844\n",
            "Epoch 578/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.2349e-07 - accuracy: 1.0000 - val_loss: 0.1424 - val_accuracy: 0.9829\n",
            "Epoch 579/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.8840e-08 - accuracy: 1.0000 - val_loss: 0.1344 - val_accuracy: 0.9858\n",
            "Epoch 580/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.7754e-07 - accuracy: 1.0000 - val_loss: 0.1332 - val_accuracy: 0.9844\n",
            "Epoch 581/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.5577e-08 - accuracy: 1.0000 - val_loss: 0.1356 - val_accuracy: 0.9851\n",
            "Epoch 582/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.7760e-08 - accuracy: 1.0000 - val_loss: 0.1355 - val_accuracy: 0.9851\n",
            "Epoch 583/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.0853e-07 - accuracy: 1.0000 - val_loss: 0.1406 - val_accuracy: 0.9844\n",
            "Epoch 584/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.5705e-08 - accuracy: 1.0000 - val_loss: 0.1418 - val_accuracy: 0.9844\n",
            "Epoch 585/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.2510e-07 - accuracy: 1.0000 - val_loss: 0.1325 - val_accuracy: 0.9865\n",
            "Epoch 586/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.7447e-08 - accuracy: 1.0000 - val_loss: 0.1374 - val_accuracy: 0.9836\n",
            "Epoch 587/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.8577e-08 - accuracy: 1.0000 - val_loss: 0.1365 - val_accuracy: 0.9836\n",
            "Epoch 588/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.5583e-08 - accuracy: 1.0000 - val_loss: 0.1410 - val_accuracy: 0.9844\n",
            "Epoch 589/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.4672e-08 - accuracy: 1.0000 - val_loss: 0.1345 - val_accuracy: 0.9851\n",
            "Epoch 590/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.8434e-08 - accuracy: 1.0000 - val_loss: 0.1381 - val_accuracy: 0.9858\n",
            "Epoch 591/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.4729e-08 - accuracy: 1.0000 - val_loss: 0.1353 - val_accuracy: 0.9851\n",
            "Epoch 592/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.7045e-08 - accuracy: 1.0000 - val_loss: 0.1383 - val_accuracy: 0.9836\n",
            "Epoch 593/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.9203e-08 - accuracy: 1.0000 - val_loss: 0.1621 - val_accuracy: 0.9844\n",
            "Epoch 594/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.0439e-07 - accuracy: 1.0000 - val_loss: 0.1430 - val_accuracy: 0.9851\n",
            "Epoch 595/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.3648e-08 - accuracy: 1.0000 - val_loss: 0.1391 - val_accuracy: 0.9836\n",
            "Epoch 596/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.5167e-08 - accuracy: 1.0000 - val_loss: 0.1377 - val_accuracy: 0.9836\n",
            "Epoch 597/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.7375e-08 - accuracy: 1.0000 - val_loss: 0.1459 - val_accuracy: 0.9844\n",
            "Epoch 598/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.2601e-07 - accuracy: 1.0000 - val_loss: 0.1429 - val_accuracy: 0.9858\n",
            "Epoch 599/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.8547e-08 - accuracy: 1.0000 - val_loss: 0.1416 - val_accuracy: 0.9844\n",
            "Epoch 600/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.4018e-08 - accuracy: 1.0000 - val_loss: 0.1413 - val_accuracy: 0.9844\n",
            "Epoch 601/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.2689e-08 - accuracy: 1.0000 - val_loss: 0.1439 - val_accuracy: 0.9844\n",
            "Epoch 602/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.5937e-08 - accuracy: 1.0000 - val_loss: 0.1395 - val_accuracy: 0.9851\n",
            "Epoch 603/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.1715e-08 - accuracy: 1.0000 - val_loss: 0.1422 - val_accuracy: 0.9858\n",
            "Epoch 604/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.9430e-08 - accuracy: 1.0000 - val_loss: 0.1417 - val_accuracy: 0.9844\n",
            "Epoch 605/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.8511e-08 - accuracy: 1.0000 - val_loss: 0.1422 - val_accuracy: 0.9844\n",
            "Epoch 606/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.7342e-08 - accuracy: 1.0000 - val_loss: 0.1404 - val_accuracy: 0.9844\n",
            "Epoch 607/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.9839e-08 - accuracy: 1.0000 - val_loss: 0.1408 - val_accuracy: 0.9844\n",
            "Epoch 608/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.2019e-08 - accuracy: 1.0000 - val_loss: 0.1431 - val_accuracy: 0.9858\n",
            "Epoch 609/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.4623e-08 - accuracy: 1.0000 - val_loss: 0.1473 - val_accuracy: 0.9844\n",
            "Epoch 610/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.2659e-08 - accuracy: 1.0000 - val_loss: 0.1385 - val_accuracy: 0.9858\n",
            "Epoch 611/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.5619e-08 - accuracy: 1.0000 - val_loss: 0.1400 - val_accuracy: 0.9858\n",
            "Epoch 612/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.2433e-08 - accuracy: 1.0000 - val_loss: 0.1571 - val_accuracy: 0.9844\n",
            "Epoch 613/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.1555e-08 - accuracy: 1.0000 - val_loss: 0.1432 - val_accuracy: 0.9865\n",
            "Epoch 614/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.1988e-08 - accuracy: 1.0000 - val_loss: 0.1469 - val_accuracy: 0.9836\n",
            "Epoch 615/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.7557e-08 - accuracy: 1.0000 - val_loss: 0.1416 - val_accuracy: 0.9858\n",
            "Epoch 616/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.1896e-08 - accuracy: 1.0000 - val_loss: 0.1421 - val_accuracy: 0.9844\n",
            "Epoch 617/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.0649e-09 - accuracy: 1.0000 - val_loss: 0.1451 - val_accuracy: 0.9851\n",
            "Epoch 618/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.0432e-08 - accuracy: 1.0000 - val_loss: 0.1403 - val_accuracy: 0.9851\n",
            "Epoch 619/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.3458e-08 - accuracy: 1.0000 - val_loss: 0.1438 - val_accuracy: 0.9851\n",
            "Epoch 620/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.3257e-09 - accuracy: 1.0000 - val_loss: 0.1449 - val_accuracy: 0.9858\n",
            "Epoch 621/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.0747e-08 - accuracy: 1.0000 - val_loss: 0.1441 - val_accuracy: 0.9851\n",
            "Epoch 622/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.0218e-08 - accuracy: 1.0000 - val_loss: 0.1437 - val_accuracy: 0.9851\n",
            "Epoch 623/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.4071e-08 - accuracy: 1.0000 - val_loss: 0.1433 - val_accuracy: 0.9844\n",
            "Epoch 624/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.0586e-08 - accuracy: 1.0000 - val_loss: 0.1454 - val_accuracy: 0.9858\n",
            "Epoch 625/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.0738e-08 - accuracy: 1.0000 - val_loss: 0.1459 - val_accuracy: 0.9851\n",
            "Epoch 626/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.9688e-09 - accuracy: 1.0000 - val_loss: 0.1441 - val_accuracy: 0.9858\n",
            "Epoch 627/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.9617e-09 - accuracy: 1.0000 - val_loss: 0.1519 - val_accuracy: 0.9858\n",
            "Epoch 628/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.0216e-09 - accuracy: 1.0000 - val_loss: 0.1447 - val_accuracy: 0.9858\n",
            "Epoch 629/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.0705e-09 - accuracy: 1.0000 - val_loss: 0.1486 - val_accuracy: 0.9851\n",
            "Epoch 630/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.5680e-09 - accuracy: 1.0000 - val_loss: 0.1450 - val_accuracy: 0.9865\n",
            "Epoch 631/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.6141e-09 - accuracy: 1.0000 - val_loss: 0.1446 - val_accuracy: 0.9844\n",
            "Epoch 632/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.4576e-09 - accuracy: 1.0000 - val_loss: 0.1462 - val_accuracy: 0.9858\n",
            "Epoch 633/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.8328e-09 - accuracy: 1.0000 - val_loss: 0.1473 - val_accuracy: 0.9844\n",
            "Epoch 634/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.1128e-09 - accuracy: 1.0000 - val_loss: 0.1493 - val_accuracy: 0.9851\n",
            "Epoch 635/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.1988e-09 - accuracy: 1.0000 - val_loss: 0.1465 - val_accuracy: 0.9844\n",
            "Epoch 636/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.7680e-09 - accuracy: 1.0000 - val_loss: 0.1546 - val_accuracy: 0.9851\n",
            "Epoch 637/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.3684e-09 - accuracy: 1.0000 - val_loss: 0.1463 - val_accuracy: 0.9858\n",
            "Epoch 638/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.5493e-09 - accuracy: 1.0000 - val_loss: 0.1449 - val_accuracy: 0.9858\n",
            "Epoch 639/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.4120e-09 - accuracy: 1.0000 - val_loss: 0.1473 - val_accuracy: 0.9851\n",
            "Epoch 640/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.4180e-09 - accuracy: 1.0000 - val_loss: 0.1516 - val_accuracy: 0.9851\n",
            "Epoch 641/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.2841e-09 - accuracy: 1.0000 - val_loss: 0.1469 - val_accuracy: 0.9858\n",
            "Epoch 642/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.0844e-09 - accuracy: 1.0000 - val_loss: 0.1518 - val_accuracy: 0.9858\n",
            "Epoch 643/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.5942e-09 - accuracy: 1.0000 - val_loss: 0.1504 - val_accuracy: 0.9851\n",
            "Epoch 644/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.3587e-09 - accuracy: 1.0000 - val_loss: 0.1466 - val_accuracy: 0.9851\n",
            "Epoch 645/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.4615e-09 - accuracy: 1.0000 - val_loss: 0.1519 - val_accuracy: 0.9858\n",
            "Epoch 646/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.1234e-09 - accuracy: 1.0000 - val_loss: 0.1469 - val_accuracy: 0.9858\n",
            "Epoch 647/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.9878e-09 - accuracy: 1.0000 - val_loss: 0.1498 - val_accuracy: 0.9858\n",
            "Epoch 648/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.0707e-09 - accuracy: 1.0000 - val_loss: 0.1541 - val_accuracy: 0.9851\n",
            "Epoch 649/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.1119e-09 - accuracy: 1.0000 - val_loss: 0.1487 - val_accuracy: 0.9851\n",
            "Epoch 650/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.8360e-09 - accuracy: 1.0000 - val_loss: 0.1502 - val_accuracy: 0.9851\n",
            "Epoch 651/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.2170e-09 - accuracy: 1.0000 - val_loss: 0.1488 - val_accuracy: 0.9851\n",
            "Epoch 652/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.0633e-09 - accuracy: 1.0000 - val_loss: 0.1536 - val_accuracy: 0.9858\n",
            "Epoch 653/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.8583e-09 - accuracy: 1.0000 - val_loss: 0.1492 - val_accuracy: 0.9844\n",
            "Epoch 654/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.1447e-09 - accuracy: 1.0000 - val_loss: 0.1496 - val_accuracy: 0.9858\n",
            "Epoch 655/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.3739e-09 - accuracy: 1.0000 - val_loss: 0.1497 - val_accuracy: 0.9851\n",
            "Epoch 656/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.9992e-09 - accuracy: 1.0000 - val_loss: 0.1510 - val_accuracy: 0.9851\n",
            "Epoch 657/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.2712e-09 - accuracy: 1.0000 - val_loss: 0.1510 - val_accuracy: 0.9851\n",
            "Epoch 658/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.7601e-09 - accuracy: 1.0000 - val_loss: 0.1486 - val_accuracy: 0.9858\n",
            "Epoch 659/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.1803e-09 - accuracy: 1.0000 - val_loss: 0.1484 - val_accuracy: 0.9851\n",
            "Epoch 660/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.0295e-09 - accuracy: 1.0000 - val_loss: 0.1522 - val_accuracy: 0.9851\n",
            "Epoch 661/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.5700e-09 - accuracy: 1.0000 - val_loss: 0.1472 - val_accuracy: 0.9858\n",
            "Epoch 662/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.0219e-09 - accuracy: 1.0000 - val_loss: 0.1509 - val_accuracy: 0.9851\n",
            "Epoch 663/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.7515e-09 - accuracy: 1.0000 - val_loss: 0.1481 - val_accuracy: 0.9851\n",
            "Epoch 664/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.6267e-09 - accuracy: 1.0000 - val_loss: 0.1506 - val_accuracy: 0.9851\n",
            "Epoch 665/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.7987e-09 - accuracy: 1.0000 - val_loss: 0.1499 - val_accuracy: 0.9851\n",
            "Epoch 666/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.7235e-09 - accuracy: 1.0000 - val_loss: 0.1511 - val_accuracy: 0.9851\n",
            "Epoch 667/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.8011e-09 - accuracy: 1.0000 - val_loss: 0.1508 - val_accuracy: 0.9858\n",
            "Epoch 668/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.5450e-09 - accuracy: 1.0000 - val_loss: 0.1509 - val_accuracy: 0.9844\n",
            "Epoch 669/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.3904e-09 - accuracy: 1.0000 - val_loss: 0.1496 - val_accuracy: 0.9851\n",
            "Epoch 670/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.5337e-09 - accuracy: 1.0000 - val_loss: 0.1501 - val_accuracy: 0.9851\n",
            "Epoch 671/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.4032e-09 - accuracy: 1.0000 - val_loss: 0.1486 - val_accuracy: 0.9851\n",
            "Epoch 672/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.6818e-09 - accuracy: 1.0000 - val_loss: 0.1503 - val_accuracy: 0.9858\n",
            "Epoch 673/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.5107e-09 - accuracy: 1.0000 - val_loss: 0.1512 - val_accuracy: 0.9844\n",
            "Epoch 674/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.3719e-09 - accuracy: 1.0000 - val_loss: 0.1505 - val_accuracy: 0.9858\n",
            "Epoch 675/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.5429e-09 - accuracy: 1.0000 - val_loss: 0.1507 - val_accuracy: 0.9851\n",
            "Epoch 676/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.3845e-09 - accuracy: 1.0000 - val_loss: 0.1504 - val_accuracy: 0.9851\n",
            "Epoch 677/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.3605e-09 - accuracy: 1.0000 - val_loss: 0.1497 - val_accuracy: 0.9858\n",
            "Epoch 678/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.2269e-09 - accuracy: 1.0000 - val_loss: 0.1500 - val_accuracy: 0.9851\n",
            "Epoch 679/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.1815e-09 - accuracy: 1.0000 - val_loss: 0.1519 - val_accuracy: 0.9851\n",
            "Epoch 680/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.1459e-09 - accuracy: 1.0000 - val_loss: 0.1497 - val_accuracy: 0.9851\n",
            "Epoch 681/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.0725e-09 - accuracy: 1.0000 - val_loss: 0.1512 - val_accuracy: 0.9851\n",
            "Epoch 682/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.0420e-09 - accuracy: 1.0000 - val_loss: 0.1492 - val_accuracy: 0.9858\n",
            "Epoch 683/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.1717e-09 - accuracy: 1.0000 - val_loss: 0.1506 - val_accuracy: 0.9851\n",
            "Epoch 684/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.0944e-09 - accuracy: 1.0000 - val_loss: 0.1530 - val_accuracy: 0.9858\n",
            "Epoch 685/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.0262e-09 - accuracy: 1.0000 - val_loss: 0.1533 - val_accuracy: 0.9844\n",
            "Epoch 686/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.8927e-09 - accuracy: 1.0000 - val_loss: 0.1510 - val_accuracy: 0.9851\n",
            "Epoch 687/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.9269e-09 - accuracy: 1.0000 - val_loss: 0.1530 - val_accuracy: 0.9858\n",
            "Epoch 688/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.9610e-09 - accuracy: 1.0000 - val_loss: 0.1506 - val_accuracy: 0.9851\n",
            "Epoch 689/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.8977e-09 - accuracy: 1.0000 - val_loss: 0.1527 - val_accuracy: 0.9851\n",
            "Epoch 690/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.0487e-09 - accuracy: 1.0000 - val_loss: 0.1511 - val_accuracy: 0.9851\n",
            "Epoch 691/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.9191e-09 - accuracy: 1.0000 - val_loss: 0.1518 - val_accuracy: 0.9851\n",
            "Epoch 692/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.8714e-09 - accuracy: 1.0000 - val_loss: 0.1504 - val_accuracy: 0.9858\n",
            "Epoch 693/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.9672e-09 - accuracy: 1.0000 - val_loss: 0.1517 - val_accuracy: 0.9858\n",
            "Epoch 694/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.8296e-09 - accuracy: 1.0000 - val_loss: 0.1528 - val_accuracy: 0.9858\n",
            "Epoch 695/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.7782e-09 - accuracy: 1.0000 - val_loss: 0.1519 - val_accuracy: 0.9851\n",
            "Epoch 696/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.8452e-09 - accuracy: 1.0000 - val_loss: 0.1513 - val_accuracy: 0.9858\n",
            "Epoch 697/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.8053e-09 - accuracy: 1.0000 - val_loss: 0.1512 - val_accuracy: 0.9858\n",
            "Epoch 698/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.7988e-09 - accuracy: 1.0000 - val_loss: 0.1539 - val_accuracy: 0.9858\n",
            "Epoch 699/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.8256e-09 - accuracy: 1.0000 - val_loss: 0.1530 - val_accuracy: 0.9851\n",
            "Epoch 700/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.7889e-09 - accuracy: 1.0000 - val_loss: 0.1531 - val_accuracy: 0.9851\n",
            "Epoch 701/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.6674e-09 - accuracy: 1.0000 - val_loss: 0.1518 - val_accuracy: 0.9858\n",
            "Epoch 702/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.7706e-09 - accuracy: 1.0000 - val_loss: 0.1526 - val_accuracy: 0.9851\n",
            "Epoch 703/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.7749e-09 - accuracy: 1.0000 - val_loss: 0.1519 - val_accuracy: 0.9851\n",
            "Epoch 704/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.6343e-09 - accuracy: 1.0000 - val_loss: 0.1520 - val_accuracy: 0.9851\n",
            "Epoch 705/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.6697e-09 - accuracy: 1.0000 - val_loss: 0.1518 - val_accuracy: 0.9851\n",
            "Epoch 706/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.7223e-09 - accuracy: 1.0000 - val_loss: 0.1529 - val_accuracy: 0.9851\n",
            "Epoch 707/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.6600e-09 - accuracy: 1.0000 - val_loss: 0.1529 - val_accuracy: 0.9851\n",
            "Epoch 708/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.6210e-09 - accuracy: 1.0000 - val_loss: 0.1516 - val_accuracy: 0.9858\n",
            "Epoch 709/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.6617e-09 - accuracy: 1.0000 - val_loss: 0.1519 - val_accuracy: 0.9851\n",
            "Epoch 710/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.5586e-09 - accuracy: 1.0000 - val_loss: 0.1527 - val_accuracy: 0.9851\n",
            "Epoch 711/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.6009e-09 - accuracy: 1.0000 - val_loss: 0.1537 - val_accuracy: 0.9851\n",
            "Epoch 712/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.5707e-09 - accuracy: 1.0000 - val_loss: 0.1529 - val_accuracy: 0.9858\n",
            "Epoch 713/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.5552e-09 - accuracy: 1.0000 - val_loss: 0.1537 - val_accuracy: 0.9851\n",
            "Epoch 714/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.5691e-09 - accuracy: 1.0000 - val_loss: 0.1531 - val_accuracy: 0.9851\n",
            "Epoch 715/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.5265e-09 - accuracy: 1.0000 - val_loss: 0.1542 - val_accuracy: 0.9858\n",
            "Epoch 716/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.5206e-09 - accuracy: 1.0000 - val_loss: 0.1523 - val_accuracy: 0.9851\n",
            "Epoch 717/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.5332e-09 - accuracy: 1.0000 - val_loss: 0.1520 - val_accuracy: 0.9858\n",
            "Epoch 718/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.4944e-09 - accuracy: 1.0000 - val_loss: 0.1529 - val_accuracy: 0.9858\n",
            "Epoch 719/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.5238e-09 - accuracy: 1.0000 - val_loss: 0.1527 - val_accuracy: 0.9851\n",
            "Epoch 720/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.4937e-09 - accuracy: 1.0000 - val_loss: 0.1540 - val_accuracy: 0.9851\n",
            "Epoch 721/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.5003e-09 - accuracy: 1.0000 - val_loss: 0.1531 - val_accuracy: 0.9851\n",
            "Epoch 722/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.5113e-09 - accuracy: 1.0000 - val_loss: 0.1533 - val_accuracy: 0.9851\n",
            "Epoch 723/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.4387e-09 - accuracy: 1.0000 - val_loss: 0.1527 - val_accuracy: 0.9851\n",
            "Epoch 724/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.4553e-09 - accuracy: 1.0000 - val_loss: 0.1529 - val_accuracy: 0.9858\n",
            "Epoch 725/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.4177e-09 - accuracy: 1.0000 - val_loss: 0.1526 - val_accuracy: 0.9851\n",
            "Epoch 726/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.3992e-09 - accuracy: 1.0000 - val_loss: 0.1545 - val_accuracy: 0.9851\n",
            "Epoch 727/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.4946e-09 - accuracy: 1.0000 - val_loss: 0.1539 - val_accuracy: 0.9851\n",
            "Epoch 728/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.4288e-09 - accuracy: 1.0000 - val_loss: 0.1536 - val_accuracy: 0.9851\n",
            "Epoch 729/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.3598e-09 - accuracy: 1.0000 - val_loss: 0.1531 - val_accuracy: 0.9851\n",
            "Epoch 730/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.4220e-09 - accuracy: 1.0000 - val_loss: 0.1527 - val_accuracy: 0.9851\n",
            "Epoch 731/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.3780e-09 - accuracy: 1.0000 - val_loss: 0.1538 - val_accuracy: 0.9851\n",
            "Epoch 732/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.3380e-09 - accuracy: 1.0000 - val_loss: 0.1533 - val_accuracy: 0.9851\n",
            "Epoch 733/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.3487e-09 - accuracy: 1.0000 - val_loss: 0.1530 - val_accuracy: 0.9851\n",
            "Epoch 734/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.3407e-09 - accuracy: 1.0000 - val_loss: 0.1537 - val_accuracy: 0.9851\n",
            "Epoch 735/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.3722e-09 - accuracy: 1.0000 - val_loss: 0.1540 - val_accuracy: 0.9851\n",
            "Epoch 736/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.3166e-09 - accuracy: 1.0000 - val_loss: 0.1526 - val_accuracy: 0.9858\n",
            "Epoch 737/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.3631e-09 - accuracy: 1.0000 - val_loss: 0.1538 - val_accuracy: 0.9851\n",
            "Epoch 738/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.3211e-09 - accuracy: 1.0000 - val_loss: 0.1536 - val_accuracy: 0.9851\n",
            "Epoch 739/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.3318e-09 - accuracy: 1.0000 - val_loss: 0.1537 - val_accuracy: 0.9851\n",
            "Epoch 740/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.2385e-09 - accuracy: 1.0000 - val_loss: 0.1535 - val_accuracy: 0.9851\n",
            "Epoch 741/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.2499e-09 - accuracy: 1.0000 - val_loss: 0.1550 - val_accuracy: 0.9858\n",
            "Epoch 742/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.2575e-09 - accuracy: 1.0000 - val_loss: 0.1536 - val_accuracy: 0.9851\n",
            "Epoch 743/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.2456e-09 - accuracy: 1.0000 - val_loss: 0.1533 - val_accuracy: 0.9851\n",
            "Epoch 744/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.2630e-09 - accuracy: 1.0000 - val_loss: 0.1543 - val_accuracy: 0.9851\n",
            "Epoch 745/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.2427e-09 - accuracy: 1.0000 - val_loss: 0.1538 - val_accuracy: 0.9851\n",
            "Epoch 746/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.2540e-09 - accuracy: 1.0000 - val_loss: 0.1537 - val_accuracy: 0.9851\n",
            "Epoch 747/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.2446e-09 - accuracy: 1.0000 - val_loss: 0.1535 - val_accuracy: 0.9851\n",
            "Epoch 748/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.2294e-09 - accuracy: 1.0000 - val_loss: 0.1537 - val_accuracy: 0.9858\n",
            "Epoch 749/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.2208e-09 - accuracy: 1.0000 - val_loss: 0.1543 - val_accuracy: 0.9851\n",
            "Epoch 750/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.2266e-09 - accuracy: 1.0000 - val_loss: 0.1539 - val_accuracy: 0.9851\n",
            "Epoch 751/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.2193e-09 - accuracy: 1.0000 - val_loss: 0.1540 - val_accuracy: 0.9851\n",
            "Epoch 752/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.2155e-09 - accuracy: 1.0000 - val_loss: 0.1540 - val_accuracy: 0.9858\n",
            "Epoch 753/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.2007e-09 - accuracy: 1.0000 - val_loss: 0.1544 - val_accuracy: 0.9851\n",
            "Epoch 754/1056\n",
            "103/103 [==============================] - 0s 5ms/step - loss: 1.2004e-09 - accuracy: 1.0000 - val_loss: 0.1545 - val_accuracy: 0.9858\n",
            "Epoch 755/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.1773e-09 - accuracy: 1.0000 - val_loss: 0.1553 - val_accuracy: 0.9858\n",
            "Epoch 756/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.1809e-09 - accuracy: 1.0000 - val_loss: 0.1552 - val_accuracy: 0.9858\n",
            "Epoch 757/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.1293e-09 - accuracy: 1.0000 - val_loss: 0.1536 - val_accuracy: 0.9858\n",
            "Epoch 758/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.1733e-09 - accuracy: 1.0000 - val_loss: 0.1536 - val_accuracy: 0.9851\n",
            "Epoch 759/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.1612e-09 - accuracy: 1.0000 - val_loss: 0.1539 - val_accuracy: 0.9858\n",
            "Epoch 760/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.1604e-09 - accuracy: 1.0000 - val_loss: 0.1541 - val_accuracy: 0.9858\n",
            "Epoch 761/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.1402e-09 - accuracy: 1.0000 - val_loss: 0.1545 - val_accuracy: 0.9851\n",
            "Epoch 762/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.1540e-09 - accuracy: 1.0000 - val_loss: 0.1545 - val_accuracy: 0.9851\n",
            "Epoch 763/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.1357e-09 - accuracy: 1.0000 - val_loss: 0.1552 - val_accuracy: 0.9858\n",
            "Epoch 764/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.1321e-09 - accuracy: 1.0000 - val_loss: 0.1542 - val_accuracy: 0.9851\n",
            "Epoch 765/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.1313e-09 - accuracy: 1.0000 - val_loss: 0.1548 - val_accuracy: 0.9851\n",
            "Epoch 766/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.1340e-09 - accuracy: 1.0000 - val_loss: 0.1544 - val_accuracy: 0.9851\n",
            "Epoch 767/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.0990e-09 - accuracy: 1.0000 - val_loss: 0.1544 - val_accuracy: 0.9865\n",
            "Epoch 768/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.1238e-09 - accuracy: 1.0000 - val_loss: 0.1544 - val_accuracy: 0.9851\n",
            "Epoch 769/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.1050e-09 - accuracy: 1.0000 - val_loss: 0.1553 - val_accuracy: 0.9858\n",
            "Epoch 770/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.0847e-09 - accuracy: 1.0000 - val_loss: 0.1543 - val_accuracy: 0.9851\n",
            "Epoch 771/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.0819e-09 - accuracy: 1.0000 - val_loss: 0.1549 - val_accuracy: 0.9851\n",
            "Epoch 772/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.0998e-09 - accuracy: 1.0000 - val_loss: 0.1548 - val_accuracy: 0.9858\n",
            "Epoch 773/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.0912e-09 - accuracy: 1.0000 - val_loss: 0.1554 - val_accuracy: 0.9858\n",
            "Epoch 774/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.0837e-09 - accuracy: 1.0000 - val_loss: 0.1556 - val_accuracy: 0.9865\n",
            "Epoch 775/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.0761e-09 - accuracy: 1.0000 - val_loss: 0.1556 - val_accuracy: 0.9865\n",
            "Epoch 776/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.0870e-09 - accuracy: 1.0000 - val_loss: 0.1545 - val_accuracy: 0.9865\n",
            "Epoch 777/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.0816e-09 - accuracy: 1.0000 - val_loss: 0.1544 - val_accuracy: 0.9865\n",
            "Epoch 778/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.0617e-09 - accuracy: 1.0000 - val_loss: 0.1540 - val_accuracy: 0.9851\n",
            "Epoch 779/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.0450e-09 - accuracy: 1.0000 - val_loss: 0.1542 - val_accuracy: 0.9865\n",
            "Epoch 780/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.0540e-09 - accuracy: 1.0000 - val_loss: 0.1550 - val_accuracy: 0.9851\n",
            "Epoch 781/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.0594e-09 - accuracy: 1.0000 - val_loss: 0.1551 - val_accuracy: 0.9851\n",
            "Epoch 782/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.0617e-09 - accuracy: 1.0000 - val_loss: 0.1550 - val_accuracy: 0.9858\n",
            "Epoch 783/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.0387e-09 - accuracy: 1.0000 - val_loss: 0.1545 - val_accuracy: 0.9858\n",
            "Epoch 784/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.0386e-09 - accuracy: 1.0000 - val_loss: 0.1545 - val_accuracy: 0.9851\n",
            "Epoch 785/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.0419e-09 - accuracy: 1.0000 - val_loss: 0.1548 - val_accuracy: 0.9865\n",
            "Epoch 786/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.0430e-09 - accuracy: 1.0000 - val_loss: 0.1550 - val_accuracy: 0.9858\n",
            "Epoch 787/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.0308e-09 - accuracy: 1.0000 - val_loss: 0.1555 - val_accuracy: 0.9858\n",
            "Epoch 788/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.0518e-09 - accuracy: 1.0000 - val_loss: 0.1548 - val_accuracy: 0.9858\n",
            "Epoch 789/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.0212e-09 - accuracy: 1.0000 - val_loss: 0.1554 - val_accuracy: 0.9865\n",
            "Epoch 790/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.0345e-09 - accuracy: 1.0000 - val_loss: 0.1553 - val_accuracy: 0.9865\n",
            "Epoch 791/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.0175e-09 - accuracy: 1.0000 - val_loss: 0.1549 - val_accuracy: 0.9865\n",
            "Epoch 792/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.0125e-09 - accuracy: 1.0000 - val_loss: 0.1545 - val_accuracy: 0.9851\n",
            "Epoch 793/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.9737e-10 - accuracy: 1.0000 - val_loss: 0.1553 - val_accuracy: 0.9858\n",
            "Epoch 794/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.0076e-09 - accuracy: 1.0000 - val_loss: 0.1555 - val_accuracy: 0.9865\n",
            "Epoch 795/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.0023e-09 - accuracy: 1.0000 - val_loss: 0.1549 - val_accuracy: 0.9851\n",
            "Epoch 796/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.8251e-10 - accuracy: 1.0000 - val_loss: 0.1551 - val_accuracy: 0.9858\n",
            "Epoch 797/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.0067e-09 - accuracy: 1.0000 - val_loss: 0.1553 - val_accuracy: 0.9865\n",
            "Epoch 798/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.7931e-10 - accuracy: 1.0000 - val_loss: 0.1560 - val_accuracy: 0.9865\n",
            "Epoch 799/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.9756e-10 - accuracy: 1.0000 - val_loss: 0.1553 - val_accuracy: 0.9865\n",
            "Epoch 800/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.9760e-10 - accuracy: 1.0000 - val_loss: 0.1552 - val_accuracy: 0.9865\n",
            "Epoch 801/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.7444e-10 - accuracy: 1.0000 - val_loss: 0.1550 - val_accuracy: 0.9865\n",
            "Epoch 802/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.8129e-10 - accuracy: 1.0000 - val_loss: 0.1555 - val_accuracy: 0.9865\n",
            "Epoch 803/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.6634e-10 - accuracy: 1.0000 - val_loss: 0.1560 - val_accuracy: 0.9865\n",
            "Epoch 804/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.7167e-10 - accuracy: 1.0000 - val_loss: 0.1554 - val_accuracy: 0.9858\n",
            "Epoch 805/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.5948e-10 - accuracy: 1.0000 - val_loss: 0.1552 - val_accuracy: 0.9865\n",
            "Epoch 806/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.6969e-10 - accuracy: 1.0000 - val_loss: 0.1557 - val_accuracy: 0.9865\n",
            "Epoch 807/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.5969e-10 - accuracy: 1.0000 - val_loss: 0.1552 - val_accuracy: 0.9865\n",
            "Epoch 808/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.6004e-10 - accuracy: 1.0000 - val_loss: 0.1553 - val_accuracy: 0.9865\n",
            "Epoch 809/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.6523e-10 - accuracy: 1.0000 - val_loss: 0.1552 - val_accuracy: 0.9865\n",
            "Epoch 810/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.4474e-10 - accuracy: 1.0000 - val_loss: 0.1553 - val_accuracy: 0.9865\n",
            "Epoch 811/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.4338e-10 - accuracy: 1.0000 - val_loss: 0.1561 - val_accuracy: 0.9865\n",
            "Epoch 812/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.4953e-10 - accuracy: 1.0000 - val_loss: 0.1554 - val_accuracy: 0.9858\n",
            "Epoch 813/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.2971e-10 - accuracy: 1.0000 - val_loss: 0.1555 - val_accuracy: 0.9865\n",
            "Epoch 814/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.2831e-10 - accuracy: 1.0000 - val_loss: 0.1560 - val_accuracy: 0.9865\n",
            "Epoch 815/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.2513e-10 - accuracy: 1.0000 - val_loss: 0.1559 - val_accuracy: 0.9865\n",
            "Epoch 816/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.3506e-10 - accuracy: 1.0000 - val_loss: 0.1556 - val_accuracy: 0.9865\n",
            "Epoch 817/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.1740e-10 - accuracy: 1.0000 - val_loss: 0.1558 - val_accuracy: 0.9858\n",
            "Epoch 818/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.3193e-10 - accuracy: 1.0000 - val_loss: 0.1559 - val_accuracy: 0.9865\n",
            "Epoch 819/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.0204e-10 - accuracy: 1.0000 - val_loss: 0.1559 - val_accuracy: 0.9865\n",
            "Epoch 820/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.0958e-10 - accuracy: 1.0000 - val_loss: 0.1562 - val_accuracy: 0.9865\n",
            "Epoch 821/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.0943e-10 - accuracy: 1.0000 - val_loss: 0.1558 - val_accuracy: 0.9865\n",
            "Epoch 822/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.1364e-10 - accuracy: 1.0000 - val_loss: 0.1557 - val_accuracy: 0.9865\n",
            "Epoch 823/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.0949e-10 - accuracy: 1.0000 - val_loss: 0.1563 - val_accuracy: 0.9865\n",
            "Epoch 824/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.1788e-10 - accuracy: 1.0000 - val_loss: 0.1556 - val_accuracy: 0.9865\n",
            "Epoch 825/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.8205e-10 - accuracy: 1.0000 - val_loss: 0.1557 - val_accuracy: 0.9865\n",
            "Epoch 826/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.9402e-10 - accuracy: 1.0000 - val_loss: 0.1559 - val_accuracy: 0.9865\n",
            "Epoch 827/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.9545e-10 - accuracy: 1.0000 - val_loss: 0.1557 - val_accuracy: 0.9865\n",
            "Epoch 828/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.9275e-10 - accuracy: 1.0000 - val_loss: 0.1568 - val_accuracy: 0.9865\n",
            "Epoch 829/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.9206e-10 - accuracy: 1.0000 - val_loss: 0.1562 - val_accuracy: 0.9865\n",
            "Epoch 830/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.8822e-10 - accuracy: 1.0000 - val_loss: 0.1558 - val_accuracy: 0.9865\n",
            "Epoch 831/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.8861e-10 - accuracy: 1.0000 - val_loss: 0.1560 - val_accuracy: 0.9865\n",
            "Epoch 832/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.8065e-10 - accuracy: 1.0000 - val_loss: 0.1559 - val_accuracy: 0.9865\n",
            "Epoch 833/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.6926e-10 - accuracy: 1.0000 - val_loss: 0.1564 - val_accuracy: 0.9865\n",
            "Epoch 834/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.7201e-10 - accuracy: 1.0000 - val_loss: 0.1558 - val_accuracy: 0.9865\n",
            "Epoch 835/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.9244e-10 - accuracy: 1.0000 - val_loss: 0.1562 - val_accuracy: 0.9865\n",
            "Epoch 836/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.6723e-10 - accuracy: 1.0000 - val_loss: 0.1561 - val_accuracy: 0.9865\n",
            "Epoch 837/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.7772e-10 - accuracy: 1.0000 - val_loss: 0.1564 - val_accuracy: 0.9865\n",
            "Epoch 838/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.6908e-10 - accuracy: 1.0000 - val_loss: 0.1564 - val_accuracy: 0.9865\n",
            "Epoch 839/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.8048e-10 - accuracy: 1.0000 - val_loss: 0.1564 - val_accuracy: 0.9865\n",
            "Epoch 840/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.5427e-10 - accuracy: 1.0000 - val_loss: 0.1560 - val_accuracy: 0.9865\n",
            "Epoch 841/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.6417e-10 - accuracy: 1.0000 - val_loss: 0.1560 - val_accuracy: 0.9865\n",
            "Epoch 842/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.6574e-10 - accuracy: 1.0000 - val_loss: 0.1562 - val_accuracy: 0.9865\n",
            "Epoch 843/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.6515e-10 - accuracy: 1.0000 - val_loss: 0.1566 - val_accuracy: 0.9865\n",
            "Epoch 844/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.7251e-10 - accuracy: 1.0000 - val_loss: 0.1562 - val_accuracy: 0.9865\n",
            "Epoch 845/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.5595e-10 - accuracy: 1.0000 - val_loss: 0.1562 - val_accuracy: 0.9865\n",
            "Epoch 846/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.5344e-10 - accuracy: 1.0000 - val_loss: 0.1567 - val_accuracy: 0.9865\n",
            "Epoch 847/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.4774e-10 - accuracy: 1.0000 - val_loss: 0.1561 - val_accuracy: 0.9865\n",
            "Epoch 848/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.5208e-10 - accuracy: 1.0000 - val_loss: 0.1568 - val_accuracy: 0.9865\n",
            "Epoch 849/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.4960e-10 - accuracy: 1.0000 - val_loss: 0.1565 - val_accuracy: 0.9865\n",
            "Epoch 850/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.4029e-10 - accuracy: 1.0000 - val_loss: 0.1567 - val_accuracy: 0.9865\n",
            "Epoch 851/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.2829e-10 - accuracy: 1.0000 - val_loss: 0.1566 - val_accuracy: 0.9865\n",
            "Epoch 852/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.3159e-10 - accuracy: 1.0000 - val_loss: 0.1566 - val_accuracy: 0.9865\n",
            "Epoch 853/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.4088e-10 - accuracy: 1.0000 - val_loss: 0.1571 - val_accuracy: 0.9865\n",
            "Epoch 854/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.3416e-10 - accuracy: 1.0000 - val_loss: 0.1569 - val_accuracy: 0.9865\n",
            "Epoch 855/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.1516e-10 - accuracy: 1.0000 - val_loss: 0.1572 - val_accuracy: 0.9865\n",
            "Epoch 856/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.3069e-10 - accuracy: 1.0000 - val_loss: 0.1568 - val_accuracy: 0.9865\n",
            "Epoch 857/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.0767e-10 - accuracy: 1.0000 - val_loss: 0.1571 - val_accuracy: 0.9865\n",
            "Epoch 858/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.2862e-10 - accuracy: 1.0000 - val_loss: 0.1569 - val_accuracy: 0.9865\n",
            "Epoch 859/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.0671e-10 - accuracy: 1.0000 - val_loss: 0.1568 - val_accuracy: 0.9865\n",
            "Epoch 860/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.2817e-10 - accuracy: 1.0000 - val_loss: 0.1571 - val_accuracy: 0.9865\n",
            "Epoch 861/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.1967e-10 - accuracy: 1.0000 - val_loss: 0.1570 - val_accuracy: 0.9865\n",
            "Epoch 862/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.2280e-10 - accuracy: 1.0000 - val_loss: 0.1573 - val_accuracy: 0.9865\n",
            "Epoch 863/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.1569e-10 - accuracy: 1.0000 - val_loss: 0.1568 - val_accuracy: 0.9865\n",
            "Epoch 864/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.0625e-10 - accuracy: 1.0000 - val_loss: 0.1572 - val_accuracy: 0.9865\n",
            "Epoch 865/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.1000e-10 - accuracy: 1.0000 - val_loss: 0.1567 - val_accuracy: 0.9865\n",
            "Epoch 866/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.0224e-10 - accuracy: 1.0000 - val_loss: 0.1565 - val_accuracy: 0.9865\n",
            "Epoch 867/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.0989e-10 - accuracy: 1.0000 - val_loss: 0.1573 - val_accuracy: 0.9865\n",
            "Epoch 868/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.1524e-10 - accuracy: 1.0000 - val_loss: 0.1573 - val_accuracy: 0.9865\n",
            "Epoch 869/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.0745e-10 - accuracy: 1.0000 - val_loss: 0.1573 - val_accuracy: 0.9865\n",
            "Epoch 870/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.9930e-10 - accuracy: 1.0000 - val_loss: 0.1574 - val_accuracy: 0.9865\n",
            "Epoch 871/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.9624e-10 - accuracy: 1.0000 - val_loss: 0.1571 - val_accuracy: 0.9865\n",
            "Epoch 872/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.8676e-10 - accuracy: 1.0000 - val_loss: 0.1569 - val_accuracy: 0.9865\n",
            "Epoch 873/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.9658e-10 - accuracy: 1.0000 - val_loss: 0.1572 - val_accuracy: 0.9865\n",
            "Epoch 874/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.8020e-10 - accuracy: 1.0000 - val_loss: 0.1568 - val_accuracy: 0.9865\n",
            "Epoch 875/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.9470e-10 - accuracy: 1.0000 - val_loss: 0.1572 - val_accuracy: 0.9865\n",
            "Epoch 876/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.9992e-10 - accuracy: 1.0000 - val_loss: 0.1572 - val_accuracy: 0.9865\n",
            "Epoch 877/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.9321e-10 - accuracy: 1.0000 - val_loss: 0.1569 - val_accuracy: 0.9865\n",
            "Epoch 878/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.8441e-10 - accuracy: 1.0000 - val_loss: 0.1573 - val_accuracy: 0.9865\n",
            "Epoch 879/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.7621e-10 - accuracy: 1.0000 - val_loss: 0.1574 - val_accuracy: 0.9865\n",
            "Epoch 880/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.8842e-10 - accuracy: 1.0000 - val_loss: 0.1575 - val_accuracy: 0.9865\n",
            "Epoch 881/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.8354e-10 - accuracy: 1.0000 - val_loss: 0.1577 - val_accuracy: 0.9865\n",
            "Epoch 882/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.9002e-10 - accuracy: 1.0000 - val_loss: 0.1577 - val_accuracy: 0.9865\n",
            "Epoch 883/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.8663e-10 - accuracy: 1.0000 - val_loss: 0.1577 - val_accuracy: 0.9865\n",
            "Epoch 884/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.8116e-10 - accuracy: 1.0000 - val_loss: 0.1577 - val_accuracy: 0.9865\n",
            "Epoch 885/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.6814e-10 - accuracy: 1.0000 - val_loss: 0.1579 - val_accuracy: 0.9865\n",
            "Epoch 886/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.6913e-10 - accuracy: 1.0000 - val_loss: 0.1574 - val_accuracy: 0.9865\n",
            "Epoch 887/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.6498e-10 - accuracy: 1.0000 - val_loss: 0.1572 - val_accuracy: 0.9865\n",
            "Epoch 888/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.7754e-10 - accuracy: 1.0000 - val_loss: 0.1574 - val_accuracy: 0.9865\n",
            "Epoch 889/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.7368e-10 - accuracy: 1.0000 - val_loss: 0.1576 - val_accuracy: 0.9865\n",
            "Epoch 890/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.6307e-10 - accuracy: 1.0000 - val_loss: 0.1577 - val_accuracy: 0.9865\n",
            "Epoch 891/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.7300e-10 - accuracy: 1.0000 - val_loss: 0.1579 - val_accuracy: 0.9865\n",
            "Epoch 892/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.7139e-10 - accuracy: 1.0000 - val_loss: 0.1575 - val_accuracy: 0.9865\n",
            "Epoch 893/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.6124e-10 - accuracy: 1.0000 - val_loss: 0.1579 - val_accuracy: 0.9865\n",
            "Epoch 894/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.6912e-10 - accuracy: 1.0000 - val_loss: 0.1579 - val_accuracy: 0.9865\n",
            "Epoch 895/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.5278e-10 - accuracy: 1.0000 - val_loss: 0.1584 - val_accuracy: 0.9865\n",
            "Epoch 896/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.5873e-10 - accuracy: 1.0000 - val_loss: 0.1581 - val_accuracy: 0.9865\n",
            "Epoch 897/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.6528e-10 - accuracy: 1.0000 - val_loss: 0.1583 - val_accuracy: 0.9865\n",
            "Epoch 898/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.5244e-10 - accuracy: 1.0000 - val_loss: 0.1573 - val_accuracy: 0.9865\n",
            "Epoch 899/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.4274e-10 - accuracy: 1.0000 - val_loss: 0.1579 - val_accuracy: 0.9865\n",
            "Epoch 900/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.4548e-10 - accuracy: 1.0000 - val_loss: 0.1578 - val_accuracy: 0.9865\n",
            "Epoch 901/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.5125e-10 - accuracy: 1.0000 - val_loss: 0.1580 - val_accuracy: 0.9865\n",
            "Epoch 902/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.4161e-10 - accuracy: 1.0000 - val_loss: 0.1585 - val_accuracy: 0.9865\n",
            "Epoch 903/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.4517e-10 - accuracy: 1.0000 - val_loss: 0.1581 - val_accuracy: 0.9865\n",
            "Epoch 904/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.4392e-10 - accuracy: 1.0000 - val_loss: 0.1581 - val_accuracy: 0.9865\n",
            "Epoch 905/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.4101e-10 - accuracy: 1.0000 - val_loss: 0.1585 - val_accuracy: 0.9865\n",
            "Epoch 906/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.3975e-10 - accuracy: 1.0000 - val_loss: 0.1580 - val_accuracy: 0.9865\n",
            "Epoch 907/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.3705e-10 - accuracy: 1.0000 - val_loss: 0.1577 - val_accuracy: 0.9865\n",
            "Epoch 908/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.3334e-10 - accuracy: 1.0000 - val_loss: 0.1582 - val_accuracy: 0.9865\n",
            "Epoch 909/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.3643e-10 - accuracy: 1.0000 - val_loss: 0.1580 - val_accuracy: 0.9865\n",
            "Epoch 910/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.1898e-10 - accuracy: 1.0000 - val_loss: 0.1583 - val_accuracy: 0.9865\n",
            "Epoch 911/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.3199e-10 - accuracy: 1.0000 - val_loss: 0.1580 - val_accuracy: 0.9865\n",
            "Epoch 912/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.2830e-10 - accuracy: 1.0000 - val_loss: 0.1583 - val_accuracy: 0.9865\n",
            "Epoch 913/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.2720e-10 - accuracy: 1.0000 - val_loss: 0.1581 - val_accuracy: 0.9865\n",
            "Epoch 914/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.3727e-10 - accuracy: 1.0000 - val_loss: 0.1579 - val_accuracy: 0.9865\n",
            "Epoch 915/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.3599e-10 - accuracy: 1.0000 - val_loss: 0.1580 - val_accuracy: 0.9865\n",
            "Epoch 916/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.3326e-10 - accuracy: 1.0000 - val_loss: 0.1586 - val_accuracy: 0.9865\n",
            "Epoch 917/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.1872e-10 - accuracy: 1.0000 - val_loss: 0.1579 - val_accuracy: 0.9865\n",
            "Epoch 918/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.2394e-10 - accuracy: 1.0000 - val_loss: 0.1580 - val_accuracy: 0.9865\n",
            "Epoch 919/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.2502e-10 - accuracy: 1.0000 - val_loss: 0.1582 - val_accuracy: 0.9865\n",
            "Epoch 920/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.1812e-10 - accuracy: 1.0000 - val_loss: 0.1582 - val_accuracy: 0.9865\n",
            "Epoch 921/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.2713e-10 - accuracy: 1.0000 - val_loss: 0.1585 - val_accuracy: 0.9865\n",
            "Epoch 922/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.2298e-10 - accuracy: 1.0000 - val_loss: 0.1582 - val_accuracy: 0.9865\n",
            "Epoch 923/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.2152e-10 - accuracy: 1.0000 - val_loss: 0.1582 - val_accuracy: 0.9865\n",
            "Epoch 924/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.2389e-10 - accuracy: 1.0000 - val_loss: 0.1585 - val_accuracy: 0.9865\n",
            "Epoch 925/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.0452e-10 - accuracy: 1.0000 - val_loss: 0.1584 - val_accuracy: 0.9865\n",
            "Epoch 926/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.1205e-10 - accuracy: 1.0000 - val_loss: 0.1582 - val_accuracy: 0.9865\n",
            "Epoch 927/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.1550e-10 - accuracy: 1.0000 - val_loss: 0.1583 - val_accuracy: 0.9865\n",
            "Epoch 928/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.9824e-10 - accuracy: 1.0000 - val_loss: 0.1583 - val_accuracy: 0.9865\n",
            "Epoch 929/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.1305e-10 - accuracy: 1.0000 - val_loss: 0.1582 - val_accuracy: 0.9865\n",
            "Epoch 930/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.1545e-10 - accuracy: 1.0000 - val_loss: 0.1585 - val_accuracy: 0.9865\n",
            "Epoch 931/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.9845e-10 - accuracy: 1.0000 - val_loss: 0.1581 - val_accuracy: 0.9872\n",
            "Epoch 932/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.9421e-10 - accuracy: 1.0000 - val_loss: 0.1584 - val_accuracy: 0.9865\n",
            "Epoch 933/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.0772e-10 - accuracy: 1.0000 - val_loss: 0.1583 - val_accuracy: 0.9865\n",
            "Epoch 934/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.0767e-10 - accuracy: 1.0000 - val_loss: 0.1589 - val_accuracy: 0.9865\n",
            "Epoch 935/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.0800e-10 - accuracy: 1.0000 - val_loss: 0.1585 - val_accuracy: 0.9865\n",
            "Epoch 936/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.0562e-10 - accuracy: 1.0000 - val_loss: 0.1585 - val_accuracy: 0.9865\n",
            "Epoch 937/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.0073e-10 - accuracy: 1.0000 - val_loss: 0.1586 - val_accuracy: 0.9865\n",
            "Epoch 938/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.0085e-10 - accuracy: 1.0000 - val_loss: 0.1583 - val_accuracy: 0.9865\n",
            "Epoch 939/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.1209e-10 - accuracy: 1.0000 - val_loss: 0.1585 - val_accuracy: 0.9865\n",
            "Epoch 940/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.9248e-10 - accuracy: 1.0000 - val_loss: 0.1589 - val_accuracy: 0.9865\n",
            "Epoch 941/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.9218e-10 - accuracy: 1.0000 - val_loss: 0.1593 - val_accuracy: 0.9865\n",
            "Epoch 942/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.0227e-10 - accuracy: 1.0000 - val_loss: 0.1588 - val_accuracy: 0.9865\n",
            "Epoch 943/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.9680e-10 - accuracy: 1.0000 - val_loss: 0.1589 - val_accuracy: 0.9865\n",
            "Epoch 944/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.9342e-10 - accuracy: 1.0000 - val_loss: 0.1588 - val_accuracy: 0.9865\n",
            "Epoch 945/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.9914e-10 - accuracy: 1.0000 - val_loss: 0.1587 - val_accuracy: 0.9865\n",
            "Epoch 946/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.9295e-10 - accuracy: 1.0000 - val_loss: 0.1589 - val_accuracy: 0.9865\n",
            "Epoch 947/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.0403e-10 - accuracy: 1.0000 - val_loss: 0.1585 - val_accuracy: 0.9865\n",
            "Epoch 948/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.8288e-10 - accuracy: 1.0000 - val_loss: 0.1588 - val_accuracy: 0.9865\n",
            "Epoch 949/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.9559e-10 - accuracy: 1.0000 - val_loss: 0.1592 - val_accuracy: 0.9865\n",
            "Epoch 950/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.9304e-10 - accuracy: 1.0000 - val_loss: 0.1589 - val_accuracy: 0.9865\n",
            "Epoch 951/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.8850e-10 - accuracy: 1.0000 - val_loss: 0.1586 - val_accuracy: 0.9865\n",
            "Epoch 952/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.9599e-10 - accuracy: 1.0000 - val_loss: 0.1589 - val_accuracy: 0.9865\n",
            "Epoch 953/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.6442e-10 - accuracy: 1.0000 - val_loss: 0.1587 - val_accuracy: 0.9865\n",
            "Epoch 954/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.8141e-10 - accuracy: 1.0000 - val_loss: 0.1587 - val_accuracy: 0.9865\n",
            "Epoch 955/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.7597e-10 - accuracy: 1.0000 - val_loss: 0.1594 - val_accuracy: 0.9865\n",
            "Epoch 956/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.8270e-10 - accuracy: 1.0000 - val_loss: 0.1588 - val_accuracy: 0.9865\n",
            "Epoch 957/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.7830e-10 - accuracy: 1.0000 - val_loss: 0.1591 - val_accuracy: 0.9865\n",
            "Epoch 958/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.8461e-10 - accuracy: 1.0000 - val_loss: 0.1588 - val_accuracy: 0.9865\n",
            "Epoch 959/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.7376e-10 - accuracy: 1.0000 - val_loss: 0.1595 - val_accuracy: 0.9865\n",
            "Epoch 960/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.8295e-10 - accuracy: 1.0000 - val_loss: 0.1589 - val_accuracy: 0.9865\n",
            "Epoch 961/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.8094e-10 - accuracy: 1.0000 - val_loss: 0.1591 - val_accuracy: 0.9865\n",
            "Epoch 962/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.7998e-10 - accuracy: 1.0000 - val_loss: 0.1589 - val_accuracy: 0.9865\n",
            "Epoch 963/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.5308e-10 - accuracy: 1.0000 - val_loss: 0.1589 - val_accuracy: 0.9865\n",
            "Epoch 964/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.7526e-10 - accuracy: 1.0000 - val_loss: 0.1592 - val_accuracy: 0.9865\n",
            "Epoch 965/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.7350e-10 - accuracy: 1.0000 - val_loss: 0.1589 - val_accuracy: 0.9865\n",
            "Epoch 966/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.7260e-10 - accuracy: 1.0000 - val_loss: 0.1591 - val_accuracy: 0.9865\n",
            "Epoch 967/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.7260e-10 - accuracy: 1.0000 - val_loss: 0.1591 - val_accuracy: 0.9865\n",
            "Epoch 968/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.7169e-10 - accuracy: 1.0000 - val_loss: 0.1594 - val_accuracy: 0.9865\n",
            "Epoch 969/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.7757e-10 - accuracy: 1.0000 - val_loss: 0.1591 - val_accuracy: 0.9865\n",
            "Epoch 970/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.7049e-10 - accuracy: 1.0000 - val_loss: 0.1595 - val_accuracy: 0.9865\n",
            "Epoch 971/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.6807e-10 - accuracy: 1.0000 - val_loss: 0.1594 - val_accuracy: 0.9865\n",
            "Epoch 972/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.6898e-10 - accuracy: 1.0000 - val_loss: 0.1591 - val_accuracy: 0.9865\n",
            "Epoch 973/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.8174e-10 - accuracy: 1.0000 - val_loss: 0.1594 - val_accuracy: 0.9865\n",
            "Epoch 974/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.6915e-10 - accuracy: 1.0000 - val_loss: 0.1590 - val_accuracy: 0.9865\n",
            "Epoch 975/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.6965e-10 - accuracy: 1.0000 - val_loss: 0.1593 - val_accuracy: 0.9865\n",
            "Epoch 976/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.7049e-10 - accuracy: 1.0000 - val_loss: 0.1590 - val_accuracy: 0.9865\n",
            "Epoch 977/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.6250e-10 - accuracy: 1.0000 - val_loss: 0.1590 - val_accuracy: 0.9865\n",
            "Epoch 978/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.6666e-10 - accuracy: 1.0000 - val_loss: 0.1595 - val_accuracy: 0.9865\n",
            "Epoch 979/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.7007e-10 - accuracy: 1.0000 - val_loss: 0.1588 - val_accuracy: 0.9865\n",
            "Epoch 980/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.6272e-10 - accuracy: 1.0000 - val_loss: 0.1593 - val_accuracy: 0.9865\n",
            "Epoch 981/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.6684e-10 - accuracy: 1.0000 - val_loss: 0.1590 - val_accuracy: 0.9865\n",
            "Epoch 982/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.4902e-10 - accuracy: 1.0000 - val_loss: 0.1591 - val_accuracy: 0.9865\n",
            "Epoch 983/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.4359e-10 - accuracy: 1.0000 - val_loss: 0.1594 - val_accuracy: 0.9865\n",
            "Epoch 984/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.6418e-10 - accuracy: 1.0000 - val_loss: 0.1592 - val_accuracy: 0.9865\n",
            "Epoch 985/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.5862e-10 - accuracy: 1.0000 - val_loss: 0.1595 - val_accuracy: 0.9865\n",
            "Epoch 986/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.4997e-10 - accuracy: 1.0000 - val_loss: 0.1593 - val_accuracy: 0.9865\n",
            "Epoch 987/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.5927e-10 - accuracy: 1.0000 - val_loss: 0.1595 - val_accuracy: 0.9865\n",
            "Epoch 988/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.6208e-10 - accuracy: 1.0000 - val_loss: 0.1589 - val_accuracy: 0.9865\n",
            "Epoch 989/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.5308e-10 - accuracy: 1.0000 - val_loss: 0.1595 - val_accuracy: 0.9865\n",
            "Epoch 990/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.5393e-10 - accuracy: 1.0000 - val_loss: 0.1595 - val_accuracy: 0.9865\n",
            "Epoch 991/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.5027e-10 - accuracy: 1.0000 - val_loss: 0.1592 - val_accuracy: 0.9865\n",
            "Epoch 992/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.5567e-10 - accuracy: 1.0000 - val_loss: 0.1595 - val_accuracy: 0.9865\n",
            "Epoch 993/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.5892e-10 - accuracy: 1.0000 - val_loss: 0.1594 - val_accuracy: 0.9865\n",
            "Epoch 994/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.5344e-10 - accuracy: 1.0000 - val_loss: 0.1592 - val_accuracy: 0.9865\n",
            "Epoch 995/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.5029e-10 - accuracy: 1.0000 - val_loss: 0.1591 - val_accuracy: 0.9865\n",
            "Epoch 996/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.5868e-10 - accuracy: 1.0000 - val_loss: 0.1593 - val_accuracy: 0.9865\n",
            "Epoch 997/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.5787e-10 - accuracy: 1.0000 - val_loss: 0.1596 - val_accuracy: 0.9865\n",
            "Epoch 998/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.5188e-10 - accuracy: 1.0000 - val_loss: 0.1596 - val_accuracy: 0.9865\n",
            "Epoch 999/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.5394e-10 - accuracy: 1.0000 - val_loss: 0.1595 - val_accuracy: 0.9865\n",
            "Epoch 1000/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.4832e-10 - accuracy: 1.0000 - val_loss: 0.1592 - val_accuracy: 0.9865\n",
            "Epoch 1001/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.3205e-10 - accuracy: 1.0000 - val_loss: 0.1592 - val_accuracy: 0.9865\n",
            "Epoch 1002/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.4094e-10 - accuracy: 1.0000 - val_loss: 0.1592 - val_accuracy: 0.9865\n",
            "Epoch 1003/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.4011e-10 - accuracy: 1.0000 - val_loss: 0.1597 - val_accuracy: 0.9865\n",
            "Epoch 1004/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.4033e-10 - accuracy: 1.0000 - val_loss: 0.1595 - val_accuracy: 0.9865\n",
            "Epoch 1005/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.5030e-10 - accuracy: 1.0000 - val_loss: 0.1596 - val_accuracy: 0.9865\n",
            "Epoch 1006/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.4988e-10 - accuracy: 1.0000 - val_loss: 0.1597 - val_accuracy: 0.9865\n",
            "Epoch 1007/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.3982e-10 - accuracy: 1.0000 - val_loss: 0.1597 - val_accuracy: 0.9865\n",
            "Epoch 1008/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.4813e-10 - accuracy: 1.0000 - val_loss: 0.1597 - val_accuracy: 0.9865\n",
            "Epoch 1009/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.5770e-10 - accuracy: 1.0000 - val_loss: 0.1594 - val_accuracy: 0.9865\n",
            "Epoch 1010/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.3047e-10 - accuracy: 1.0000 - val_loss: 0.1599 - val_accuracy: 0.9865\n",
            "Epoch 1011/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.3412e-10 - accuracy: 1.0000 - val_loss: 0.1600 - val_accuracy: 0.9865\n",
            "Epoch 1012/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.4883e-10 - accuracy: 1.0000 - val_loss: 0.1598 - val_accuracy: 0.9865\n",
            "Epoch 1013/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.2331e-10 - accuracy: 1.0000 - val_loss: 0.1601 - val_accuracy: 0.9865\n",
            "Epoch 1014/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.3255e-10 - accuracy: 1.0000 - val_loss: 0.1597 - val_accuracy: 0.9865\n",
            "Epoch 1015/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.4442e-10 - accuracy: 1.0000 - val_loss: 0.1600 - val_accuracy: 0.9865\n",
            "Epoch 1016/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.4231e-10 - accuracy: 1.0000 - val_loss: 0.1595 - val_accuracy: 0.9865\n",
            "Epoch 1017/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.3451e-10 - accuracy: 1.0000 - val_loss: 0.1602 - val_accuracy: 0.9865\n",
            "Epoch 1018/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.3898e-10 - accuracy: 1.0000 - val_loss: 0.1598 - val_accuracy: 0.9865\n",
            "Epoch 1019/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.4760e-10 - accuracy: 1.0000 - val_loss: 0.1597 - val_accuracy: 0.9865\n",
            "Epoch 1020/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.3773e-10 - accuracy: 1.0000 - val_loss: 0.1598 - val_accuracy: 0.9865\n",
            "Epoch 1021/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.4733e-10 - accuracy: 1.0000 - val_loss: 0.1597 - val_accuracy: 0.9865\n",
            "Epoch 1022/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.4294e-10 - accuracy: 1.0000 - val_loss: 0.1601 - val_accuracy: 0.9865\n",
            "Epoch 1023/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.2931e-10 - accuracy: 1.0000 - val_loss: 0.1603 - val_accuracy: 0.9865\n",
            "Epoch 1024/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.4205e-10 - accuracy: 1.0000 - val_loss: 0.1595 - val_accuracy: 0.9865\n",
            "Epoch 1025/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.2944e-10 - accuracy: 1.0000 - val_loss: 0.1598 - val_accuracy: 0.9865\n",
            "Epoch 1026/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.2813e-10 - accuracy: 1.0000 - val_loss: 0.1600 - val_accuracy: 0.9865\n",
            "Epoch 1027/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.3221e-10 - accuracy: 1.0000 - val_loss: 0.1598 - val_accuracy: 0.9865\n",
            "Epoch 1028/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.2786e-10 - accuracy: 1.0000 - val_loss: 0.1599 - val_accuracy: 0.9865\n",
            "Epoch 1029/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.2307e-10 - accuracy: 1.0000 - val_loss: 0.1604 - val_accuracy: 0.9865\n",
            "Epoch 1030/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.2036e-10 - accuracy: 1.0000 - val_loss: 0.1600 - val_accuracy: 0.9865\n",
            "Epoch 1031/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.3417e-10 - accuracy: 1.0000 - val_loss: 0.1601 - val_accuracy: 0.9865\n",
            "Epoch 1032/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.1802e-10 - accuracy: 1.0000 - val_loss: 0.1603 - val_accuracy: 0.9865\n",
            "Epoch 1033/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.2727e-10 - accuracy: 1.0000 - val_loss: 0.1601 - val_accuracy: 0.9865\n",
            "Epoch 1034/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.3514e-10 - accuracy: 1.0000 - val_loss: 0.1601 - val_accuracy: 0.9865\n",
            "Epoch 1035/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.2867e-10 - accuracy: 1.0000 - val_loss: 0.1601 - val_accuracy: 0.9865\n",
            "Epoch 1036/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.2904e-10 - accuracy: 1.0000 - val_loss: 0.1603 - val_accuracy: 0.9865\n",
            "Epoch 1037/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.2118e-10 - accuracy: 1.0000 - val_loss: 0.1602 - val_accuracy: 0.9865\n",
            "Epoch 1038/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.1840e-10 - accuracy: 1.0000 - val_loss: 0.1601 - val_accuracy: 0.9865\n",
            "Epoch 1039/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.2547e-10 - accuracy: 1.0000 - val_loss: 0.1601 - val_accuracy: 0.9865\n",
            "Epoch 1040/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.1743e-10 - accuracy: 1.0000 - val_loss: 0.1604 - val_accuracy: 0.9865\n",
            "Epoch 1041/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.2672e-10 - accuracy: 1.0000 - val_loss: 0.1601 - val_accuracy: 0.9865\n",
            "Epoch 1042/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.2718e-10 - accuracy: 1.0000 - val_loss: 0.1604 - val_accuracy: 0.9865\n",
            "Epoch 1043/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.2490e-10 - accuracy: 1.0000 - val_loss: 0.1605 - val_accuracy: 0.9865\n",
            "Epoch 1044/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.2172e-10 - accuracy: 1.0000 - val_loss: 0.1607 - val_accuracy: 0.9865\n",
            "Epoch 1045/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.2341e-10 - accuracy: 1.0000 - val_loss: 0.1602 - val_accuracy: 0.9865\n",
            "Epoch 1046/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.0929e-10 - accuracy: 1.0000 - val_loss: 0.1603 - val_accuracy: 0.9865\n",
            "Epoch 1047/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.2185e-10 - accuracy: 1.0000 - val_loss: 0.1601 - val_accuracy: 0.9865\n",
            "Epoch 1048/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.1668e-10 - accuracy: 1.0000 - val_loss: 0.1602 - val_accuracy: 0.9865\n",
            "Epoch 1049/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.2069e-10 - accuracy: 1.0000 - val_loss: 0.1605 - val_accuracy: 0.9865\n",
            "Epoch 1050/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.2297e-10 - accuracy: 1.0000 - val_loss: 0.1603 - val_accuracy: 0.9865\n",
            "Epoch 1051/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.2444e-10 - accuracy: 1.0000 - val_loss: 0.1605 - val_accuracy: 0.9865\n",
            "Epoch 1052/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.2108e-10 - accuracy: 1.0000 - val_loss: 0.1604 - val_accuracy: 0.9865\n",
            "Epoch 1053/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.1717e-10 - accuracy: 1.0000 - val_loss: 0.1606 - val_accuracy: 0.9865\n",
            "Epoch 1054/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.2058e-10 - accuracy: 1.0000 - val_loss: 0.1601 - val_accuracy: 0.9865\n",
            "Epoch 1055/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.1994e-10 - accuracy: 1.0000 - val_loss: 0.1607 - val_accuracy: 0.9865\n",
            "Epoch 1056/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.2419e-10 - accuracy: 1.0000 - val_loss: 0.1607 - val_accuracy: 0.9865\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utNb0bH3W8ME",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "2b2d938c-78bc-4ae7-fec2-c3eba293bb32"
      },
      "source": [
        "prediction = model.predict(XVALID)\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "accuracy = accuracy_score(YVALID, prediction.round())\n",
        "precision = precision_score(YVALID, prediction.round())\n",
        "recall = recall_score(YVALID, prediction.round())\n",
        "f1score = f1_score(YVALID, prediction.round())\n",
        "print(\"Multilayer NN Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
        "print(\"Precision: %.2f%%\" % (precision * 100.0))\n",
        "print(\"Recall: %.2f%%\" % (recall * 100.0))\n",
        "print(\"F1-score: %.2f\" % (f1score*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Multilayer NN Accuracy: 98.65%\n",
            "Precision: 96.63%\n",
            "Recall: 94.37%\n",
            "F1-score: 95.49\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZvHvNcbLLH7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "079a5475-e782-466d-c496-9d749905acc3"
      },
      "source": [
        "prediction=model.predict(XTRAIN)\n",
        "accuracy = accuracy_score(YTRAIN, prediction.round())\n",
        "precision = precision_score(YTRAIN, prediction.round())\n",
        "recall = recall_score(YTRAIN, prediction.round())\n",
        "f1score = f1_score(YTRAIN, prediction.round())\n",
        "print(\"Training Set Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
        "print(\"Precision: %.2f%%\" % (precision * 100.0))\n",
        "print(\"Recall: %.2f%%\" % (recall * 100.0))\n",
        "print(\"F1-score: %.2f\" % (f1score))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Set Accuracy: 100.00%\n",
            "Precision: 100.00%\n",
            "Recall: 100.00%\n",
            "F1-score: 1.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zD9g7R4G0vaO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 624
        },
        "outputId": "6b230e60-95f0-49f2-9772-bf7cd7f83ba4"
      },
      "source": [
        "print(history.params)\n",
        "plt.plot(history.history['loss']) \n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['Training data', 'Validation data'], loc='lower right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'verbose': 1, 'epochs': 1056, 'steps': 103}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bL95JMcy6FvM",
        "colab_type": "text"
      },
      "source": [
        "#A neural Network with 5 layers but with low number of neurons:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rJi8KTO6E3Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "549a3814-646c-4382-fbb1-3cc85170e0fe"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(32, input_dim = len(XTRAIN[0, :]), activation='sigmoid'))\n",
        "model.add(Dense(16, activation='relu'))\n",
        "model.add(Dense(8, activation='sigmoid'))\n",
        "model.add(Dense(4, activation='sigmoid'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer = 'rmsprop', metrics=['accuracy'])\n",
        "history = model.fit(XTRAIN, YTRAIN, validation_data=(XVALID, YVALID),epochs = 1056, verbose = 1 )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1056\n",
            "103/103 [==============================] - 0s 3ms/step - loss: 0.6465 - accuracy: 0.6471 - val_loss: 0.5480 - val_accuracy: 0.8371\n",
            "Epoch 2/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.5042 - accuracy: 0.8397 - val_loss: 0.4756 - val_accuracy: 0.8371\n",
            "Epoch 3/1056\n",
            "103/103 [==============================] - 0s 4ms/step - loss: 0.4547 - accuracy: 0.8397 - val_loss: 0.4377 - val_accuracy: 0.8371\n",
            "Epoch 4/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.4196 - accuracy: 0.8397 - val_loss: 0.3956 - val_accuracy: 0.8371\n",
            "Epoch 5/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.3769 - accuracy: 0.8397 - val_loss: 0.3450 - val_accuracy: 0.8371\n",
            "Epoch 6/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.3318 - accuracy: 0.8397 - val_loss: 0.2989 - val_accuracy: 0.8371\n",
            "Epoch 7/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.2913 - accuracy: 0.8397 - val_loss: 0.2563 - val_accuracy: 0.8371\n",
            "Epoch 8/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.2538 - accuracy: 0.8619 - val_loss: 0.2196 - val_accuracy: 0.8969\n",
            "Epoch 9/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.2221 - accuracy: 0.9186 - val_loss: 0.1898 - val_accuracy: 0.9417\n",
            "Epoch 10/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.1963 - accuracy: 0.9375 - val_loss: 0.1700 - val_accuracy: 0.9573\n",
            "Epoch 11/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.1786 - accuracy: 0.9467 - val_loss: 0.1541 - val_accuracy: 0.9595\n",
            "Epoch 12/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.1641 - accuracy: 0.9485 - val_loss: 0.1458 - val_accuracy: 0.9609\n",
            "Epoch 13/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.1549 - accuracy: 0.9485 - val_loss: 0.1339 - val_accuracy: 0.9623\n",
            "Epoch 14/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.1476 - accuracy: 0.9482 - val_loss: 0.1263 - val_accuracy: 0.9609\n",
            "Epoch 15/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.1404 - accuracy: 0.9494 - val_loss: 0.1198 - val_accuracy: 0.9616\n",
            "Epoch 16/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.1367 - accuracy: 0.9497 - val_loss: 0.1177 - val_accuracy: 0.9630\n",
            "Epoch 17/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.1331 - accuracy: 0.9506 - val_loss: 0.1152 - val_accuracy: 0.9609\n",
            "Epoch 18/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.1310 - accuracy: 0.9476 - val_loss: 0.1099 - val_accuracy: 0.9623\n",
            "Epoch 19/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.1284 - accuracy: 0.9479 - val_loss: 0.1130 - val_accuracy: 0.9630\n",
            "Epoch 20/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.1255 - accuracy: 0.9512 - val_loss: 0.1046 - val_accuracy: 0.9616\n",
            "Epoch 21/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.1245 - accuracy: 0.9503 - val_loss: 0.1027 - val_accuracy: 0.9623\n",
            "Epoch 22/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.1214 - accuracy: 0.9491 - val_loss: 0.1004 - val_accuracy: 0.9616\n",
            "Epoch 23/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.1208 - accuracy: 0.9515 - val_loss: 0.0995 - val_accuracy: 0.9673\n",
            "Epoch 24/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.1203 - accuracy: 0.9485 - val_loss: 0.0992 - val_accuracy: 0.9616\n",
            "Epoch 25/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.1186 - accuracy: 0.9491 - val_loss: 0.1051 - val_accuracy: 0.9538\n",
            "Epoch 26/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.1169 - accuracy: 0.9515 - val_loss: 0.0988 - val_accuracy: 0.9609\n",
            "Epoch 27/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.1159 - accuracy: 0.9525 - val_loss: 0.0909 - val_accuracy: 0.9644\n",
            "Epoch 28/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.1143 - accuracy: 0.9525 - val_loss: 0.1025 - val_accuracy: 0.9637\n",
            "Epoch 29/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.1138 - accuracy: 0.9561 - val_loss: 0.0893 - val_accuracy: 0.9701\n",
            "Epoch 30/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.1132 - accuracy: 0.9488 - val_loss: 0.0924 - val_accuracy: 0.9680\n",
            "Epoch 31/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.1127 - accuracy: 0.9515 - val_loss: 0.0973 - val_accuracy: 0.9609\n",
            "Epoch 32/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.1116 - accuracy: 0.9540 - val_loss: 0.0862 - val_accuracy: 0.9659\n",
            "Epoch 33/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.1081 - accuracy: 0.9543 - val_loss: 0.0928 - val_accuracy: 0.9609\n",
            "Epoch 34/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.1090 - accuracy: 0.9552 - val_loss: 0.0861 - val_accuracy: 0.9644\n",
            "Epoch 35/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.1078 - accuracy: 0.9549 - val_loss: 0.0803 - val_accuracy: 0.9723\n",
            "Epoch 36/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.1076 - accuracy: 0.9540 - val_loss: 0.0818 - val_accuracy: 0.9716\n",
            "Epoch 37/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.1064 - accuracy: 0.9567 - val_loss: 0.0821 - val_accuracy: 0.9680\n",
            "Epoch 38/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.1070 - accuracy: 0.9558 - val_loss: 0.0788 - val_accuracy: 0.9708\n",
            "Epoch 39/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.1035 - accuracy: 0.9558 - val_loss: 0.0794 - val_accuracy: 0.9708\n",
            "Epoch 40/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.1030 - accuracy: 0.9561 - val_loss: 0.0754 - val_accuracy: 0.9694\n",
            "Epoch 41/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.1021 - accuracy: 0.9561 - val_loss: 0.0813 - val_accuracy: 0.9623\n",
            "Epoch 42/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.1018 - accuracy: 0.9570 - val_loss: 0.0741 - val_accuracy: 0.9730\n",
            "Epoch 43/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.1017 - accuracy: 0.9598 - val_loss: 0.0745 - val_accuracy: 0.9701\n",
            "Epoch 44/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0990 - accuracy: 0.9604 - val_loss: 0.0779 - val_accuracy: 0.9644\n",
            "Epoch 45/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0988 - accuracy: 0.9589 - val_loss: 0.0730 - val_accuracy: 0.9708\n",
            "Epoch 46/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0968 - accuracy: 0.9619 - val_loss: 0.0733 - val_accuracy: 0.9701\n",
            "Epoch 47/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0966 - accuracy: 0.9592 - val_loss: 0.0734 - val_accuracy: 0.9673\n",
            "Epoch 48/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0967 - accuracy: 0.9585 - val_loss: 0.0720 - val_accuracy: 0.9744\n",
            "Epoch 49/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0955 - accuracy: 0.9619 - val_loss: 0.0707 - val_accuracy: 0.9751\n",
            "Epoch 50/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0937 - accuracy: 0.9619 - val_loss: 0.0704 - val_accuracy: 0.9765\n",
            "Epoch 51/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0913 - accuracy: 0.9643 - val_loss: 0.0708 - val_accuracy: 0.9659\n",
            "Epoch 52/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0924 - accuracy: 0.9601 - val_loss: 0.0707 - val_accuracy: 0.9680\n",
            "Epoch 53/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0912 - accuracy: 0.9616 - val_loss: 0.0721 - val_accuracy: 0.9673\n",
            "Epoch 54/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0906 - accuracy: 0.9616 - val_loss: 0.0709 - val_accuracy: 0.9751\n",
            "Epoch 55/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0901 - accuracy: 0.9628 - val_loss: 0.0663 - val_accuracy: 0.9751\n",
            "Epoch 56/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0884 - accuracy: 0.9637 - val_loss: 0.0635 - val_accuracy: 0.9694\n",
            "Epoch 57/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0879 - accuracy: 0.9649 - val_loss: 0.0640 - val_accuracy: 0.9744\n",
            "Epoch 58/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0859 - accuracy: 0.9646 - val_loss: 0.0634 - val_accuracy: 0.9751\n",
            "Epoch 59/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0851 - accuracy: 0.9643 - val_loss: 0.0588 - val_accuracy: 0.9787\n",
            "Epoch 60/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0853 - accuracy: 0.9646 - val_loss: 0.0595 - val_accuracy: 0.9765\n",
            "Epoch 61/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0845 - accuracy: 0.9674 - val_loss: 0.0596 - val_accuracy: 0.9772\n",
            "Epoch 62/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0840 - accuracy: 0.9683 - val_loss: 0.0579 - val_accuracy: 0.9787\n",
            "Epoch 63/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0802 - accuracy: 0.9698 - val_loss: 0.0590 - val_accuracy: 0.9744\n",
            "Epoch 64/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0822 - accuracy: 0.9665 - val_loss: 0.0578 - val_accuracy: 0.9808\n",
            "Epoch 65/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0796 - accuracy: 0.9677 - val_loss: 0.0593 - val_accuracy: 0.9744\n",
            "Epoch 66/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0804 - accuracy: 0.9689 - val_loss: 0.0594 - val_accuracy: 0.9730\n",
            "Epoch 67/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0797 - accuracy: 0.9710 - val_loss: 0.0593 - val_accuracy: 0.9716\n",
            "Epoch 68/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0777 - accuracy: 0.9695 - val_loss: 0.0529 - val_accuracy: 0.9787\n",
            "Epoch 69/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0775 - accuracy: 0.9698 - val_loss: 0.0574 - val_accuracy: 0.9730\n",
            "Epoch 70/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0755 - accuracy: 0.9717 - val_loss: 0.0522 - val_accuracy: 0.9815\n",
            "Epoch 71/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0752 - accuracy: 0.9701 - val_loss: 0.0514 - val_accuracy: 0.9808\n",
            "Epoch 72/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0747 - accuracy: 0.9729 - val_loss: 0.0538 - val_accuracy: 0.9808\n",
            "Epoch 73/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0734 - accuracy: 0.9707 - val_loss: 0.0522 - val_accuracy: 0.9780\n",
            "Epoch 74/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0728 - accuracy: 0.9717 - val_loss: 0.0500 - val_accuracy: 0.9808\n",
            "Epoch 75/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0727 - accuracy: 0.9726 - val_loss: 0.0501 - val_accuracy: 0.9787\n",
            "Epoch 76/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0721 - accuracy: 0.9729 - val_loss: 0.0476 - val_accuracy: 0.9815\n",
            "Epoch 77/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0699 - accuracy: 0.9738 - val_loss: 0.0498 - val_accuracy: 0.9808\n",
            "Epoch 78/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0684 - accuracy: 0.9735 - val_loss: 0.0510 - val_accuracy: 0.9815\n",
            "Epoch 79/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0678 - accuracy: 0.9723 - val_loss: 0.0467 - val_accuracy: 0.9815\n",
            "Epoch 80/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0667 - accuracy: 0.9744 - val_loss: 0.0518 - val_accuracy: 0.9815\n",
            "Epoch 81/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0669 - accuracy: 0.9741 - val_loss: 0.0464 - val_accuracy: 0.9808\n",
            "Epoch 82/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0638 - accuracy: 0.9762 - val_loss: 0.0459 - val_accuracy: 0.9836\n",
            "Epoch 83/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0656 - accuracy: 0.9759 - val_loss: 0.0460 - val_accuracy: 0.9808\n",
            "Epoch 84/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0620 - accuracy: 0.9759 - val_loss: 0.0482 - val_accuracy: 0.9794\n",
            "Epoch 85/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0609 - accuracy: 0.9750 - val_loss: 0.0444 - val_accuracy: 0.9801\n",
            "Epoch 86/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0604 - accuracy: 0.9774 - val_loss: 0.0479 - val_accuracy: 0.9815\n",
            "Epoch 87/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0598 - accuracy: 0.9778 - val_loss: 0.0455 - val_accuracy: 0.9815\n",
            "Epoch 88/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0591 - accuracy: 0.9765 - val_loss: 0.0431 - val_accuracy: 0.9829\n",
            "Epoch 89/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0584 - accuracy: 0.9784 - val_loss: 0.0425 - val_accuracy: 0.9822\n",
            "Epoch 90/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0559 - accuracy: 0.9796 - val_loss: 0.0429 - val_accuracy: 0.9829\n",
            "Epoch 91/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0563 - accuracy: 0.9802 - val_loss: 0.0437 - val_accuracy: 0.9815\n",
            "Epoch 92/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0532 - accuracy: 0.9793 - val_loss: 0.0407 - val_accuracy: 0.9822\n",
            "Epoch 93/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0531 - accuracy: 0.9799 - val_loss: 0.0434 - val_accuracy: 0.9808\n",
            "Epoch 94/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0525 - accuracy: 0.9808 - val_loss: 0.0546 - val_accuracy: 0.9765\n",
            "Epoch 95/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0524 - accuracy: 0.9811 - val_loss: 0.0426 - val_accuracy: 0.9822\n",
            "Epoch 96/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0507 - accuracy: 0.9811 - val_loss: 0.0439 - val_accuracy: 0.9808\n",
            "Epoch 97/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0508 - accuracy: 0.9811 - val_loss: 0.0429 - val_accuracy: 0.9808\n",
            "Epoch 98/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0504 - accuracy: 0.9817 - val_loss: 0.0421 - val_accuracy: 0.9822\n",
            "Epoch 99/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0508 - accuracy: 0.9811 - val_loss: 0.0398 - val_accuracy: 0.9801\n",
            "Epoch 100/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0490 - accuracy: 0.9842 - val_loss: 0.0505 - val_accuracy: 0.9794\n",
            "Epoch 101/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0489 - accuracy: 0.9820 - val_loss: 0.0466 - val_accuracy: 0.9808\n",
            "Epoch 102/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0481 - accuracy: 0.9835 - val_loss: 0.0370 - val_accuracy: 0.9851\n",
            "Epoch 103/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0478 - accuracy: 0.9838 - val_loss: 0.0424 - val_accuracy: 0.9815\n",
            "Epoch 104/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0460 - accuracy: 0.9820 - val_loss: 0.0370 - val_accuracy: 0.9858\n",
            "Epoch 105/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0456 - accuracy: 0.9838 - val_loss: 0.0398 - val_accuracy: 0.9836\n",
            "Epoch 106/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0450 - accuracy: 0.9842 - val_loss: 0.0427 - val_accuracy: 0.9808\n",
            "Epoch 107/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0448 - accuracy: 0.9851 - val_loss: 0.0396 - val_accuracy: 0.9829\n",
            "Epoch 108/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0442 - accuracy: 0.9838 - val_loss: 0.0412 - val_accuracy: 0.9822\n",
            "Epoch 109/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0426 - accuracy: 0.9838 - val_loss: 0.0380 - val_accuracy: 0.9858\n",
            "Epoch 110/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0427 - accuracy: 0.9829 - val_loss: 0.0414 - val_accuracy: 0.9844\n",
            "Epoch 111/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0411 - accuracy: 0.9857 - val_loss: 0.0393 - val_accuracy: 0.9822\n",
            "Epoch 112/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0411 - accuracy: 0.9860 - val_loss: 0.0353 - val_accuracy: 0.9851\n",
            "Epoch 113/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0393 - accuracy: 0.9893 - val_loss: 0.0368 - val_accuracy: 0.9851\n",
            "Epoch 114/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0387 - accuracy: 0.9869 - val_loss: 0.0395 - val_accuracy: 0.9844\n",
            "Epoch 115/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0377 - accuracy: 0.9875 - val_loss: 0.0344 - val_accuracy: 0.9865\n",
            "Epoch 116/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0371 - accuracy: 0.9884 - val_loss: 0.0359 - val_accuracy: 0.9844\n",
            "Epoch 117/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0376 - accuracy: 0.9878 - val_loss: 0.0414 - val_accuracy: 0.9836\n",
            "Epoch 118/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0368 - accuracy: 0.9878 - val_loss: 0.0339 - val_accuracy: 0.9844\n",
            "Epoch 119/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0358 - accuracy: 0.9878 - val_loss: 0.0377 - val_accuracy: 0.9851\n",
            "Epoch 120/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0355 - accuracy: 0.9878 - val_loss: 0.0334 - val_accuracy: 0.9879\n",
            "Epoch 121/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0346 - accuracy: 0.9884 - val_loss: 0.0357 - val_accuracy: 0.9851\n",
            "Epoch 122/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0349 - accuracy: 0.9869 - val_loss: 0.0337 - val_accuracy: 0.9879\n",
            "Epoch 123/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0336 - accuracy: 0.9902 - val_loss: 0.0343 - val_accuracy: 0.9872\n",
            "Epoch 124/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0326 - accuracy: 0.9893 - val_loss: 0.0345 - val_accuracy: 0.9865\n",
            "Epoch 125/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0326 - accuracy: 0.9893 - val_loss: 0.0338 - val_accuracy: 0.9872\n",
            "Epoch 126/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0322 - accuracy: 0.9884 - val_loss: 0.0330 - val_accuracy: 0.9879\n",
            "Epoch 127/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0324 - accuracy: 0.9887 - val_loss: 0.0363 - val_accuracy: 0.9858\n",
            "Epoch 128/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0299 - accuracy: 0.9902 - val_loss: 0.0351 - val_accuracy: 0.9858\n",
            "Epoch 129/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0295 - accuracy: 0.9890 - val_loss: 0.0339 - val_accuracy: 0.9865\n",
            "Epoch 130/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0304 - accuracy: 0.9899 - val_loss: 0.0409 - val_accuracy: 0.9836\n",
            "Epoch 131/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0304 - accuracy: 0.9872 - val_loss: 0.0333 - val_accuracy: 0.9879\n",
            "Epoch 132/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0278 - accuracy: 0.9896 - val_loss: 0.0344 - val_accuracy: 0.9872\n",
            "Epoch 133/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0283 - accuracy: 0.9909 - val_loss: 0.0342 - val_accuracy: 0.9879\n",
            "Epoch 134/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0274 - accuracy: 0.9909 - val_loss: 0.0393 - val_accuracy: 0.9829\n",
            "Epoch 135/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0273 - accuracy: 0.9899 - val_loss: 0.0353 - val_accuracy: 0.9872\n",
            "Epoch 136/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0282 - accuracy: 0.9875 - val_loss: 0.0349 - val_accuracy: 0.9872\n",
            "Epoch 137/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0258 - accuracy: 0.9924 - val_loss: 0.0407 - val_accuracy: 0.9829\n",
            "Epoch 138/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0268 - accuracy: 0.9915 - val_loss: 0.0348 - val_accuracy: 0.9900\n",
            "Epoch 139/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0269 - accuracy: 0.9906 - val_loss: 0.0330 - val_accuracy: 0.9879\n",
            "Epoch 140/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0248 - accuracy: 0.9918 - val_loss: 0.0318 - val_accuracy: 0.9872\n",
            "Epoch 141/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0242 - accuracy: 0.9927 - val_loss: 0.0335 - val_accuracy: 0.9886\n",
            "Epoch 142/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0247 - accuracy: 0.9918 - val_loss: 0.0342 - val_accuracy: 0.9872\n",
            "Epoch 143/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0245 - accuracy: 0.9906 - val_loss: 0.0394 - val_accuracy: 0.9858\n",
            "Epoch 144/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0222 - accuracy: 0.9921 - val_loss: 0.0443 - val_accuracy: 0.9844\n",
            "Epoch 145/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0238 - accuracy: 0.9930 - val_loss: 0.0408 - val_accuracy: 0.9865\n",
            "Epoch 146/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0210 - accuracy: 0.9948 - val_loss: 0.0374 - val_accuracy: 0.9858\n",
            "Epoch 147/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0214 - accuracy: 0.9930 - val_loss: 0.0364 - val_accuracy: 0.9858\n",
            "Epoch 148/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0229 - accuracy: 0.9927 - val_loss: 0.0349 - val_accuracy: 0.9858\n",
            "Epoch 149/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0209 - accuracy: 0.9927 - val_loss: 0.0420 - val_accuracy: 0.9858\n",
            "Epoch 150/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0219 - accuracy: 0.9921 - val_loss: 0.0359 - val_accuracy: 0.9858\n",
            "Epoch 151/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0221 - accuracy: 0.9921 - val_loss: 0.0358 - val_accuracy: 0.9865\n",
            "Epoch 152/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0212 - accuracy: 0.9927 - val_loss: 0.0428 - val_accuracy: 0.9844\n",
            "Epoch 153/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0211 - accuracy: 0.9930 - val_loss: 0.0334 - val_accuracy: 0.9872\n",
            "Epoch 154/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0204 - accuracy: 0.9924 - val_loss: 0.0424 - val_accuracy: 0.9836\n",
            "Epoch 155/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0215 - accuracy: 0.9933 - val_loss: 0.0301 - val_accuracy: 0.9908\n",
            "Epoch 156/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0198 - accuracy: 0.9927 - val_loss: 0.0331 - val_accuracy: 0.9886\n",
            "Epoch 157/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0188 - accuracy: 0.9927 - val_loss: 0.0300 - val_accuracy: 0.9915\n",
            "Epoch 158/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0193 - accuracy: 0.9945 - val_loss: 0.0329 - val_accuracy: 0.9893\n",
            "Epoch 159/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0189 - accuracy: 0.9930 - val_loss: 0.0352 - val_accuracy: 0.9879\n",
            "Epoch 160/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0190 - accuracy: 0.9933 - val_loss: 0.0375 - val_accuracy: 0.9865\n",
            "Epoch 161/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0182 - accuracy: 0.9942 - val_loss: 0.0343 - val_accuracy: 0.9879\n",
            "Epoch 162/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0172 - accuracy: 0.9930 - val_loss: 0.0416 - val_accuracy: 0.9858\n",
            "Epoch 163/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0179 - accuracy: 0.9936 - val_loss: 0.0472 - val_accuracy: 0.9829\n",
            "Epoch 164/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0178 - accuracy: 0.9936 - val_loss: 0.0338 - val_accuracy: 0.9900\n",
            "Epoch 165/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0173 - accuracy: 0.9939 - val_loss: 0.0487 - val_accuracy: 0.9836\n",
            "Epoch 166/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0149 - accuracy: 0.9957 - val_loss: 0.0309 - val_accuracy: 0.9922\n",
            "Epoch 167/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0176 - accuracy: 0.9936 - val_loss: 0.0352 - val_accuracy: 0.9886\n",
            "Epoch 168/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0159 - accuracy: 0.9948 - val_loss: 0.0365 - val_accuracy: 0.9872\n",
            "Epoch 169/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0155 - accuracy: 0.9957 - val_loss: 0.0318 - val_accuracy: 0.9900\n",
            "Epoch 170/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0158 - accuracy: 0.9951 - val_loss: 0.0314 - val_accuracy: 0.9915\n",
            "Epoch 171/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0146 - accuracy: 0.9951 - val_loss: 0.0316 - val_accuracy: 0.9922\n",
            "Epoch 172/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0139 - accuracy: 0.9963 - val_loss: 0.0344 - val_accuracy: 0.9893\n",
            "Epoch 173/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0147 - accuracy: 0.9957 - val_loss: 0.0335 - val_accuracy: 0.9900\n",
            "Epoch 174/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0143 - accuracy: 0.9954 - val_loss: 0.0324 - val_accuracy: 0.9915\n",
            "Epoch 175/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0141 - accuracy: 0.9957 - val_loss: 0.0390 - val_accuracy: 0.9865\n",
            "Epoch 176/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0144 - accuracy: 0.9948 - val_loss: 0.0329 - val_accuracy: 0.9908\n",
            "Epoch 177/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0122 - accuracy: 0.9973 - val_loss: 0.0354 - val_accuracy: 0.9879\n",
            "Epoch 178/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0138 - accuracy: 0.9966 - val_loss: 0.0369 - val_accuracy: 0.9865\n",
            "Epoch 179/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0122 - accuracy: 0.9966 - val_loss: 0.0347 - val_accuracy: 0.9893\n",
            "Epoch 180/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0134 - accuracy: 0.9954 - val_loss: 0.0346 - val_accuracy: 0.9893\n",
            "Epoch 181/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0120 - accuracy: 0.9960 - val_loss: 0.0378 - val_accuracy: 0.9886\n",
            "Epoch 182/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0130 - accuracy: 0.9963 - val_loss: 0.0312 - val_accuracy: 0.9908\n",
            "Epoch 183/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0126 - accuracy: 0.9951 - val_loss: 0.0392 - val_accuracy: 0.9872\n",
            "Epoch 184/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0123 - accuracy: 0.9948 - val_loss: 0.0330 - val_accuracy: 0.9900\n",
            "Epoch 185/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0127 - accuracy: 0.9963 - val_loss: 0.0355 - val_accuracy: 0.9908\n",
            "Epoch 186/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0120 - accuracy: 0.9966 - val_loss: 0.0563 - val_accuracy: 0.9829\n",
            "Epoch 187/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0115 - accuracy: 0.9966 - val_loss: 0.0356 - val_accuracy: 0.9879\n",
            "Epoch 188/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0102 - accuracy: 0.9982 - val_loss: 0.0376 - val_accuracy: 0.9886\n",
            "Epoch 189/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0108 - accuracy: 0.9963 - val_loss: 0.0368 - val_accuracy: 0.9886\n",
            "Epoch 190/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0113 - accuracy: 0.9973 - val_loss: 0.0341 - val_accuracy: 0.9893\n",
            "Epoch 191/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0107 - accuracy: 0.9970 - val_loss: 0.0408 - val_accuracy: 0.9872\n",
            "Epoch 192/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0124 - accuracy: 0.9963 - val_loss: 0.0345 - val_accuracy: 0.9908\n",
            "Epoch 193/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0097 - accuracy: 0.9973 - val_loss: 0.0428 - val_accuracy: 0.9865\n",
            "Epoch 194/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0102 - accuracy: 0.9960 - val_loss: 0.0374 - val_accuracy: 0.9886\n",
            "Epoch 195/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0105 - accuracy: 0.9973 - val_loss: 0.0389 - val_accuracy: 0.9886\n",
            "Epoch 196/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0108 - accuracy: 0.9966 - val_loss: 0.0372 - val_accuracy: 0.9879\n",
            "Epoch 197/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0095 - accuracy: 0.9966 - val_loss: 0.0451 - val_accuracy: 0.9844\n",
            "Epoch 198/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0096 - accuracy: 0.9973 - val_loss: 0.0348 - val_accuracy: 0.9893\n",
            "Epoch 199/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0095 - accuracy: 0.9979 - val_loss: 0.0412 - val_accuracy: 0.9886\n",
            "Epoch 200/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0102 - accuracy: 0.9973 - val_loss: 0.0349 - val_accuracy: 0.9886\n",
            "Epoch 201/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0097 - accuracy: 0.9979 - val_loss: 0.0433 - val_accuracy: 0.9879\n",
            "Epoch 202/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0090 - accuracy: 0.9973 - val_loss: 0.0405 - val_accuracy: 0.9893\n",
            "Epoch 203/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0090 - accuracy: 0.9976 - val_loss: 0.0375 - val_accuracy: 0.9893\n",
            "Epoch 204/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0096 - accuracy: 0.9976 - val_loss: 0.0347 - val_accuracy: 0.9908\n",
            "Epoch 205/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0100 - accuracy: 0.9970 - val_loss: 0.0368 - val_accuracy: 0.9893\n",
            "Epoch 206/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0082 - accuracy: 0.9982 - val_loss: 0.0376 - val_accuracy: 0.9900\n",
            "Epoch 207/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0078 - accuracy: 0.9985 - val_loss: 0.0346 - val_accuracy: 0.9908\n",
            "Epoch 208/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0080 - accuracy: 0.9976 - val_loss: 0.0427 - val_accuracy: 0.9879\n",
            "Epoch 209/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0084 - accuracy: 0.9976 - val_loss: 0.0395 - val_accuracy: 0.9893\n",
            "Epoch 210/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0077 - accuracy: 0.9988 - val_loss: 0.0545 - val_accuracy: 0.9829\n",
            "Epoch 211/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0073 - accuracy: 0.9979 - val_loss: 0.0409 - val_accuracy: 0.9879\n",
            "Epoch 212/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0077 - accuracy: 0.9982 - val_loss: 0.0458 - val_accuracy: 0.9879\n",
            "Epoch 213/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0088 - accuracy: 0.9979 - val_loss: 0.0338 - val_accuracy: 0.9915\n",
            "Epoch 214/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0077 - accuracy: 0.9979 - val_loss: 0.0364 - val_accuracy: 0.9915\n",
            "Epoch 215/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0073 - accuracy: 0.9985 - val_loss: 0.0366 - val_accuracy: 0.9908\n",
            "Epoch 216/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0077 - accuracy: 0.9985 - val_loss: 0.0386 - val_accuracy: 0.9879\n",
            "Epoch 217/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0082 - accuracy: 0.9985 - val_loss: 0.0415 - val_accuracy: 0.9879\n",
            "Epoch 218/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0069 - accuracy: 0.9982 - val_loss: 0.0440 - val_accuracy: 0.9893\n",
            "Epoch 219/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0074 - accuracy: 0.9979 - val_loss: 0.0490 - val_accuracy: 0.9858\n",
            "Epoch 220/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0070 - accuracy: 0.9985 - val_loss: 0.0405 - val_accuracy: 0.9893\n",
            "Epoch 221/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0065 - accuracy: 0.9994 - val_loss: 0.0424 - val_accuracy: 0.9879\n",
            "Epoch 222/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0075 - accuracy: 0.9982 - val_loss: 0.0500 - val_accuracy: 0.9872\n",
            "Epoch 223/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0068 - accuracy: 0.9982 - val_loss: 0.0365 - val_accuracy: 0.9908\n",
            "Epoch 224/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0068 - accuracy: 0.9985 - val_loss: 0.0409 - val_accuracy: 0.9908\n",
            "Epoch 225/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0064 - accuracy: 0.9982 - val_loss: 0.0522 - val_accuracy: 0.9865\n",
            "Epoch 226/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0066 - accuracy: 0.9988 - val_loss: 0.0377 - val_accuracy: 0.9915\n",
            "Epoch 227/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0050 - accuracy: 0.9994 - val_loss: 0.0383 - val_accuracy: 0.9893\n",
            "Epoch 228/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0069 - accuracy: 0.9988 - val_loss: 0.0379 - val_accuracy: 0.9915\n",
            "Epoch 229/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0062 - accuracy: 0.9988 - val_loss: 0.0458 - val_accuracy: 0.9893\n",
            "Epoch 230/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0059 - accuracy: 0.9988 - val_loss: 0.0421 - val_accuracy: 0.9893\n",
            "Epoch 231/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0066 - accuracy: 0.9985 - val_loss: 0.0370 - val_accuracy: 0.9900\n",
            "Epoch 232/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0059 - accuracy: 0.9982 - val_loss: 0.0438 - val_accuracy: 0.9893\n",
            "Epoch 233/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0062 - accuracy: 0.9991 - val_loss: 0.0389 - val_accuracy: 0.9900\n",
            "Epoch 234/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0073 - accuracy: 0.9985 - val_loss: 0.0527 - val_accuracy: 0.9879\n",
            "Epoch 235/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0064 - accuracy: 0.9985 - val_loss: 0.0441 - val_accuracy: 0.9872\n",
            "Epoch 236/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0056 - accuracy: 0.9988 - val_loss: 0.0422 - val_accuracy: 0.9893\n",
            "Epoch 237/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0065 - accuracy: 0.9985 - val_loss: 0.0445 - val_accuracy: 0.9893\n",
            "Epoch 238/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0051 - accuracy: 0.9994 - val_loss: 0.0393 - val_accuracy: 0.9893\n",
            "Epoch 239/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0063 - accuracy: 0.9982 - val_loss: 0.0568 - val_accuracy: 0.9893\n",
            "Epoch 240/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0050 - accuracy: 0.9988 - val_loss: 0.0529 - val_accuracy: 0.9893\n",
            "Epoch 241/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0059 - accuracy: 0.9991 - val_loss: 0.0401 - val_accuracy: 0.9886\n",
            "Epoch 242/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0056 - accuracy: 0.9988 - val_loss: 0.0453 - val_accuracy: 0.9900\n",
            "Epoch 243/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0058 - accuracy: 0.9991 - val_loss: 0.0461 - val_accuracy: 0.9893\n",
            "Epoch 244/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0053 - accuracy: 0.9991 - val_loss: 0.0403 - val_accuracy: 0.9908\n",
            "Epoch 245/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0055 - accuracy: 0.9991 - val_loss: 0.0551 - val_accuracy: 0.9865\n",
            "Epoch 246/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0055 - accuracy: 0.9988 - val_loss: 0.0430 - val_accuracy: 0.9900\n",
            "Epoch 247/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0058 - accuracy: 0.9988 - val_loss: 0.0434 - val_accuracy: 0.9893\n",
            "Epoch 248/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0051 - accuracy: 0.9991 - val_loss: 0.0540 - val_accuracy: 0.9886\n",
            "Epoch 249/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0049 - accuracy: 0.9988 - val_loss: 0.0447 - val_accuracy: 0.9879\n",
            "Epoch 250/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0047 - accuracy: 0.9994 - val_loss: 0.0496 - val_accuracy: 0.9879\n",
            "Epoch 251/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0052 - accuracy: 0.9994 - val_loss: 0.0419 - val_accuracy: 0.9900\n",
            "Epoch 252/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0051 - accuracy: 0.9988 - val_loss: 0.0432 - val_accuracy: 0.9900\n",
            "Epoch 253/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0048 - accuracy: 0.9991 - val_loss: 0.0413 - val_accuracy: 0.9915\n",
            "Epoch 254/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0049 - accuracy: 0.9991 - val_loss: 0.0397 - val_accuracy: 0.9915\n",
            "Epoch 255/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0045 - accuracy: 0.9994 - val_loss: 0.0474 - val_accuracy: 0.9879\n",
            "Epoch 256/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0045 - accuracy: 0.9994 - val_loss: 0.0444 - val_accuracy: 0.9908\n",
            "Epoch 257/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0050 - accuracy: 0.9994 - val_loss: 0.0491 - val_accuracy: 0.9879\n",
            "Epoch 258/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0049 - accuracy: 0.9988 - val_loss: 0.0430 - val_accuracy: 0.9900\n",
            "Epoch 259/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0053 - accuracy: 0.9994 - val_loss: 0.0447 - val_accuracy: 0.9900\n",
            "Epoch 260/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0049 - accuracy: 0.9988 - val_loss: 0.0453 - val_accuracy: 0.9893\n",
            "Epoch 261/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0058 - accuracy: 0.9991 - val_loss: 0.0496 - val_accuracy: 0.9886\n",
            "Epoch 262/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0045 - accuracy: 0.9994 - val_loss: 0.0478 - val_accuracy: 0.9879\n",
            "Epoch 263/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0048 - accuracy: 0.9988 - val_loss: 0.0472 - val_accuracy: 0.9879\n",
            "Epoch 264/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0049 - accuracy: 0.9991 - val_loss: 0.0524 - val_accuracy: 0.9872\n",
            "Epoch 265/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0052 - accuracy: 0.9988 - val_loss: 0.0471 - val_accuracy: 0.9900\n",
            "Epoch 266/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0051 - accuracy: 0.9991 - val_loss: 0.0504 - val_accuracy: 0.9886\n",
            "Epoch 267/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0046 - accuracy: 0.9994 - val_loss: 0.0445 - val_accuracy: 0.9900\n",
            "Epoch 268/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0051 - accuracy: 0.9985 - val_loss: 0.0447 - val_accuracy: 0.9900\n",
            "Epoch 269/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0039 - accuracy: 0.9997 - val_loss: 0.0490 - val_accuracy: 0.9872\n",
            "Epoch 270/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0042 - accuracy: 0.9994 - val_loss: 0.0465 - val_accuracy: 0.9886\n",
            "Epoch 271/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0045 - accuracy: 0.9991 - val_loss: 0.0565 - val_accuracy: 0.9879\n",
            "Epoch 272/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0039 - accuracy: 0.9994 - val_loss: 0.0508 - val_accuracy: 0.9879\n",
            "Epoch 273/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0043 - accuracy: 0.9994 - val_loss: 0.0465 - val_accuracy: 0.9886\n",
            "Epoch 274/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0046 - accuracy: 0.9988 - val_loss: 0.0434 - val_accuracy: 0.9893\n",
            "Epoch 275/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0048 - accuracy: 0.9988 - val_loss: 0.0454 - val_accuracy: 0.9879\n",
            "Epoch 276/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0039 - accuracy: 0.9997 - val_loss: 0.0470 - val_accuracy: 0.9900\n",
            "Epoch 277/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0041 - accuracy: 0.9994 - val_loss: 0.0456 - val_accuracy: 0.9879\n",
            "Epoch 278/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0045 - accuracy: 0.9991 - val_loss: 0.0455 - val_accuracy: 0.9908\n",
            "Epoch 279/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0039 - accuracy: 0.9994 - val_loss: 0.0418 - val_accuracy: 0.9915\n",
            "Epoch 280/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0038 - accuracy: 0.9994 - val_loss: 0.0582 - val_accuracy: 0.9865\n",
            "Epoch 281/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0048 - accuracy: 0.9991 - val_loss: 0.0523 - val_accuracy: 0.9900\n",
            "Epoch 282/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0042 - accuracy: 0.9991 - val_loss: 0.0449 - val_accuracy: 0.9879\n",
            "Epoch 283/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0037 - accuracy: 0.9997 - val_loss: 0.0447 - val_accuracy: 0.9908\n",
            "Epoch 284/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0042 - accuracy: 0.9994 - val_loss: 0.0479 - val_accuracy: 0.9900\n",
            "Epoch 285/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0039 - accuracy: 0.9997 - val_loss: 0.0486 - val_accuracy: 0.9915\n",
            "Epoch 286/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0043 - accuracy: 0.9991 - val_loss: 0.0474 - val_accuracy: 0.9915\n",
            "Epoch 287/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0040 - accuracy: 0.9994 - val_loss: 0.0438 - val_accuracy: 0.9879\n",
            "Epoch 288/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0037 - accuracy: 0.9997 - val_loss: 0.0498 - val_accuracy: 0.9879\n",
            "Epoch 289/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0038 - accuracy: 0.9997 - val_loss: 0.0453 - val_accuracy: 0.9893\n",
            "Epoch 290/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0041 - accuracy: 0.9991 - val_loss: 0.0458 - val_accuracy: 0.9908\n",
            "Epoch 291/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0044 - accuracy: 0.9988 - val_loss: 0.0492 - val_accuracy: 0.9900\n",
            "Epoch 292/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0040 - accuracy: 0.9994 - val_loss: 0.0672 - val_accuracy: 0.9872\n",
            "Epoch 293/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0042 - accuracy: 0.9994 - val_loss: 0.0547 - val_accuracy: 0.9900\n",
            "Epoch 294/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0049 - accuracy: 0.9994 - val_loss: 0.0512 - val_accuracy: 0.9908\n",
            "Epoch 295/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0036 - accuracy: 0.9994 - val_loss: 0.0537 - val_accuracy: 0.9886\n",
            "Epoch 296/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0037 - accuracy: 0.9994 - val_loss: 0.0476 - val_accuracy: 0.9900\n",
            "Epoch 297/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0040 - accuracy: 0.9991 - val_loss: 0.0494 - val_accuracy: 0.9893\n",
            "Epoch 298/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0042 - accuracy: 0.9991 - val_loss: 0.0460 - val_accuracy: 0.9908\n",
            "Epoch 299/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0032 - accuracy: 0.9997 - val_loss: 0.0495 - val_accuracy: 0.9908\n",
            "Epoch 300/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0041 - accuracy: 0.9991 - val_loss: 0.0428 - val_accuracy: 0.9900\n",
            "Epoch 301/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0039 - accuracy: 0.9994 - val_loss: 0.0599 - val_accuracy: 0.9893\n",
            "Epoch 302/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0034 - accuracy: 0.9994 - val_loss: 0.0449 - val_accuracy: 0.9908\n",
            "Epoch 303/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0036 - accuracy: 0.9994 - val_loss: 0.0590 - val_accuracy: 0.9886\n",
            "Epoch 304/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0039 - accuracy: 0.9991 - val_loss: 0.0487 - val_accuracy: 0.9879\n",
            "Epoch 305/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0037 - accuracy: 0.9994 - val_loss: 0.0431 - val_accuracy: 0.9893\n",
            "Epoch 306/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0036 - accuracy: 0.9994 - val_loss: 0.0573 - val_accuracy: 0.9879\n",
            "Epoch 307/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0036 - accuracy: 0.9994 - val_loss: 0.0465 - val_accuracy: 0.9915\n",
            "Epoch 308/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0037 - accuracy: 0.9997 - val_loss: 0.0597 - val_accuracy: 0.9886\n",
            "Epoch 309/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0034 - accuracy: 0.9997 - val_loss: 0.0536 - val_accuracy: 0.9886\n",
            "Epoch 310/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0040 - accuracy: 0.9994 - val_loss: 0.0469 - val_accuracy: 0.9893\n",
            "Epoch 311/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0041 - accuracy: 0.9985 - val_loss: 0.0464 - val_accuracy: 0.9900\n",
            "Epoch 312/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0034 - accuracy: 0.9997 - val_loss: 0.0526 - val_accuracy: 0.9886\n",
            "Epoch 313/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0038 - accuracy: 0.9994 - val_loss: 0.0507 - val_accuracy: 0.9886\n",
            "Epoch 314/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0038 - accuracy: 0.9994 - val_loss: 0.0537 - val_accuracy: 0.9908\n",
            "Epoch 315/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0032 - accuracy: 0.9997 - val_loss: 0.0605 - val_accuracy: 0.9893\n",
            "Epoch 316/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0029 - accuracy: 0.9997 - val_loss: 0.0636 - val_accuracy: 0.9872\n",
            "Epoch 317/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0031 - accuracy: 0.9997 - val_loss: 0.0606 - val_accuracy: 0.9872\n",
            "Epoch 318/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0032 - accuracy: 0.9994 - val_loss: 0.0626 - val_accuracy: 0.9893\n",
            "Epoch 319/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0030 - accuracy: 0.9994 - val_loss: 0.0503 - val_accuracy: 0.9900\n",
            "Epoch 320/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0036 - accuracy: 0.9997 - val_loss: 0.0590 - val_accuracy: 0.9879\n",
            "Epoch 321/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0033 - accuracy: 0.9997 - val_loss: 0.0519 - val_accuracy: 0.9872\n",
            "Epoch 322/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0034 - accuracy: 0.9994 - val_loss: 0.0540 - val_accuracy: 0.9872\n",
            "Epoch 323/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0032 - accuracy: 0.9997 - val_loss: 0.0654 - val_accuracy: 0.9886\n",
            "Epoch 324/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0036 - accuracy: 0.9994 - val_loss: 0.0591 - val_accuracy: 0.9886\n",
            "Epoch 325/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0031 - accuracy: 0.9997 - val_loss: 0.0567 - val_accuracy: 0.9893\n",
            "Epoch 326/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0038 - accuracy: 0.9991 - val_loss: 0.0494 - val_accuracy: 0.9893\n",
            "Epoch 327/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0039 - accuracy: 0.9991 - val_loss: 0.0541 - val_accuracy: 0.9893\n",
            "Epoch 328/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0033 - accuracy: 0.9997 - val_loss: 0.0571 - val_accuracy: 0.9886\n",
            "Epoch 329/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0031 - accuracy: 0.9994 - val_loss: 0.0498 - val_accuracy: 0.9893\n",
            "Epoch 330/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0029 - accuracy: 0.9997 - val_loss: 0.0579 - val_accuracy: 0.9893\n",
            "Epoch 331/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0037 - accuracy: 0.9991 - val_loss: 0.0559 - val_accuracy: 0.9886\n",
            "Epoch 332/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0037 - accuracy: 0.9994 - val_loss: 0.0527 - val_accuracy: 0.9900\n",
            "Epoch 333/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0037 - accuracy: 0.9994 - val_loss: 0.0533 - val_accuracy: 0.9893\n",
            "Epoch 334/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0033 - accuracy: 0.9994 - val_loss: 0.0538 - val_accuracy: 0.9893\n",
            "Epoch 335/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0032 - accuracy: 0.9994 - val_loss: 0.0632 - val_accuracy: 0.9879\n",
            "Epoch 336/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0040 - accuracy: 0.9988 - val_loss: 0.0595 - val_accuracy: 0.9886\n",
            "Epoch 337/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0036 - accuracy: 0.9994 - val_loss: 0.0431 - val_accuracy: 0.9915\n",
            "Epoch 338/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0033 - accuracy: 0.9991 - val_loss: 0.0658 - val_accuracy: 0.9872\n",
            "Epoch 339/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0029 - accuracy: 0.9997 - val_loss: 0.0515 - val_accuracy: 0.9886\n",
            "Epoch 340/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0029 - accuracy: 0.9997 - val_loss: 0.0612 - val_accuracy: 0.9879\n",
            "Epoch 341/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0031 - accuracy: 0.9994 - val_loss: 0.0520 - val_accuracy: 0.9900\n",
            "Epoch 342/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0034 - accuracy: 0.9994 - val_loss: 0.0491 - val_accuracy: 0.9893\n",
            "Epoch 343/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0036 - accuracy: 0.9994 - val_loss: 0.0626 - val_accuracy: 0.9879\n",
            "Epoch 344/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0032 - accuracy: 0.9994 - val_loss: 0.0511 - val_accuracy: 0.9893\n",
            "Epoch 345/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.0584 - val_accuracy: 0.9886\n",
            "Epoch 346/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0027 - accuracy: 0.9997 - val_loss: 0.0479 - val_accuracy: 0.9922\n",
            "Epoch 347/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0027 - accuracy: 0.9997 - val_loss: 0.0560 - val_accuracy: 0.9900\n",
            "Epoch 348/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.0502 - val_accuracy: 0.9908\n",
            "Epoch 349/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0036 - accuracy: 0.9988 - val_loss: 0.0633 - val_accuracy: 0.9886\n",
            "Epoch 350/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0026 - accuracy: 0.9997 - val_loss: 0.0476 - val_accuracy: 0.9915\n",
            "Epoch 351/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0038 - accuracy: 0.9994 - val_loss: 0.0515 - val_accuracy: 0.9893\n",
            "Epoch 352/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0741 - val_accuracy: 0.9872\n",
            "Epoch 353/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0031 - accuracy: 0.9997 - val_loss: 0.0580 - val_accuracy: 0.9900\n",
            "Epoch 354/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0030 - accuracy: 0.9994 - val_loss: 0.0605 - val_accuracy: 0.9886\n",
            "Epoch 355/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0039 - accuracy: 0.9991 - val_loss: 0.0619 - val_accuracy: 0.9900\n",
            "Epoch 356/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.0598 - val_accuracy: 0.9879\n",
            "Epoch 357/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0026 - accuracy: 0.9997 - val_loss: 0.0556 - val_accuracy: 0.9893\n",
            "Epoch 358/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0020 - accuracy: 0.9994 - val_loss: 0.0512 - val_accuracy: 0.9908\n",
            "Epoch 359/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0026 - accuracy: 0.9997 - val_loss: 0.0568 - val_accuracy: 0.9900\n",
            "Epoch 360/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.0621 - val_accuracy: 0.9893\n",
            "Epoch 361/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0036 - accuracy: 0.9988 - val_loss: 0.0613 - val_accuracy: 0.9893\n",
            "Epoch 362/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0039 - accuracy: 0.9988 - val_loss: 0.0609 - val_accuracy: 0.9886\n",
            "Epoch 363/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.0521 - val_accuracy: 0.9915\n",
            "Epoch 364/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0022 - accuracy: 0.9997 - val_loss: 0.0614 - val_accuracy: 0.9893\n",
            "Epoch 365/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.0661 - val_accuracy: 0.9886\n",
            "Epoch 366/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.0677 - val_accuracy: 0.9872\n",
            "Epoch 367/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0034 - accuracy: 0.9994 - val_loss: 0.0533 - val_accuracy: 0.9915\n",
            "Epoch 368/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0023 - accuracy: 0.9997 - val_loss: 0.0601 - val_accuracy: 0.9886\n",
            "Epoch 369/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.0538 - val_accuracy: 0.9908\n",
            "Epoch 370/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.0859 - val_accuracy: 0.9858\n",
            "Epoch 371/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0019 - accuracy: 0.9994 - val_loss: 0.0512 - val_accuracy: 0.9908\n",
            "Epoch 372/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0024 - accuracy: 0.9997 - val_loss: 0.0643 - val_accuracy: 0.9886\n",
            "Epoch 373/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.0579 - val_accuracy: 0.9922\n",
            "Epoch 374/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 0.0563 - val_accuracy: 0.9908\n",
            "Epoch 375/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0022 - accuracy: 0.9994 - val_loss: 0.0577 - val_accuracy: 0.9908\n",
            "Epoch 376/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0032 - accuracy: 0.9991 - val_loss: 0.0627 - val_accuracy: 0.9893\n",
            "Epoch 377/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0014 - accuracy: 0.9994 - val_loss: 0.0575 - val_accuracy: 0.9886\n",
            "Epoch 378/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0020 - accuracy: 0.9991 - val_loss: 0.0514 - val_accuracy: 0.9908\n",
            "Epoch 379/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.5976e-04 - accuracy: 1.0000 - val_loss: 0.0648 - val_accuracy: 0.9886\n",
            "Epoch 380/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0011 - accuracy: 0.9997 - val_loss: 0.0534 - val_accuracy: 0.9915\n",
            "Epoch 381/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0013 - accuracy: 0.9997 - val_loss: 0.0625 - val_accuracy: 0.9879\n",
            "Epoch 382/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0022 - accuracy: 0.9991 - val_loss: 0.0523 - val_accuracy: 0.9915\n",
            "Epoch 383/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0015 - accuracy: 0.9994 - val_loss: 0.0535 - val_accuracy: 0.9908\n",
            "Epoch 384/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0015 - accuracy: 0.9994 - val_loss: 0.0667 - val_accuracy: 0.9879\n",
            "Epoch 385/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0020 - accuracy: 0.9997 - val_loss: 0.0621 - val_accuracy: 0.9879\n",
            "Epoch 386/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.5940e-04 - accuracy: 0.9997 - val_loss: 0.0573 - val_accuracy: 0.9900\n",
            "Epoch 387/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0028 - accuracy: 0.9991 - val_loss: 0.0498 - val_accuracy: 0.9922\n",
            "Epoch 388/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.8202e-04 - accuracy: 1.0000 - val_loss: 0.0495 - val_accuracy: 0.9900\n",
            "Epoch 389/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0011 - accuracy: 0.9994 - val_loss: 0.0562 - val_accuracy: 0.9915\n",
            "Epoch 390/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.2349e-04 - accuracy: 1.0000 - val_loss: 0.0606 - val_accuracy: 0.9893\n",
            "Epoch 391/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0017 - accuracy: 0.9991 - val_loss: 0.0571 - val_accuracy: 0.9908\n",
            "Epoch 392/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0015 - accuracy: 0.9994 - val_loss: 0.0613 - val_accuracy: 0.9900\n",
            "Epoch 393/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.5013e-04 - accuracy: 0.9997 - val_loss: 0.0592 - val_accuracy: 0.9893\n",
            "Epoch 394/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.9349e-04 - accuracy: 0.9997 - val_loss: 0.0590 - val_accuracy: 0.9908\n",
            "Epoch 395/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0020 - accuracy: 0.9991 - val_loss: 0.0583 - val_accuracy: 0.9900\n",
            "Epoch 396/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.9867e-04 - accuracy: 0.9997 - val_loss: 0.0605 - val_accuracy: 0.9893\n",
            "Epoch 397/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0011 - accuracy: 0.9994 - val_loss: 0.0575 - val_accuracy: 0.9893\n",
            "Epoch 398/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0015 - accuracy: 0.9994 - val_loss: 0.0492 - val_accuracy: 0.9915\n",
            "Epoch 399/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.0480e-04 - accuracy: 0.9997 - val_loss: 0.0581 - val_accuracy: 0.9900\n",
            "Epoch 400/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0022 - accuracy: 0.9994 - val_loss: 0.0530 - val_accuracy: 0.9900\n",
            "Epoch 401/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.4621e-04 - accuracy: 1.0000 - val_loss: 0.0595 - val_accuracy: 0.9893\n",
            "Epoch 402/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 0.0591 - val_accuracy: 0.9893\n",
            "Epoch 403/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.9672e-04 - accuracy: 0.9997 - val_loss: 0.0595 - val_accuracy: 0.9893\n",
            "Epoch 404/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.5874e-04 - accuracy: 0.9997 - val_loss: 0.0631 - val_accuracy: 0.9908\n",
            "Epoch 405/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0013 - accuracy: 0.9997 - val_loss: 0.0622 - val_accuracy: 0.9900\n",
            "Epoch 406/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.0236e-04 - accuracy: 1.0000 - val_loss: 0.0725 - val_accuracy: 0.9865\n",
            "Epoch 407/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.0699e-04 - accuracy: 1.0000 - val_loss: 0.0518 - val_accuracy: 0.9908\n",
            "Epoch 408/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.7029e-04 - accuracy: 0.9994 - val_loss: 0.0751 - val_accuracy: 0.9879\n",
            "Epoch 409/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.3961e-04 - accuracy: 1.0000 - val_loss: 0.0741 - val_accuracy: 0.9879\n",
            "Epoch 410/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.3884e-04 - accuracy: 0.9997 - val_loss: 0.0801 - val_accuracy: 0.9872\n",
            "Epoch 411/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0019 - accuracy: 0.9997 - val_loss: 0.0615 - val_accuracy: 0.9900\n",
            "Epoch 412/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.5444e-04 - accuracy: 0.9997 - val_loss: 0.0639 - val_accuracy: 0.9886\n",
            "Epoch 413/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.9284e-04 - accuracy: 1.0000 - val_loss: 0.0644 - val_accuracy: 0.9900\n",
            "Epoch 414/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.5930e-04 - accuracy: 0.9997 - val_loss: 0.0635 - val_accuracy: 0.9900\n",
            "Epoch 415/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.4287e-04 - accuracy: 0.9997 - val_loss: 0.0589 - val_accuracy: 0.9900\n",
            "Epoch 416/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.9584e-04 - accuracy: 0.9997 - val_loss: 0.0587 - val_accuracy: 0.9893\n",
            "Epoch 417/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0013 - accuracy: 0.9997 - val_loss: 0.0736 - val_accuracy: 0.9865\n",
            "Epoch 418/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.1120e-04 - accuracy: 0.9997 - val_loss: 0.0611 - val_accuracy: 0.9879\n",
            "Epoch 419/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.1450e-04 - accuracy: 0.9997 - val_loss: 0.0603 - val_accuracy: 0.9893\n",
            "Epoch 420/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.7481e-04 - accuracy: 0.9997 - val_loss: 0.0633 - val_accuracy: 0.9908\n",
            "Epoch 421/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.3766e-04 - accuracy: 1.0000 - val_loss: 0.0708 - val_accuracy: 0.9886\n",
            "Epoch 422/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.2997e-04 - accuracy: 1.0000 - val_loss: 0.0652 - val_accuracy: 0.9893\n",
            "Epoch 423/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.6787e-04 - accuracy: 0.9994 - val_loss: 0.0776 - val_accuracy: 0.9851\n",
            "Epoch 424/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.2522e-04 - accuracy: 0.9997 - val_loss: 0.0551 - val_accuracy: 0.9915\n",
            "Epoch 425/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.3728e-04 - accuracy: 0.9997 - val_loss: 0.0592 - val_accuracy: 0.9915\n",
            "Epoch 426/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.5183e-04 - accuracy: 1.0000 - val_loss: 0.0642 - val_accuracy: 0.9893\n",
            "Epoch 427/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.2505e-04 - accuracy: 1.0000 - val_loss: 0.0621 - val_accuracy: 0.9908\n",
            "Epoch 428/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.2584e-04 - accuracy: 0.9997 - val_loss: 0.0695 - val_accuracy: 0.9879\n",
            "Epoch 429/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.8707e-04 - accuracy: 0.9997 - val_loss: 0.0651 - val_accuracy: 0.9900\n",
            "Epoch 430/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0013 - accuracy: 0.9997 - val_loss: 0.0711 - val_accuracy: 0.9879\n",
            "Epoch 431/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.0076e-04 - accuracy: 0.9997 - val_loss: 0.0592 - val_accuracy: 0.9900\n",
            "Epoch 432/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.2506e-04 - accuracy: 0.9997 - val_loss: 0.0578 - val_accuracy: 0.9900\n",
            "Epoch 433/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.3236e-05 - accuracy: 1.0000 - val_loss: 0.0584 - val_accuracy: 0.9915\n",
            "Epoch 434/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.3134e-05 - accuracy: 1.0000 - val_loss: 0.0688 - val_accuracy: 0.9900\n",
            "Epoch 435/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0013 - accuracy: 0.9994 - val_loss: 0.0565 - val_accuracy: 0.9922\n",
            "Epoch 436/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.6696e-04 - accuracy: 0.9997 - val_loss: 0.0551 - val_accuracy: 0.9908\n",
            "Epoch 437/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.8020e-05 - accuracy: 1.0000 - val_loss: 0.0581 - val_accuracy: 0.9900\n",
            "Epoch 438/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.0924e-04 - accuracy: 0.9997 - val_loss: 0.0698 - val_accuracy: 0.9893\n",
            "Epoch 439/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.4165e-04 - accuracy: 0.9997 - val_loss: 0.0585 - val_accuracy: 0.9893\n",
            "Epoch 440/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.2161e-04 - accuracy: 1.0000 - val_loss: 0.0642 - val_accuracy: 0.9900\n",
            "Epoch 441/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.7172e-04 - accuracy: 0.9997 - val_loss: 0.0835 - val_accuracy: 0.9858\n",
            "Epoch 442/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.0207e-05 - accuracy: 1.0000 - val_loss: 0.0537 - val_accuracy: 0.9915\n",
            "Epoch 443/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.9183e-04 - accuracy: 0.9997 - val_loss: 0.0547 - val_accuracy: 0.9908\n",
            "Epoch 444/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.1849e-04 - accuracy: 0.9997 - val_loss: 0.0653 - val_accuracy: 0.9900\n",
            "Epoch 445/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.2525e-05 - accuracy: 1.0000 - val_loss: 0.0675 - val_accuracy: 0.9893\n",
            "Epoch 446/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.2174e-04 - accuracy: 0.9997 - val_loss: 0.0714 - val_accuracy: 0.9879\n",
            "Epoch 447/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.6425e-04 - accuracy: 0.9997 - val_loss: 0.0597 - val_accuracy: 0.9900\n",
            "Epoch 448/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.8717e-05 - accuracy: 1.0000 - val_loss: 0.0576 - val_accuracy: 0.9915\n",
            "Epoch 449/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.4355e-05 - accuracy: 1.0000 - val_loss: 0.0549 - val_accuracy: 0.9900\n",
            "Epoch 450/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.1757e-04 - accuracy: 0.9997 - val_loss: 0.0542 - val_accuracy: 0.9936\n",
            "Epoch 451/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.8173e-05 - accuracy: 1.0000 - val_loss: 0.0592 - val_accuracy: 0.9908\n",
            "Epoch 452/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.1948e-04 - accuracy: 1.0000 - val_loss: 0.0549 - val_accuracy: 0.9915\n",
            "Epoch 453/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.7872e-04 - accuracy: 1.0000 - val_loss: 0.0715 - val_accuracy: 0.9893\n",
            "Epoch 454/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.2390e-04 - accuracy: 1.0000 - val_loss: 0.0590 - val_accuracy: 0.9908\n",
            "Epoch 455/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.8366e-04 - accuracy: 1.0000 - val_loss: 0.0727 - val_accuracy: 0.9908\n",
            "Epoch 456/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.6862e-05 - accuracy: 1.0000 - val_loss: 0.0591 - val_accuracy: 0.9922\n",
            "Epoch 457/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.4101e-05 - accuracy: 1.0000 - val_loss: 0.0779 - val_accuracy: 0.9893\n",
            "Epoch 458/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.6990e-04 - accuracy: 1.0000 - val_loss: 0.0575 - val_accuracy: 0.9908\n",
            "Epoch 459/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.8533e-04 - accuracy: 0.9997 - val_loss: 0.0576 - val_accuracy: 0.9915\n",
            "Epoch 460/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.8120e-04 - accuracy: 1.0000 - val_loss: 0.0564 - val_accuracy: 0.9922\n",
            "Epoch 461/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.2556e-05 - accuracy: 1.0000 - val_loss: 0.0507 - val_accuracy: 0.9915\n",
            "Epoch 462/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.9634e-04 - accuracy: 1.0000 - val_loss: 0.0583 - val_accuracy: 0.9929\n",
            "Epoch 463/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.8684e-04 - accuracy: 0.9997 - val_loss: 0.0635 - val_accuracy: 0.9893\n",
            "Epoch 464/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.6110e-05 - accuracy: 1.0000 - val_loss: 0.0698 - val_accuracy: 0.9886\n",
            "Epoch 465/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.4267e-05 - accuracy: 1.0000 - val_loss: 0.0605 - val_accuracy: 0.9922\n",
            "Epoch 466/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.2826e-04 - accuracy: 0.9997 - val_loss: 0.0629 - val_accuracy: 0.9908\n",
            "Epoch 467/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.6568e-04 - accuracy: 1.0000 - val_loss: 0.0491 - val_accuracy: 0.9929\n",
            "Epoch 468/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.8848e-04 - accuracy: 1.0000 - val_loss: 0.0839 - val_accuracy: 0.9886\n",
            "Epoch 469/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.0156e-05 - accuracy: 1.0000 - val_loss: 0.0588 - val_accuracy: 0.9900\n",
            "Epoch 470/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0013 - accuracy: 0.9994 - val_loss: 0.0582 - val_accuracy: 0.9915\n",
            "Epoch 471/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.5283e-05 - accuracy: 1.0000 - val_loss: 0.0558 - val_accuracy: 0.9929\n",
            "Epoch 472/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.8792e-04 - accuracy: 0.9997 - val_loss: 0.0587 - val_accuracy: 0.9908\n",
            "Epoch 473/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.6882e-04 - accuracy: 0.9997 - val_loss: 0.0528 - val_accuracy: 0.9915\n",
            "Epoch 474/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.8576e-05 - accuracy: 1.0000 - val_loss: 0.0607 - val_accuracy: 0.9908\n",
            "Epoch 475/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.5668e-05 - accuracy: 1.0000 - val_loss: 0.0524 - val_accuracy: 0.9929\n",
            "Epoch 476/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.8156e-04 - accuracy: 0.9997 - val_loss: 0.0550 - val_accuracy: 0.9922\n",
            "Epoch 477/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.8588e-05 - accuracy: 1.0000 - val_loss: 0.0533 - val_accuracy: 0.9922\n",
            "Epoch 478/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.2865e-04 - accuracy: 0.9997 - val_loss: 0.0508 - val_accuracy: 0.9922\n",
            "Epoch 479/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.0151e-04 - accuracy: 0.9997 - val_loss: 0.0520 - val_accuracy: 0.9915\n",
            "Epoch 480/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.0939e-05 - accuracy: 1.0000 - val_loss: 0.0712 - val_accuracy: 0.9908\n",
            "Epoch 481/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.2255e-04 - accuracy: 0.9997 - val_loss: 0.0563 - val_accuracy: 0.9915\n",
            "Epoch 482/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.0585e-04 - accuracy: 0.9997 - val_loss: 0.0524 - val_accuracy: 0.9929\n",
            "Epoch 483/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.0212e-05 - accuracy: 1.0000 - val_loss: 0.0550 - val_accuracy: 0.9922\n",
            "Epoch 484/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.5169e-05 - accuracy: 1.0000 - val_loss: 0.0688 - val_accuracy: 0.9886\n",
            "Epoch 485/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.8950e-04 - accuracy: 1.0000 - val_loss: 0.0593 - val_accuracy: 0.9900\n",
            "Epoch 486/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.1218e-05 - accuracy: 1.0000 - val_loss: 0.1046 - val_accuracy: 0.9865\n",
            "Epoch 487/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.7298e-04 - accuracy: 0.9997 - val_loss: 0.0687 - val_accuracy: 0.9900\n",
            "Epoch 488/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.6150e-05 - accuracy: 1.0000 - val_loss: 0.0725 - val_accuracy: 0.9893\n",
            "Epoch 489/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.1765e-04 - accuracy: 1.0000 - val_loss: 0.0646 - val_accuracy: 0.9908\n",
            "Epoch 490/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.1151e-04 - accuracy: 1.0000 - val_loss: 0.0628 - val_accuracy: 0.9908\n",
            "Epoch 491/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.7105e-05 - accuracy: 1.0000 - val_loss: 0.0626 - val_accuracy: 0.9915\n",
            "Epoch 492/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.1512e-05 - accuracy: 1.0000 - val_loss: 0.0655 - val_accuracy: 0.9915\n",
            "Epoch 493/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.7039e-04 - accuracy: 0.9997 - val_loss: 0.0543 - val_accuracy: 0.9922\n",
            "Epoch 494/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0012 - accuracy: 0.9997 - val_loss: 0.0650 - val_accuracy: 0.9915\n",
            "Epoch 495/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.1253e-05 - accuracy: 1.0000 - val_loss: 0.0566 - val_accuracy: 0.9922\n",
            "Epoch 496/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.0893e-05 - accuracy: 1.0000 - val_loss: 0.0671 - val_accuracy: 0.9908\n",
            "Epoch 497/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.3836e-05 - accuracy: 1.0000 - val_loss: 0.0665 - val_accuracy: 0.9908\n",
            "Epoch 498/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.1910e-05 - accuracy: 1.0000 - val_loss: 0.0613 - val_accuracy: 0.9908\n",
            "Epoch 499/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.4329e-05 - accuracy: 1.0000 - val_loss: 0.0606 - val_accuracy: 0.9915\n",
            "Epoch 500/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0011 - accuracy: 0.9994 - val_loss: 0.0636 - val_accuracy: 0.9908\n",
            "Epoch 501/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.3113e-06 - accuracy: 1.0000 - val_loss: 0.0587 - val_accuracy: 0.9908\n",
            "Epoch 502/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.6650e-05 - accuracy: 1.0000 - val_loss: 0.0749 - val_accuracy: 0.9893\n",
            "Epoch 503/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.8012e-04 - accuracy: 0.9994 - val_loss: 0.0717 - val_accuracy: 0.9900\n",
            "Epoch 504/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.2438e-06 - accuracy: 1.0000 - val_loss: 0.0711 - val_accuracy: 0.9886\n",
            "Epoch 505/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.9164e-05 - accuracy: 1.0000 - val_loss: 0.0634 - val_accuracy: 0.9908\n",
            "Epoch 506/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.2737e-06 - accuracy: 1.0000 - val_loss: 0.0593 - val_accuracy: 0.9929\n",
            "Epoch 507/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.2451e-04 - accuracy: 1.0000 - val_loss: 0.0667 - val_accuracy: 0.9908\n",
            "Epoch 508/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.6333e-05 - accuracy: 1.0000 - val_loss: 0.0791 - val_accuracy: 0.9893\n",
            "Epoch 509/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.1372e-04 - accuracy: 0.9997 - val_loss: 0.0874 - val_accuracy: 0.9886\n",
            "Epoch 510/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.7629e-06 - accuracy: 1.0000 - val_loss: 0.0638 - val_accuracy: 0.9908\n",
            "Epoch 511/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.5308e-05 - accuracy: 1.0000 - val_loss: 0.0702 - val_accuracy: 0.9900\n",
            "Epoch 512/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.8766e-06 - accuracy: 1.0000 - val_loss: 0.0809 - val_accuracy: 0.9908\n",
            "Epoch 513/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.3162e-05 - accuracy: 1.0000 - val_loss: 0.0810 - val_accuracy: 0.9900\n",
            "Epoch 514/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.1537e-05 - accuracy: 1.0000 - val_loss: 0.0851 - val_accuracy: 0.9886\n",
            "Epoch 515/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.8871e-05 - accuracy: 1.0000 - val_loss: 0.0684 - val_accuracy: 0.9922\n",
            "Epoch 516/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.7315e-06 - accuracy: 1.0000 - val_loss: 0.0686 - val_accuracy: 0.9908\n",
            "Epoch 517/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.0152e-05 - accuracy: 1.0000 - val_loss: 0.0681 - val_accuracy: 0.9929\n",
            "Epoch 518/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.1386e-04 - accuracy: 1.0000 - val_loss: 0.0671 - val_accuracy: 0.9922\n",
            "Epoch 519/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.5514e-06 - accuracy: 1.0000 - val_loss: 0.0650 - val_accuracy: 0.9908\n",
            "Epoch 520/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.0251e-06 - accuracy: 1.0000 - val_loss: 0.0639 - val_accuracy: 0.9915\n",
            "Epoch 521/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.9868e-05 - accuracy: 1.0000 - val_loss: 0.0791 - val_accuracy: 0.9900\n",
            "Epoch 522/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.7992e-06 - accuracy: 1.0000 - val_loss: 0.0634 - val_accuracy: 0.9922\n",
            "Epoch 523/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.0280e-04 - accuracy: 1.0000 - val_loss: 0.0851 - val_accuracy: 0.9893\n",
            "Epoch 524/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.6000e-05 - accuracy: 1.0000 - val_loss: 0.0923 - val_accuracy: 0.9886\n",
            "Epoch 525/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.5116e-04 - accuracy: 0.9997 - val_loss: 0.0882 - val_accuracy: 0.9886\n",
            "Epoch 526/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.2601e-06 - accuracy: 1.0000 - val_loss: 0.0872 - val_accuracy: 0.9886\n",
            "Epoch 527/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.4063e-05 - accuracy: 1.0000 - val_loss: 0.0879 - val_accuracy: 0.9893\n",
            "Epoch 528/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.5418e-05 - accuracy: 1.0000 - val_loss: 0.0670 - val_accuracy: 0.9900\n",
            "Epoch 529/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.2746e-06 - accuracy: 1.0000 - val_loss: 0.0664 - val_accuracy: 0.9922\n",
            "Epoch 530/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.1560e-05 - accuracy: 1.0000 - val_loss: 0.0744 - val_accuracy: 0.9893\n",
            "Epoch 531/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.2687e-05 - accuracy: 1.0000 - val_loss: 0.0697 - val_accuracy: 0.9893\n",
            "Epoch 532/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.6741e-06 - accuracy: 1.0000 - val_loss: 0.0722 - val_accuracy: 0.9922\n",
            "Epoch 533/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.2169e-06 - accuracy: 1.0000 - val_loss: 0.0825 - val_accuracy: 0.9900\n",
            "Epoch 534/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.7282e-06 - accuracy: 1.0000 - val_loss: 0.0685 - val_accuracy: 0.9908\n",
            "Epoch 535/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.4546e-04 - accuracy: 0.9997 - val_loss: 0.0751 - val_accuracy: 0.9893\n",
            "Epoch 536/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.1997e-06 - accuracy: 1.0000 - val_loss: 0.0752 - val_accuracy: 0.9915\n",
            "Epoch 537/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.5508e-04 - accuracy: 0.9997 - val_loss: 0.0695 - val_accuracy: 0.9922\n",
            "Epoch 538/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.3868e-06 - accuracy: 1.0000 - val_loss: 0.0685 - val_accuracy: 0.9929\n",
            "Epoch 539/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.5785e-06 - accuracy: 1.0000 - val_loss: 0.0678 - val_accuracy: 0.9915\n",
            "Epoch 540/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.8481e-05 - accuracy: 1.0000 - val_loss: 0.0611 - val_accuracy: 0.9915\n",
            "Epoch 541/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.7511e-06 - accuracy: 1.0000 - val_loss: 0.0950 - val_accuracy: 0.9893\n",
            "Epoch 542/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.6604e-05 - accuracy: 1.0000 - val_loss: 0.0648 - val_accuracy: 0.9915\n",
            "Epoch 543/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.6251e-06 - accuracy: 1.0000 - val_loss: 0.0797 - val_accuracy: 0.9893\n",
            "Epoch 544/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.7873e-06 - accuracy: 1.0000 - val_loss: 0.0770 - val_accuracy: 0.9893\n",
            "Epoch 545/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.8668e-04 - accuracy: 1.0000 - val_loss: 0.0792 - val_accuracy: 0.9886\n",
            "Epoch 546/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.4888e-06 - accuracy: 1.0000 - val_loss: 0.0949 - val_accuracy: 0.9886\n",
            "Epoch 547/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.1662e-05 - accuracy: 1.0000 - val_loss: 0.0705 - val_accuracy: 0.9893\n",
            "Epoch 548/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.2412e-06 - accuracy: 1.0000 - val_loss: 0.1053 - val_accuracy: 0.9908\n",
            "Epoch 549/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.3533e-06 - accuracy: 1.0000 - val_loss: 0.0842 - val_accuracy: 0.9893\n",
            "Epoch 550/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.3304e-06 - accuracy: 1.0000 - val_loss: 0.0893 - val_accuracy: 0.9886\n",
            "Epoch 551/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.3300e-05 - accuracy: 1.0000 - val_loss: 0.1161 - val_accuracy: 0.9886\n",
            "Epoch 552/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.6601e-06 - accuracy: 1.0000 - val_loss: 0.0932 - val_accuracy: 0.9900\n",
            "Epoch 553/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.0103e-06 - accuracy: 1.0000 - val_loss: 0.0761 - val_accuracy: 0.9922\n",
            "Epoch 554/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.1741e-06 - accuracy: 1.0000 - val_loss: 0.0811 - val_accuracy: 0.9900\n",
            "Epoch 555/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.1468e-05 - accuracy: 1.0000 - val_loss: 0.0810 - val_accuracy: 0.9908\n",
            "Epoch 556/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.7532e-06 - accuracy: 1.0000 - val_loss: 0.0756 - val_accuracy: 0.9908\n",
            "Epoch 557/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.7878e-04 - accuracy: 0.9997 - val_loss: 0.0857 - val_accuracy: 0.9922\n",
            "Epoch 558/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.7431e-07 - accuracy: 1.0000 - val_loss: 0.0765 - val_accuracy: 0.9908\n",
            "Epoch 559/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.2675e-05 - accuracy: 1.0000 - val_loss: 0.0855 - val_accuracy: 0.9900\n",
            "Epoch 560/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.4362e-07 - accuracy: 1.0000 - val_loss: 0.0834 - val_accuracy: 0.9893\n",
            "Epoch 561/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.0921e-05 - accuracy: 1.0000 - val_loss: 0.0740 - val_accuracy: 0.9915\n",
            "Epoch 562/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.1783e-06 - accuracy: 1.0000 - val_loss: 0.0714 - val_accuracy: 0.9908\n",
            "Epoch 563/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.6104e-06 - accuracy: 1.0000 - val_loss: 0.0860 - val_accuracy: 0.9886\n",
            "Epoch 564/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.8204e-06 - accuracy: 1.0000 - val_loss: 0.0876 - val_accuracy: 0.9915\n",
            "Epoch 565/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.0722e-05 - accuracy: 1.0000 - val_loss: 0.0775 - val_accuracy: 0.9908\n",
            "Epoch 566/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.6038e-06 - accuracy: 1.0000 - val_loss: 0.1016 - val_accuracy: 0.9872\n",
            "Epoch 567/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.2942e-05 - accuracy: 1.0000 - val_loss: 0.0825 - val_accuracy: 0.9900\n",
            "Epoch 568/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.4126e-04 - accuracy: 0.9997 - val_loss: 0.0790 - val_accuracy: 0.9908\n",
            "Epoch 569/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.5206e-05 - accuracy: 1.0000 - val_loss: 0.0857 - val_accuracy: 0.9900\n",
            "Epoch 570/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.0561e-07 - accuracy: 1.0000 - val_loss: 0.0798 - val_accuracy: 0.9915\n",
            "Epoch 571/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.0529e-05 - accuracy: 1.0000 - val_loss: 0.0992 - val_accuracy: 0.9886\n",
            "Epoch 572/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.2475e-06 - accuracy: 1.0000 - val_loss: 0.0889 - val_accuracy: 0.9915\n",
            "Epoch 573/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.4203e-06 - accuracy: 1.0000 - val_loss: 0.0935 - val_accuracy: 0.9893\n",
            "Epoch 574/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.3487e-06 - accuracy: 1.0000 - val_loss: 0.0857 - val_accuracy: 0.9893\n",
            "Epoch 575/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.8164e-07 - accuracy: 1.0000 - val_loss: 0.0780 - val_accuracy: 0.9929\n",
            "Epoch 576/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.1248e-06 - accuracy: 1.0000 - val_loss: 0.1072 - val_accuracy: 0.9886\n",
            "Epoch 577/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.0681e-06 - accuracy: 1.0000 - val_loss: 0.1074 - val_accuracy: 0.9893\n",
            "Epoch 578/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.6700e-06 - accuracy: 1.0000 - val_loss: 0.1139 - val_accuracy: 0.9886\n",
            "Epoch 579/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.0962e-04 - accuracy: 0.9997 - val_loss: 0.1125 - val_accuracy: 0.9893\n",
            "Epoch 580/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.7156e-05 - accuracy: 1.0000 - val_loss: 0.0912 - val_accuracy: 0.9900\n",
            "Epoch 581/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.8968e-06 - accuracy: 1.0000 - val_loss: 0.0906 - val_accuracy: 0.9900\n",
            "Epoch 582/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.6003e-04 - accuracy: 1.0000 - val_loss: 0.0843 - val_accuracy: 0.9900\n",
            "Epoch 583/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.9581e-07 - accuracy: 1.0000 - val_loss: 0.0846 - val_accuracy: 0.9900\n",
            "Epoch 584/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.4260e-07 - accuracy: 1.0000 - val_loss: 0.1050 - val_accuracy: 0.9893\n",
            "Epoch 585/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.0203e-06 - accuracy: 1.0000 - val_loss: 0.0814 - val_accuracy: 0.9900\n",
            "Epoch 586/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.6037e-06 - accuracy: 1.0000 - val_loss: 0.0868 - val_accuracy: 0.9900\n",
            "Epoch 587/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.9716e-07 - accuracy: 1.0000 - val_loss: 0.0932 - val_accuracy: 0.9893\n",
            "Epoch 588/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.4761e-06 - accuracy: 1.0000 - val_loss: 0.0910 - val_accuracy: 0.9893\n",
            "Epoch 589/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.1192e-05 - accuracy: 1.0000 - val_loss: 0.0893 - val_accuracy: 0.9922\n",
            "Epoch 590/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.6655e-06 - accuracy: 1.0000 - val_loss: 0.0814 - val_accuracy: 0.9908\n",
            "Epoch 591/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.1150e-06 - accuracy: 1.0000 - val_loss: 0.1003 - val_accuracy: 0.9886\n",
            "Epoch 592/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.3640e-06 - accuracy: 1.0000 - val_loss: 0.0875 - val_accuracy: 0.9886\n",
            "Epoch 593/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.2925e-07 - accuracy: 1.0000 - val_loss: 0.1013 - val_accuracy: 0.9893\n",
            "Epoch 594/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.9884e-06 - accuracy: 1.0000 - val_loss: 0.1038 - val_accuracy: 0.9893\n",
            "Epoch 595/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.2465e-07 - accuracy: 1.0000 - val_loss: 0.1245 - val_accuracy: 0.9893\n",
            "Epoch 596/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.1010e-07 - accuracy: 1.0000 - val_loss: 0.1111 - val_accuracy: 0.9879\n",
            "Epoch 597/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.9297e-05 - accuracy: 1.0000 - val_loss: 0.0846 - val_accuracy: 0.9915\n",
            "Epoch 598/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.3832e-04 - accuracy: 0.9997 - val_loss: 0.0747 - val_accuracy: 0.9929\n",
            "Epoch 599/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.3258e-06 - accuracy: 1.0000 - val_loss: 0.0750 - val_accuracy: 0.9922\n",
            "Epoch 600/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.3060e-07 - accuracy: 1.0000 - val_loss: 0.0960 - val_accuracy: 0.9893\n",
            "Epoch 601/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.8652e-06 - accuracy: 1.0000 - val_loss: 0.0899 - val_accuracy: 0.9915\n",
            "Epoch 602/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.7963e-04 - accuracy: 0.9997 - val_loss: 0.0917 - val_accuracy: 0.9900\n",
            "Epoch 603/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.7932e-07 - accuracy: 1.0000 - val_loss: 0.0946 - val_accuracy: 0.9900\n",
            "Epoch 604/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.1188e-07 - accuracy: 1.0000 - val_loss: 0.0996 - val_accuracy: 0.9900\n",
            "Epoch 605/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.0176e-06 - accuracy: 1.0000 - val_loss: 0.0838 - val_accuracy: 0.9900\n",
            "Epoch 606/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.7503e-07 - accuracy: 1.0000 - val_loss: 0.1174 - val_accuracy: 0.9893\n",
            "Epoch 607/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.5100e-06 - accuracy: 1.0000 - val_loss: 0.0901 - val_accuracy: 0.9908\n",
            "Epoch 608/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.0039e-06 - accuracy: 1.0000 - val_loss: 0.0872 - val_accuracy: 0.9900\n",
            "Epoch 609/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.1936e-07 - accuracy: 1.0000 - val_loss: 0.0975 - val_accuracy: 0.9893\n",
            "Epoch 610/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.8067e-06 - accuracy: 1.0000 - val_loss: 0.1016 - val_accuracy: 0.9900\n",
            "Epoch 611/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.0861e-04 - accuracy: 1.0000 - val_loss: 0.0918 - val_accuracy: 0.9915\n",
            "Epoch 612/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.1617e-06 - accuracy: 1.0000 - val_loss: 0.0982 - val_accuracy: 0.9908\n",
            "Epoch 613/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.4760e-07 - accuracy: 1.0000 - val_loss: 0.0994 - val_accuracy: 0.9900\n",
            "Epoch 614/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0012 - accuracy: 0.9997 - val_loss: 0.0928 - val_accuracy: 0.9908\n",
            "Epoch 615/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.1324e-07 - accuracy: 1.0000 - val_loss: 0.0937 - val_accuracy: 0.9908\n",
            "Epoch 616/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.3540e-07 - accuracy: 1.0000 - val_loss: 0.0996 - val_accuracy: 0.9900\n",
            "Epoch 617/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.8237e-06 - accuracy: 1.0000 - val_loss: 0.0959 - val_accuracy: 0.9915\n",
            "Epoch 618/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.3302e-07 - accuracy: 1.0000 - val_loss: 0.1034 - val_accuracy: 0.9893\n",
            "Epoch 619/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.0686e-04 - accuracy: 1.0000 - val_loss: 0.0978 - val_accuracy: 0.9886\n",
            "Epoch 620/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.1678e-06 - accuracy: 1.0000 - val_loss: 0.0914 - val_accuracy: 0.9908\n",
            "Epoch 621/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.3859e-07 - accuracy: 1.0000 - val_loss: 0.1072 - val_accuracy: 0.9900\n",
            "Epoch 622/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.8215e-07 - accuracy: 1.0000 - val_loss: 0.0969 - val_accuracy: 0.9900\n",
            "Epoch 623/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.6439e-07 - accuracy: 1.0000 - val_loss: 0.1027 - val_accuracy: 0.9908\n",
            "Epoch 624/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.6817e-07 - accuracy: 1.0000 - val_loss: 0.1040 - val_accuracy: 0.9908\n",
            "Epoch 625/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.3569e-07 - accuracy: 1.0000 - val_loss: 0.0870 - val_accuracy: 0.9915\n",
            "Epoch 626/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.3853e-07 - accuracy: 1.0000 - val_loss: 0.0889 - val_accuracy: 0.9908\n",
            "Epoch 627/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.8429e-05 - accuracy: 1.0000 - val_loss: 0.0989 - val_accuracy: 0.9908\n",
            "Epoch 628/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.6450e-07 - accuracy: 1.0000 - val_loss: 0.0987 - val_accuracy: 0.9908\n",
            "Epoch 629/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.3824e-07 - accuracy: 1.0000 - val_loss: 0.1052 - val_accuracy: 0.9908\n",
            "Epoch 630/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.1252e-07 - accuracy: 1.0000 - val_loss: 0.0864 - val_accuracy: 0.9922\n",
            "Epoch 631/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.0709e-07 - accuracy: 1.0000 - val_loss: 0.0850 - val_accuracy: 0.9929\n",
            "Epoch 632/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0010 - accuracy: 0.9997 - val_loss: 0.0890 - val_accuracy: 0.9922\n",
            "Epoch 633/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.4091e-07 - accuracy: 1.0000 - val_loss: 0.0807 - val_accuracy: 0.9936\n",
            "Epoch 634/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.3720e-06 - accuracy: 1.0000 - val_loss: 0.0992 - val_accuracy: 0.9900\n",
            "Epoch 635/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.1845e-06 - accuracy: 1.0000 - val_loss: 0.0786 - val_accuracy: 0.9929\n",
            "Epoch 636/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.4675e-05 - accuracy: 1.0000 - val_loss: 0.0898 - val_accuracy: 0.9915\n",
            "Epoch 637/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.2908e-07 - accuracy: 1.0000 - val_loss: 0.0856 - val_accuracy: 0.9929\n",
            "Epoch 638/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.0045e-07 - accuracy: 1.0000 - val_loss: 0.0833 - val_accuracy: 0.9922\n",
            "Epoch 639/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.8183e-07 - accuracy: 1.0000 - val_loss: 0.0936 - val_accuracy: 0.9915\n",
            "Epoch 640/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.0774e-06 - accuracy: 1.0000 - val_loss: 0.1102 - val_accuracy: 0.9900\n",
            "Epoch 641/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.3284e-06 - accuracy: 1.0000 - val_loss: 0.0941 - val_accuracy: 0.9915\n",
            "Epoch 642/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.2133e-07 - accuracy: 1.0000 - val_loss: 0.0873 - val_accuracy: 0.9900\n",
            "Epoch 643/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.3481e-07 - accuracy: 1.0000 - val_loss: 0.0961 - val_accuracy: 0.9908\n",
            "Epoch 644/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.1660e-04 - accuracy: 1.0000 - val_loss: 0.1026 - val_accuracy: 0.9900\n",
            "Epoch 645/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.0700e-07 - accuracy: 1.0000 - val_loss: 0.0974 - val_accuracy: 0.9908\n",
            "Epoch 646/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.5838e-08 - accuracy: 1.0000 - val_loss: 0.0824 - val_accuracy: 0.9922\n",
            "Epoch 647/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.4366e-05 - accuracy: 1.0000 - val_loss: 0.1016 - val_accuracy: 0.9900\n",
            "Epoch 648/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.8366e-07 - accuracy: 1.0000 - val_loss: 0.1009 - val_accuracy: 0.9915\n",
            "Epoch 649/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.3531e-07 - accuracy: 1.0000 - val_loss: 0.0946 - val_accuracy: 0.9929\n",
            "Epoch 650/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.0231e-06 - accuracy: 1.0000 - val_loss: 0.1039 - val_accuracy: 0.9908\n",
            "Epoch 651/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.2811e-07 - accuracy: 1.0000 - val_loss: 0.0948 - val_accuracy: 0.9908\n",
            "Epoch 652/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.3359e-07 - accuracy: 1.0000 - val_loss: 0.1119 - val_accuracy: 0.9908\n",
            "Epoch 653/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.7033e-05 - accuracy: 1.0000 - val_loss: 0.0946 - val_accuracy: 0.9922\n",
            "Epoch 654/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.6932e-07 - accuracy: 1.0000 - val_loss: 0.0893 - val_accuracy: 0.9915\n",
            "Epoch 655/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.9124e-07 - accuracy: 1.0000 - val_loss: 0.1133 - val_accuracy: 0.9915\n",
            "Epoch 656/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.3391e-06 - accuracy: 1.0000 - val_loss: 0.0957 - val_accuracy: 0.9922\n",
            "Epoch 657/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.1745e-08 - accuracy: 1.0000 - val_loss: 0.1060 - val_accuracy: 0.9915\n",
            "Epoch 658/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.0857e-07 - accuracy: 1.0000 - val_loss: 0.0961 - val_accuracy: 0.9908\n",
            "Epoch 659/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.5903e-07 - accuracy: 1.0000 - val_loss: 0.0901 - val_accuracy: 0.9922\n",
            "Epoch 660/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.5975e-07 - accuracy: 1.0000 - val_loss: 0.1067 - val_accuracy: 0.9908\n",
            "Epoch 661/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.3323e-07 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 0.9908\n",
            "Epoch 662/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.6064e-06 - accuracy: 1.0000 - val_loss: 0.0932 - val_accuracy: 0.9915\n",
            "Epoch 663/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.0420e-04 - accuracy: 1.0000 - val_loss: 0.1100 - val_accuracy: 0.9915\n",
            "Epoch 664/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.2107e-06 - accuracy: 1.0000 - val_loss: 0.0993 - val_accuracy: 0.9915\n",
            "Epoch 665/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.0352e-08 - accuracy: 1.0000 - val_loss: 0.1034 - val_accuracy: 0.9908\n",
            "Epoch 666/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.2999e-07 - accuracy: 1.0000 - val_loss: 0.0935 - val_accuracy: 0.9908\n",
            "Epoch 667/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.6236e-04 - accuracy: 0.9997 - val_loss: 0.1152 - val_accuracy: 0.9900\n",
            "Epoch 668/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.2694e-07 - accuracy: 1.0000 - val_loss: 0.0938 - val_accuracy: 0.9922\n",
            "Epoch 669/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.5299e-07 - accuracy: 1.0000 - val_loss: 0.1069 - val_accuracy: 0.9915\n",
            "Epoch 670/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.2534e-08 - accuracy: 1.0000 - val_loss: 0.0960 - val_accuracy: 0.9908\n",
            "Epoch 671/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.1873e-07 - accuracy: 1.0000 - val_loss: 0.0934 - val_accuracy: 0.9915\n",
            "Epoch 672/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.4377e-07 - accuracy: 1.0000 - val_loss: 0.0953 - val_accuracy: 0.9922\n",
            "Epoch 673/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.9863e-06 - accuracy: 1.0000 - val_loss: 0.1010 - val_accuracy: 0.9908\n",
            "Epoch 674/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.7335e-08 - accuracy: 1.0000 - val_loss: 0.0947 - val_accuracy: 0.9922\n",
            "Epoch 675/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.6393e-04 - accuracy: 1.0000 - val_loss: 0.0989 - val_accuracy: 0.9922\n",
            "Epoch 676/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.1575e-05 - accuracy: 1.0000 - val_loss: 0.0944 - val_accuracy: 0.9929\n",
            "Epoch 677/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.4907e-08 - accuracy: 1.0000 - val_loss: 0.0935 - val_accuracy: 0.9922\n",
            "Epoch 678/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.9445e-06 - accuracy: 1.0000 - val_loss: 0.0964 - val_accuracy: 0.9922\n",
            "Epoch 679/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.9045e-08 - accuracy: 1.0000 - val_loss: 0.0968 - val_accuracy: 0.9922\n",
            "Epoch 680/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.3562e-08 - accuracy: 1.0000 - val_loss: 0.0972 - val_accuracy: 0.9915\n",
            "Epoch 681/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.0998e-04 - accuracy: 0.9997 - val_loss: 0.0974 - val_accuracy: 0.9900\n",
            "Epoch 682/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.4949e-08 - accuracy: 1.0000 - val_loss: 0.0970 - val_accuracy: 0.9908\n",
            "Epoch 683/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.9728e-08 - accuracy: 1.0000 - val_loss: 0.1047 - val_accuracy: 0.9915\n",
            "Epoch 684/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.0885e-07 - accuracy: 1.0000 - val_loss: 0.1069 - val_accuracy: 0.9908\n",
            "Epoch 685/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.2930e-08 - accuracy: 1.0000 - val_loss: 0.1047 - val_accuracy: 0.9893\n",
            "Epoch 686/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.8128e-07 - accuracy: 1.0000 - val_loss: 0.0924 - val_accuracy: 0.9915\n",
            "Epoch 687/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.0376e-06 - accuracy: 1.0000 - val_loss: 0.1080 - val_accuracy: 0.9915\n",
            "Epoch 688/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.6337e-08 - accuracy: 1.0000 - val_loss: 0.1104 - val_accuracy: 0.9908\n",
            "Epoch 689/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.6708e-07 - accuracy: 1.0000 - val_loss: 0.0973 - val_accuracy: 0.9900\n",
            "Epoch 690/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.4840e-08 - accuracy: 1.0000 - val_loss: 0.1004 - val_accuracy: 0.9929\n",
            "Epoch 691/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.6788e-08 - accuracy: 1.0000 - val_loss: 0.0975 - val_accuracy: 0.9908\n",
            "Epoch 692/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.5514e-08 - accuracy: 1.0000 - val_loss: 0.0910 - val_accuracy: 0.9922\n",
            "Epoch 693/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.2097e-08 - accuracy: 1.0000 - val_loss: 0.0974 - val_accuracy: 0.9908\n",
            "Epoch 694/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.9869e-08 - accuracy: 1.0000 - val_loss: 0.1072 - val_accuracy: 0.9915\n",
            "Epoch 695/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.9688e-08 - accuracy: 1.0000 - val_loss: 0.1030 - val_accuracy: 0.9908\n",
            "Epoch 696/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.1595e-07 - accuracy: 1.0000 - val_loss: 0.0871 - val_accuracy: 0.9922\n",
            "Epoch 697/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.7016e-08 - accuracy: 1.0000 - val_loss: 0.1082 - val_accuracy: 0.9908\n",
            "Epoch 698/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.3211e-07 - accuracy: 1.0000 - val_loss: 0.1043 - val_accuracy: 0.9915\n",
            "Epoch 699/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.3434e-08 - accuracy: 1.0000 - val_loss: 0.0865 - val_accuracy: 0.9922\n",
            "Epoch 700/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.1424e-08 - accuracy: 1.0000 - val_loss: 0.0982 - val_accuracy: 0.9908\n",
            "Epoch 701/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.0229e-07 - accuracy: 1.0000 - val_loss: 0.0974 - val_accuracy: 0.9922\n",
            "Epoch 702/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.4598e-08 - accuracy: 1.0000 - val_loss: 0.1006 - val_accuracy: 0.9908\n",
            "Epoch 703/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.0305e-07 - accuracy: 1.0000 - val_loss: 0.1091 - val_accuracy: 0.9915\n",
            "Epoch 704/1056\n",
            "103/103 [==============================] - 0s 4ms/step - loss: 4.1066e-08 - accuracy: 1.0000 - val_loss: 0.1047 - val_accuracy: 0.9893\n",
            "Epoch 705/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.4484e-08 - accuracy: 1.0000 - val_loss: 0.0963 - val_accuracy: 0.9908\n",
            "Epoch 706/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.7307e-08 - accuracy: 1.0000 - val_loss: 0.1096 - val_accuracy: 0.9900\n",
            "Epoch 707/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.6823e-08 - accuracy: 1.0000 - val_loss: 0.1024 - val_accuracy: 0.9908\n",
            "Epoch 708/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.7929e-08 - accuracy: 1.0000 - val_loss: 0.1061 - val_accuracy: 0.9908\n",
            "Epoch 709/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.6840e-07 - accuracy: 1.0000 - val_loss: 0.1036 - val_accuracy: 0.9900\n",
            "Epoch 710/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.6661e-08 - accuracy: 1.0000 - val_loss: 0.0992 - val_accuracy: 0.9908\n",
            "Epoch 711/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.4423e-08 - accuracy: 1.0000 - val_loss: 0.1057 - val_accuracy: 0.9915\n",
            "Epoch 712/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.0395e-08 - accuracy: 1.0000 - val_loss: 0.1048 - val_accuracy: 0.9900\n",
            "Epoch 713/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.0374e-08 - accuracy: 1.0000 - val_loss: 0.1105 - val_accuracy: 0.9900\n",
            "Epoch 714/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.1939e-08 - accuracy: 1.0000 - val_loss: 0.1011 - val_accuracy: 0.9915\n",
            "Epoch 715/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.5547e-08 - accuracy: 1.0000 - val_loss: 0.0996 - val_accuracy: 0.9915\n",
            "Epoch 716/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.1250e-08 - accuracy: 1.0000 - val_loss: 0.1054 - val_accuracy: 0.9908\n",
            "Epoch 717/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.1306e-08 - accuracy: 1.0000 - val_loss: 0.0988 - val_accuracy: 0.9915\n",
            "Epoch 718/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.1098e-08 - accuracy: 1.0000 - val_loss: 0.1004 - val_accuracy: 0.9922\n",
            "Epoch 719/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.0152e-08 - accuracy: 1.0000 - val_loss: 0.1005 - val_accuracy: 0.9915\n",
            "Epoch 720/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.9336e-08 - accuracy: 1.0000 - val_loss: 0.0994 - val_accuracy: 0.9922\n",
            "Epoch 721/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.9414e-08 - accuracy: 1.0000 - val_loss: 0.0989 - val_accuracy: 0.9915\n",
            "Epoch 722/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.8808e-08 - accuracy: 1.0000 - val_loss: 0.1029 - val_accuracy: 0.9915\n",
            "Epoch 723/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.8949e-08 - accuracy: 1.0000 - val_loss: 0.1002 - val_accuracy: 0.9915\n",
            "Epoch 724/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.8157e-08 - accuracy: 1.0000 - val_loss: 0.1021 - val_accuracy: 0.9915\n",
            "Epoch 725/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.8057e-08 - accuracy: 1.0000 - val_loss: 0.1012 - val_accuracy: 0.9915\n",
            "Epoch 726/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.7714e-08 - accuracy: 1.0000 - val_loss: 0.1031 - val_accuracy: 0.9915\n",
            "Epoch 727/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.7222e-08 - accuracy: 1.0000 - val_loss: 0.1020 - val_accuracy: 0.9915\n",
            "Epoch 728/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.7261e-08 - accuracy: 1.0000 - val_loss: 0.1024 - val_accuracy: 0.9915\n",
            "Epoch 729/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.6902e-08 - accuracy: 1.0000 - val_loss: 0.1029 - val_accuracy: 0.9915\n",
            "Epoch 730/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.6599e-08 - accuracy: 1.0000 - val_loss: 0.1051 - val_accuracy: 0.9908\n",
            "Epoch 731/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.6420e-08 - accuracy: 1.0000 - val_loss: 0.1053 - val_accuracy: 0.9915\n",
            "Epoch 732/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.6218e-08 - accuracy: 1.0000 - val_loss: 0.1005 - val_accuracy: 0.9915\n",
            "Epoch 733/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.5896e-08 - accuracy: 1.0000 - val_loss: 0.1042 - val_accuracy: 0.9900\n",
            "Epoch 734/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.5881e-08 - accuracy: 1.0000 - val_loss: 0.1037 - val_accuracy: 0.9915\n",
            "Epoch 735/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.5419e-08 - accuracy: 1.0000 - val_loss: 0.1042 - val_accuracy: 0.9915\n",
            "Epoch 736/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.5206e-08 - accuracy: 1.0000 - val_loss: 0.1013 - val_accuracy: 0.9915\n",
            "Epoch 737/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.5013e-08 - accuracy: 1.0000 - val_loss: 0.1046 - val_accuracy: 0.9915\n",
            "Epoch 738/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.4755e-08 - accuracy: 1.0000 - val_loss: 0.1058 - val_accuracy: 0.9908\n",
            "Epoch 739/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.4628e-08 - accuracy: 1.0000 - val_loss: 0.1053 - val_accuracy: 0.9915\n",
            "Epoch 740/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.4417e-08 - accuracy: 1.0000 - val_loss: 0.1035 - val_accuracy: 0.9915\n",
            "Epoch 741/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.4187e-08 - accuracy: 1.0000 - val_loss: 0.1051 - val_accuracy: 0.9915\n",
            "Epoch 742/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.4037e-08 - accuracy: 1.0000 - val_loss: 0.1060 - val_accuracy: 0.9915\n",
            "Epoch 743/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.3783e-08 - accuracy: 1.0000 - val_loss: 0.1044 - val_accuracy: 0.9915\n",
            "Epoch 744/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.3679e-08 - accuracy: 1.0000 - val_loss: 0.1049 - val_accuracy: 0.9915\n",
            "Epoch 745/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.3421e-08 - accuracy: 1.0000 - val_loss: 0.1062 - val_accuracy: 0.9915\n",
            "Epoch 746/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.3261e-08 - accuracy: 1.0000 - val_loss: 0.1065 - val_accuracy: 0.9915\n",
            "Epoch 747/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.3105e-08 - accuracy: 1.0000 - val_loss: 0.1040 - val_accuracy: 0.9915\n",
            "Epoch 748/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.2936e-08 - accuracy: 1.0000 - val_loss: 0.1066 - val_accuracy: 0.9915\n",
            "Epoch 749/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.2827e-08 - accuracy: 1.0000 - val_loss: 0.1040 - val_accuracy: 0.9915\n",
            "Epoch 750/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.2626e-08 - accuracy: 1.0000 - val_loss: 0.1050 - val_accuracy: 0.9915\n",
            "Epoch 751/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.2441e-08 - accuracy: 1.0000 - val_loss: 0.1049 - val_accuracy: 0.9915\n",
            "Epoch 752/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.2290e-08 - accuracy: 1.0000 - val_loss: 0.1059 - val_accuracy: 0.9915\n",
            "Epoch 753/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.2162e-08 - accuracy: 1.0000 - val_loss: 0.1047 - val_accuracy: 0.9915\n",
            "Epoch 754/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.1947e-08 - accuracy: 1.0000 - val_loss: 0.1035 - val_accuracy: 0.9915\n",
            "Epoch 755/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.1833e-08 - accuracy: 1.0000 - val_loss: 0.1053 - val_accuracy: 0.9915\n",
            "Epoch 756/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.1721e-08 - accuracy: 1.0000 - val_loss: 0.1062 - val_accuracy: 0.9915\n",
            "Epoch 757/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.1548e-08 - accuracy: 1.0000 - val_loss: 0.1055 - val_accuracy: 0.9915\n",
            "Epoch 758/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.1397e-08 - accuracy: 1.0000 - val_loss: 0.1059 - val_accuracy: 0.9915\n",
            "Epoch 759/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.1255e-08 - accuracy: 1.0000 - val_loss: 0.1056 - val_accuracy: 0.9915\n",
            "Epoch 760/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.1080e-08 - accuracy: 1.0000 - val_loss: 0.1078 - val_accuracy: 0.9915\n",
            "Epoch 761/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.1031e-08 - accuracy: 1.0000 - val_loss: 0.1055 - val_accuracy: 0.9915\n",
            "Epoch 762/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.0855e-08 - accuracy: 1.0000 - val_loss: 0.1046 - val_accuracy: 0.9915\n",
            "Epoch 763/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.0758e-08 - accuracy: 1.0000 - val_loss: 0.1056 - val_accuracy: 0.9915\n",
            "Epoch 764/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.0554e-08 - accuracy: 1.0000 - val_loss: 0.1048 - val_accuracy: 0.9915\n",
            "Epoch 765/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.0476e-08 - accuracy: 1.0000 - val_loss: 0.1073 - val_accuracy: 0.9915\n",
            "Epoch 766/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.0374e-08 - accuracy: 1.0000 - val_loss: 0.1067 - val_accuracy: 0.9915\n",
            "Epoch 767/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.0231e-08 - accuracy: 1.0000 - val_loss: 0.1062 - val_accuracy: 0.9915\n",
            "Epoch 768/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.0114e-08 - accuracy: 1.0000 - val_loss: 0.1061 - val_accuracy: 0.9915\n",
            "Epoch 769/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.9837e-09 - accuracy: 1.0000 - val_loss: 0.1074 - val_accuracy: 0.9915\n",
            "Epoch 770/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.8959e-09 - accuracy: 1.0000 - val_loss: 0.1070 - val_accuracy: 0.9915\n",
            "Epoch 771/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.7780e-09 - accuracy: 1.0000 - val_loss: 0.1065 - val_accuracy: 0.9915\n",
            "Epoch 772/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.6375e-09 - accuracy: 1.0000 - val_loss: 0.1060 - val_accuracy: 0.9915\n",
            "Epoch 773/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.5581e-09 - accuracy: 1.0000 - val_loss: 0.1071 - val_accuracy: 0.9915\n",
            "Epoch 774/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.4286e-09 - accuracy: 1.0000 - val_loss: 0.1068 - val_accuracy: 0.9915\n",
            "Epoch 775/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.3351e-09 - accuracy: 1.0000 - val_loss: 0.1066 - val_accuracy: 0.9915\n",
            "Epoch 776/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.2156e-09 - accuracy: 1.0000 - val_loss: 0.1059 - val_accuracy: 0.9915\n",
            "Epoch 777/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.1539e-09 - accuracy: 1.0000 - val_loss: 0.1072 - val_accuracy: 0.9915\n",
            "Epoch 778/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.0394e-09 - accuracy: 1.0000 - val_loss: 0.1060 - val_accuracy: 0.9915\n",
            "Epoch 779/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.9364e-09 - accuracy: 1.0000 - val_loss: 0.1075 - val_accuracy: 0.9915\n",
            "Epoch 780/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.8383e-09 - accuracy: 1.0000 - val_loss: 0.1064 - val_accuracy: 0.9915\n",
            "Epoch 781/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.7790e-09 - accuracy: 1.0000 - val_loss: 0.1070 - val_accuracy: 0.9915\n",
            "Epoch 782/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.7211e-09 - accuracy: 1.0000 - val_loss: 0.1065 - val_accuracy: 0.9915\n",
            "Epoch 783/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.6664e-09 - accuracy: 1.0000 - val_loss: 0.1071 - val_accuracy: 0.9915\n",
            "Epoch 784/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.6124e-09 - accuracy: 1.0000 - val_loss: 0.1060 - val_accuracy: 0.9915\n",
            "Epoch 785/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.5932e-09 - accuracy: 1.0000 - val_loss: 0.1069 - val_accuracy: 0.9915\n",
            "Epoch 786/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.5403e-09 - accuracy: 1.0000 - val_loss: 0.1073 - val_accuracy: 0.9915\n",
            "Epoch 787/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.4966e-09 - accuracy: 1.0000 - val_loss: 0.1077 - val_accuracy: 0.9915\n",
            "Epoch 788/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.4570e-09 - accuracy: 1.0000 - val_loss: 0.1078 - val_accuracy: 0.9915\n",
            "Epoch 789/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.4067e-09 - accuracy: 1.0000 - val_loss: 0.1079 - val_accuracy: 0.9915\n",
            "Epoch 790/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.3627e-09 - accuracy: 1.0000 - val_loss: 0.1080 - val_accuracy: 0.9915\n",
            "Epoch 791/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.3394e-09 - accuracy: 1.0000 - val_loss: 0.1081 - val_accuracy: 0.9915\n",
            "Epoch 792/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.2964e-09 - accuracy: 1.0000 - val_loss: 0.1076 - val_accuracy: 0.9915\n",
            "Epoch 793/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.2481e-09 - accuracy: 1.0000 - val_loss: 0.1082 - val_accuracy: 0.9915\n",
            "Epoch 794/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.2145e-09 - accuracy: 1.0000 - val_loss: 0.1078 - val_accuracy: 0.9915\n",
            "Epoch 795/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.1758e-09 - accuracy: 1.0000 - val_loss: 0.1080 - val_accuracy: 0.9915\n",
            "Epoch 796/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.1595e-09 - accuracy: 1.0000 - val_loss: 0.1078 - val_accuracy: 0.9915\n",
            "Epoch 797/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.1035e-09 - accuracy: 1.0000 - val_loss: 0.1090 - val_accuracy: 0.9915\n",
            "Epoch 798/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.0773e-09 - accuracy: 1.0000 - val_loss: 0.1091 - val_accuracy: 0.9915\n",
            "Epoch 799/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.0411e-09 - accuracy: 1.0000 - val_loss: 0.1085 - val_accuracy: 0.9915\n",
            "Epoch 800/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.0148e-09 - accuracy: 1.0000 - val_loss: 0.1083 - val_accuracy: 0.9915\n",
            "Epoch 801/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.9712e-09 - accuracy: 1.0000 - val_loss: 0.1090 - val_accuracy: 0.9915\n",
            "Epoch 802/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.9490e-09 - accuracy: 1.0000 - val_loss: 0.1085 - val_accuracy: 0.9915\n",
            "Epoch 803/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.9042e-09 - accuracy: 1.0000 - val_loss: 0.1097 - val_accuracy: 0.9915\n",
            "Epoch 804/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.8926e-09 - accuracy: 1.0000 - val_loss: 0.1086 - val_accuracy: 0.9915\n",
            "Epoch 805/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.8505e-09 - accuracy: 1.0000 - val_loss: 0.1089 - val_accuracy: 0.9915\n",
            "Epoch 806/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.8130e-09 - accuracy: 1.0000 - val_loss: 0.1089 - val_accuracy: 0.9915\n",
            "Epoch 807/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.7900e-09 - accuracy: 1.0000 - val_loss: 0.1087 - val_accuracy: 0.9915\n",
            "Epoch 808/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.7636e-09 - accuracy: 1.0000 - val_loss: 0.1087 - val_accuracy: 0.9915\n",
            "Epoch 809/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.7359e-09 - accuracy: 1.0000 - val_loss: 0.1087 - val_accuracy: 0.9915\n",
            "Epoch 810/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.7070e-09 - accuracy: 1.0000 - val_loss: 0.1092 - val_accuracy: 0.9915\n",
            "Epoch 811/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.6758e-09 - accuracy: 1.0000 - val_loss: 0.1090 - val_accuracy: 0.9915\n",
            "Epoch 812/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.6502e-09 - accuracy: 1.0000 - val_loss: 0.1091 - val_accuracy: 0.9915\n",
            "Epoch 813/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.6269e-09 - accuracy: 1.0000 - val_loss: 0.1095 - val_accuracy: 0.9915\n",
            "Epoch 814/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.5995e-09 - accuracy: 1.0000 - val_loss: 0.1099 - val_accuracy: 0.9915\n",
            "Epoch 815/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.5725e-09 - accuracy: 1.0000 - val_loss: 0.1100 - val_accuracy: 0.9915\n",
            "Epoch 816/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.5495e-09 - accuracy: 1.0000 - val_loss: 0.1101 - val_accuracy: 0.9915\n",
            "Epoch 817/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.5257e-09 - accuracy: 1.0000 - val_loss: 0.1097 - val_accuracy: 0.9915\n",
            "Epoch 818/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.4964e-09 - accuracy: 1.0000 - val_loss: 0.1102 - val_accuracy: 0.9915\n",
            "Epoch 819/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.4700e-09 - accuracy: 1.0000 - val_loss: 0.1098 - val_accuracy: 0.9915\n",
            "Epoch 820/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.4432e-09 - accuracy: 1.0000 - val_loss: 0.1102 - val_accuracy: 0.9915\n",
            "Epoch 821/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.4282e-09 - accuracy: 1.0000 - val_loss: 0.1095 - val_accuracy: 0.9915\n",
            "Epoch 822/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.4018e-09 - accuracy: 1.0000 - val_loss: 0.1098 - val_accuracy: 0.9915\n",
            "Epoch 823/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.3820e-09 - accuracy: 1.0000 - val_loss: 0.1096 - val_accuracy: 0.9915\n",
            "Epoch 824/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.3606e-09 - accuracy: 1.0000 - val_loss: 0.1095 - val_accuracy: 0.9915\n",
            "Epoch 825/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.3392e-09 - accuracy: 1.0000 - val_loss: 0.1099 - val_accuracy: 0.9915\n",
            "Epoch 826/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.3119e-09 - accuracy: 1.0000 - val_loss: 0.1102 - val_accuracy: 0.9915\n",
            "Epoch 827/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.2910e-09 - accuracy: 1.0000 - val_loss: 0.1109 - val_accuracy: 0.9915\n",
            "Epoch 828/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.2680e-09 - accuracy: 1.0000 - val_loss: 0.1108 - val_accuracy: 0.9915\n",
            "Epoch 829/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.2487e-09 - accuracy: 1.0000 - val_loss: 0.1109 - val_accuracy: 0.9915\n",
            "Epoch 830/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.2273e-09 - accuracy: 1.0000 - val_loss: 0.1109 - val_accuracy: 0.9915\n",
            "Epoch 831/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.2044e-09 - accuracy: 1.0000 - val_loss: 0.1109 - val_accuracy: 0.9915\n",
            "Epoch 832/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.1821e-09 - accuracy: 1.0000 - val_loss: 0.1111 - val_accuracy: 0.9915\n",
            "Epoch 833/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.1508e-09 - accuracy: 1.0000 - val_loss: 0.1101 - val_accuracy: 0.9915\n",
            "Epoch 834/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.1493e-09 - accuracy: 1.0000 - val_loss: 0.1107 - val_accuracy: 0.9915\n",
            "Epoch 835/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.1234e-09 - accuracy: 1.0000 - val_loss: 0.1114 - val_accuracy: 0.9915\n",
            "Epoch 836/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.1102e-09 - accuracy: 1.0000 - val_loss: 0.1113 - val_accuracy: 0.9915\n",
            "Epoch 837/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.0854e-09 - accuracy: 1.0000 - val_loss: 0.1110 - val_accuracy: 0.9915\n",
            "Epoch 838/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.0690e-09 - accuracy: 1.0000 - val_loss: 0.1113 - val_accuracy: 0.9915\n",
            "Epoch 839/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.0450e-09 - accuracy: 1.0000 - val_loss: 0.1115 - val_accuracy: 0.9915\n",
            "Epoch 840/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.0311e-09 - accuracy: 1.0000 - val_loss: 0.1112 - val_accuracy: 0.9915\n",
            "Epoch 841/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.0104e-09 - accuracy: 1.0000 - val_loss: 0.1110 - val_accuracy: 0.9915\n",
            "Epoch 842/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.9930e-09 - accuracy: 1.0000 - val_loss: 0.1110 - val_accuracy: 0.9915\n",
            "Epoch 843/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.9768e-09 - accuracy: 1.0000 - val_loss: 0.1116 - val_accuracy: 0.9915\n",
            "Epoch 844/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.9540e-09 - accuracy: 1.0000 - val_loss: 0.1112 - val_accuracy: 0.9915\n",
            "Epoch 845/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.9296e-09 - accuracy: 1.0000 - val_loss: 0.1118 - val_accuracy: 0.9915\n",
            "Epoch 846/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.9216e-09 - accuracy: 1.0000 - val_loss: 0.1119 - val_accuracy: 0.9915\n",
            "Epoch 847/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.9045e-09 - accuracy: 1.0000 - val_loss: 0.1121 - val_accuracy: 0.9915\n",
            "Epoch 848/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.8852e-09 - accuracy: 1.0000 - val_loss: 0.1117 - val_accuracy: 0.9915\n",
            "Epoch 849/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.8684e-09 - accuracy: 1.0000 - val_loss: 0.1118 - val_accuracy: 0.9915\n",
            "Epoch 850/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.8462e-09 - accuracy: 1.0000 - val_loss: 0.1122 - val_accuracy: 0.9915\n",
            "Epoch 851/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.8357e-09 - accuracy: 1.0000 - val_loss: 0.1117 - val_accuracy: 0.9915\n",
            "Epoch 852/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.8144e-09 - accuracy: 1.0000 - val_loss: 0.1117 - val_accuracy: 0.9915\n",
            "Epoch 853/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.7963e-09 - accuracy: 1.0000 - val_loss: 0.1115 - val_accuracy: 0.9915\n",
            "Epoch 854/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.7806e-09 - accuracy: 1.0000 - val_loss: 0.1120 - val_accuracy: 0.9915\n",
            "Epoch 855/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.7687e-09 - accuracy: 1.0000 - val_loss: 0.1120 - val_accuracy: 0.9915\n",
            "Epoch 856/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.7531e-09 - accuracy: 1.0000 - val_loss: 0.1119 - val_accuracy: 0.9915\n",
            "Epoch 857/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.7376e-09 - accuracy: 1.0000 - val_loss: 0.1118 - val_accuracy: 0.9915\n",
            "Epoch 858/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.7164e-09 - accuracy: 1.0000 - val_loss: 0.1120 - val_accuracy: 0.9915\n",
            "Epoch 859/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.7085e-09 - accuracy: 1.0000 - val_loss: 0.1118 - val_accuracy: 0.9915\n",
            "Epoch 860/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.6889e-09 - accuracy: 1.0000 - val_loss: 0.1126 - val_accuracy: 0.9915\n",
            "Epoch 861/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.6673e-09 - accuracy: 1.0000 - val_loss: 0.1119 - val_accuracy: 0.9915\n",
            "Epoch 862/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.6516e-09 - accuracy: 1.0000 - val_loss: 0.1130 - val_accuracy: 0.9915\n",
            "Epoch 863/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.6410e-09 - accuracy: 1.0000 - val_loss: 0.1124 - val_accuracy: 0.9915\n",
            "Epoch 864/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.6289e-09 - accuracy: 1.0000 - val_loss: 0.1130 - val_accuracy: 0.9915\n",
            "Epoch 865/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.6108e-09 - accuracy: 1.0000 - val_loss: 0.1125 - val_accuracy: 0.9915\n",
            "Epoch 866/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.6007e-09 - accuracy: 1.0000 - val_loss: 0.1127 - val_accuracy: 0.9915\n",
            "Epoch 867/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.5890e-09 - accuracy: 1.0000 - val_loss: 0.1127 - val_accuracy: 0.9915\n",
            "Epoch 868/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.5735e-09 - accuracy: 1.0000 - val_loss: 0.1130 - val_accuracy: 0.9915\n",
            "Epoch 869/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.5610e-09 - accuracy: 1.0000 - val_loss: 0.1130 - val_accuracy: 0.9915\n",
            "Epoch 870/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.5474e-09 - accuracy: 1.0000 - val_loss: 0.1127 - val_accuracy: 0.9915\n",
            "Epoch 871/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.5305e-09 - accuracy: 1.0000 - val_loss: 0.1128 - val_accuracy: 0.9915\n",
            "Epoch 872/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.5198e-09 - accuracy: 1.0000 - val_loss: 0.1128 - val_accuracy: 0.9915\n",
            "Epoch 873/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.4994e-09 - accuracy: 1.0000 - val_loss: 0.1133 - val_accuracy: 0.9915\n",
            "Epoch 874/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.4870e-09 - accuracy: 1.0000 - val_loss: 0.1135 - val_accuracy: 0.9915\n",
            "Epoch 875/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.4784e-09 - accuracy: 1.0000 - val_loss: 0.1131 - val_accuracy: 0.9915\n",
            "Epoch 876/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.4633e-09 - accuracy: 1.0000 - val_loss: 0.1135 - val_accuracy: 0.9915\n",
            "Epoch 877/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.4460e-09 - accuracy: 1.0000 - val_loss: 0.1129 - val_accuracy: 0.9915\n",
            "Epoch 878/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.4364e-09 - accuracy: 1.0000 - val_loss: 0.1130 - val_accuracy: 0.9915\n",
            "Epoch 879/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.4273e-09 - accuracy: 1.0000 - val_loss: 0.1133 - val_accuracy: 0.9915\n",
            "Epoch 880/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.4107e-09 - accuracy: 1.0000 - val_loss: 0.1135 - val_accuracy: 0.9915\n",
            "Epoch 881/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.4037e-09 - accuracy: 1.0000 - val_loss: 0.1134 - val_accuracy: 0.9915\n",
            "Epoch 882/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.3881e-09 - accuracy: 1.0000 - val_loss: 0.1134 - val_accuracy: 0.9915\n",
            "Epoch 883/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.3704e-09 - accuracy: 1.0000 - val_loss: 0.1132 - val_accuracy: 0.9915\n",
            "Epoch 884/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.3611e-09 - accuracy: 1.0000 - val_loss: 0.1136 - val_accuracy: 0.9915\n",
            "Epoch 885/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.3457e-09 - accuracy: 1.0000 - val_loss: 0.1138 - val_accuracy: 0.9915\n",
            "Epoch 886/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.3323e-09 - accuracy: 1.0000 - val_loss: 0.1141 - val_accuracy: 0.9915\n",
            "Epoch 887/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.3256e-09 - accuracy: 1.0000 - val_loss: 0.1136 - val_accuracy: 0.9915\n",
            "Epoch 888/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.3089e-09 - accuracy: 1.0000 - val_loss: 0.1138 - val_accuracy: 0.9915\n",
            "Epoch 889/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.3046e-09 - accuracy: 1.0000 - val_loss: 0.1142 - val_accuracy: 0.9915\n",
            "Epoch 890/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.2894e-09 - accuracy: 1.0000 - val_loss: 0.1137 - val_accuracy: 0.9915\n",
            "Epoch 891/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.2798e-09 - accuracy: 1.0000 - val_loss: 0.1138 - val_accuracy: 0.9915\n",
            "Epoch 892/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.2676e-09 - accuracy: 1.0000 - val_loss: 0.1138 - val_accuracy: 0.9915\n",
            "Epoch 893/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.2546e-09 - accuracy: 1.0000 - val_loss: 0.1140 - val_accuracy: 0.9915\n",
            "Epoch 894/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.2447e-09 - accuracy: 1.0000 - val_loss: 0.1140 - val_accuracy: 0.9915\n",
            "Epoch 895/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.2286e-09 - accuracy: 1.0000 - val_loss: 0.1143 - val_accuracy: 0.9915\n",
            "Epoch 896/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.2193e-09 - accuracy: 1.0000 - val_loss: 0.1141 - val_accuracy: 0.9915\n",
            "Epoch 897/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.2093e-09 - accuracy: 1.0000 - val_loss: 0.1138 - val_accuracy: 0.9915\n",
            "Epoch 898/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.1968e-09 - accuracy: 1.0000 - val_loss: 0.1139 - val_accuracy: 0.9915\n",
            "Epoch 899/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.1824e-09 - accuracy: 1.0000 - val_loss: 0.1143 - val_accuracy: 0.9915\n",
            "Epoch 900/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.1739e-09 - accuracy: 1.0000 - val_loss: 0.1146 - val_accuracy: 0.9915\n",
            "Epoch 901/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.1621e-09 - accuracy: 1.0000 - val_loss: 0.1147 - val_accuracy: 0.9915\n",
            "Epoch 902/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.1543e-09 - accuracy: 1.0000 - val_loss: 0.1143 - val_accuracy: 0.9915\n",
            "Epoch 903/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.1416e-09 - accuracy: 1.0000 - val_loss: 0.1145 - val_accuracy: 0.9915\n",
            "Epoch 904/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.1316e-09 - accuracy: 1.0000 - val_loss: 0.1143 - val_accuracy: 0.9915\n",
            "Epoch 905/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.1199e-09 - accuracy: 1.0000 - val_loss: 0.1144 - val_accuracy: 0.9915\n",
            "Epoch 906/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.1095e-09 - accuracy: 1.0000 - val_loss: 0.1147 - val_accuracy: 0.9915\n",
            "Epoch 907/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.1008e-09 - accuracy: 1.0000 - val_loss: 0.1147 - val_accuracy: 0.9915\n",
            "Epoch 908/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.0885e-09 - accuracy: 1.0000 - val_loss: 0.1146 - val_accuracy: 0.9915\n",
            "Epoch 909/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.0772e-09 - accuracy: 1.0000 - val_loss: 0.1147 - val_accuracy: 0.9915\n",
            "Epoch 910/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.0692e-09 - accuracy: 1.0000 - val_loss: 0.1150 - val_accuracy: 0.9915\n",
            "Epoch 911/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.0635e-09 - accuracy: 1.0000 - val_loss: 0.1148 - val_accuracy: 0.9915\n",
            "Epoch 912/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.0471e-09 - accuracy: 1.0000 - val_loss: 0.1148 - val_accuracy: 0.9915\n",
            "Epoch 913/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.0363e-09 - accuracy: 1.0000 - val_loss: 0.1149 - val_accuracy: 0.9915\n",
            "Epoch 914/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.0262e-09 - accuracy: 1.0000 - val_loss: 0.1151 - val_accuracy: 0.9915\n",
            "Epoch 915/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.0157e-09 - accuracy: 1.0000 - val_loss: 0.1151 - val_accuracy: 0.9915\n",
            "Epoch 916/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.0067e-09 - accuracy: 1.0000 - val_loss: 0.1151 - val_accuracy: 0.9915\n",
            "Epoch 917/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.9970e-09 - accuracy: 1.0000 - val_loss: 0.1150 - val_accuracy: 0.9915\n",
            "Epoch 918/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.9866e-09 - accuracy: 1.0000 - val_loss: 0.1148 - val_accuracy: 0.9915\n",
            "Epoch 919/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.9793e-09 - accuracy: 1.0000 - val_loss: 0.1150 - val_accuracy: 0.9915\n",
            "Epoch 920/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.9713e-09 - accuracy: 1.0000 - val_loss: 0.1149 - val_accuracy: 0.9915\n",
            "Epoch 921/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.9554e-09 - accuracy: 1.0000 - val_loss: 0.1150 - val_accuracy: 0.9915\n",
            "Epoch 922/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.9461e-09 - accuracy: 1.0000 - val_loss: 0.1151 - val_accuracy: 0.9915\n",
            "Epoch 923/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.9398e-09 - accuracy: 1.0000 - val_loss: 0.1150 - val_accuracy: 0.9915\n",
            "Epoch 924/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.9298e-09 - accuracy: 1.0000 - val_loss: 0.1152 - val_accuracy: 0.9915\n",
            "Epoch 925/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.9215e-09 - accuracy: 1.0000 - val_loss: 0.1152 - val_accuracy: 0.9915\n",
            "Epoch 926/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.9094e-09 - accuracy: 1.0000 - val_loss: 0.1149 - val_accuracy: 0.9915\n",
            "Epoch 927/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.9012e-09 - accuracy: 1.0000 - val_loss: 0.1151 - val_accuracy: 0.9915\n",
            "Epoch 928/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.8923e-09 - accuracy: 1.0000 - val_loss: 0.1151 - val_accuracy: 0.9915\n",
            "Epoch 929/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.8811e-09 - accuracy: 1.0000 - val_loss: 0.1152 - val_accuracy: 0.9915\n",
            "Epoch 930/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.8725e-09 - accuracy: 1.0000 - val_loss: 0.1155 - val_accuracy: 0.9915\n",
            "Epoch 931/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.8610e-09 - accuracy: 1.0000 - val_loss: 0.1156 - val_accuracy: 0.9915\n",
            "Epoch 932/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.8530e-09 - accuracy: 1.0000 - val_loss: 0.1157 - val_accuracy: 0.9915\n",
            "Epoch 933/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.8451e-09 - accuracy: 1.0000 - val_loss: 0.1157 - val_accuracy: 0.9915\n",
            "Epoch 934/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.8359e-09 - accuracy: 1.0000 - val_loss: 0.1156 - val_accuracy: 0.9915\n",
            "Epoch 935/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.8253e-09 - accuracy: 1.0000 - val_loss: 0.1157 - val_accuracy: 0.9915\n",
            "Epoch 936/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.8142e-09 - accuracy: 1.0000 - val_loss: 0.1158 - val_accuracy: 0.9915\n",
            "Epoch 937/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.8078e-09 - accuracy: 1.0000 - val_loss: 0.1158 - val_accuracy: 0.9915\n",
            "Epoch 938/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.7978e-09 - accuracy: 1.0000 - val_loss: 0.1157 - val_accuracy: 0.9915\n",
            "Epoch 939/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.7876e-09 - accuracy: 1.0000 - val_loss: 0.1156 - val_accuracy: 0.9915\n",
            "Epoch 940/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.7754e-09 - accuracy: 1.0000 - val_loss: 0.1160 - val_accuracy: 0.9915\n",
            "Epoch 941/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.7723e-09 - accuracy: 1.0000 - val_loss: 0.1160 - val_accuracy: 0.9915\n",
            "Epoch 942/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.7617e-09 - accuracy: 1.0000 - val_loss: 0.1162 - val_accuracy: 0.9915\n",
            "Epoch 943/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.7539e-09 - accuracy: 1.0000 - val_loss: 0.1159 - val_accuracy: 0.9915\n",
            "Epoch 944/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.7436e-09 - accuracy: 1.0000 - val_loss: 0.1161 - val_accuracy: 0.9915\n",
            "Epoch 945/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.7386e-09 - accuracy: 1.0000 - val_loss: 0.1162 - val_accuracy: 0.9915\n",
            "Epoch 946/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.7320e-09 - accuracy: 1.0000 - val_loss: 0.1163 - val_accuracy: 0.9915\n",
            "Epoch 947/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.7200e-09 - accuracy: 1.0000 - val_loss: 0.1162 - val_accuracy: 0.9915\n",
            "Epoch 948/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.7097e-09 - accuracy: 1.0000 - val_loss: 0.1164 - val_accuracy: 0.9915\n",
            "Epoch 949/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.7040e-09 - accuracy: 1.0000 - val_loss: 0.1163 - val_accuracy: 0.9915\n",
            "Epoch 950/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.6924e-09 - accuracy: 1.0000 - val_loss: 0.1164 - val_accuracy: 0.9915\n",
            "Epoch 951/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.6871e-09 - accuracy: 1.0000 - val_loss: 0.1165 - val_accuracy: 0.9915\n",
            "Epoch 952/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.6793e-09 - accuracy: 1.0000 - val_loss: 0.1167 - val_accuracy: 0.9915\n",
            "Epoch 953/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.6714e-09 - accuracy: 1.0000 - val_loss: 0.1166 - val_accuracy: 0.9915\n",
            "Epoch 954/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.6617e-09 - accuracy: 1.0000 - val_loss: 0.1165 - val_accuracy: 0.9915\n",
            "Epoch 955/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.6529e-09 - accuracy: 1.0000 - val_loss: 0.1165 - val_accuracy: 0.9915\n",
            "Epoch 956/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.6463e-09 - accuracy: 1.0000 - val_loss: 0.1164 - val_accuracy: 0.9915\n",
            "Epoch 957/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.6389e-09 - accuracy: 1.0000 - val_loss: 0.1164 - val_accuracy: 0.9915\n",
            "Epoch 958/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.6291e-09 - accuracy: 1.0000 - val_loss: 0.1164 - val_accuracy: 0.9915\n",
            "Epoch 959/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.6208e-09 - accuracy: 1.0000 - val_loss: 0.1165 - val_accuracy: 0.9915\n",
            "Epoch 960/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.6146e-09 - accuracy: 1.0000 - val_loss: 0.1165 - val_accuracy: 0.9915\n",
            "Epoch 961/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.5998e-09 - accuracy: 1.0000 - val_loss: 0.1169 - val_accuracy: 0.9915\n",
            "Epoch 962/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.5998e-09 - accuracy: 1.0000 - val_loss: 0.1168 - val_accuracy: 0.9915\n",
            "Epoch 963/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.5904e-09 - accuracy: 1.0000 - val_loss: 0.1168 - val_accuracy: 0.9915\n",
            "Epoch 964/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.5819e-09 - accuracy: 1.0000 - val_loss: 0.1167 - val_accuracy: 0.9915\n",
            "Epoch 965/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.5738e-09 - accuracy: 1.0000 - val_loss: 0.1166 - val_accuracy: 0.9915\n",
            "Epoch 966/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.5674e-09 - accuracy: 1.0000 - val_loss: 0.1169 - val_accuracy: 0.9915\n",
            "Epoch 967/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.5571e-09 - accuracy: 1.0000 - val_loss: 0.1171 - val_accuracy: 0.9915\n",
            "Epoch 968/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.5559e-09 - accuracy: 1.0000 - val_loss: 0.1171 - val_accuracy: 0.9915\n",
            "Epoch 969/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.5464e-09 - accuracy: 1.0000 - val_loss: 0.1170 - val_accuracy: 0.9915\n",
            "Epoch 970/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.5386e-09 - accuracy: 1.0000 - val_loss: 0.1171 - val_accuracy: 0.9915\n",
            "Epoch 971/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.5306e-09 - accuracy: 1.0000 - val_loss: 0.1169 - val_accuracy: 0.9915\n",
            "Epoch 972/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.5208e-09 - accuracy: 1.0000 - val_loss: 0.1171 - val_accuracy: 0.9915\n",
            "Epoch 973/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.5123e-09 - accuracy: 1.0000 - val_loss: 0.1173 - val_accuracy: 0.9915\n",
            "Epoch 974/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.5081e-09 - accuracy: 1.0000 - val_loss: 0.1174 - val_accuracy: 0.9915\n",
            "Epoch 975/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.4986e-09 - accuracy: 1.0000 - val_loss: 0.1176 - val_accuracy: 0.9915\n",
            "Epoch 976/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.4942e-09 - accuracy: 1.0000 - val_loss: 0.1175 - val_accuracy: 0.9915\n",
            "Epoch 977/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.4860e-09 - accuracy: 1.0000 - val_loss: 0.1174 - val_accuracy: 0.9915\n",
            "Epoch 978/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.4779e-09 - accuracy: 1.0000 - val_loss: 0.1176 - val_accuracy: 0.9915\n",
            "Epoch 979/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.4724e-09 - accuracy: 1.0000 - val_loss: 0.1175 - val_accuracy: 0.9915\n",
            "Epoch 980/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.4651e-09 - accuracy: 1.0000 - val_loss: 0.1175 - val_accuracy: 0.9915\n",
            "Epoch 981/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.4582e-09 - accuracy: 1.0000 - val_loss: 0.1177 - val_accuracy: 0.9915\n",
            "Epoch 982/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.4496e-09 - accuracy: 1.0000 - val_loss: 0.1178 - val_accuracy: 0.9915\n",
            "Epoch 983/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.4444e-09 - accuracy: 1.0000 - val_loss: 0.1177 - val_accuracy: 0.9915\n",
            "Epoch 984/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.4363e-09 - accuracy: 1.0000 - val_loss: 0.1178 - val_accuracy: 0.9915\n",
            "Epoch 985/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.4311e-09 - accuracy: 1.0000 - val_loss: 0.1178 - val_accuracy: 0.9915\n",
            "Epoch 986/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.4216e-09 - accuracy: 1.0000 - val_loss: 0.1176 - val_accuracy: 0.9915\n",
            "Epoch 987/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.4155e-09 - accuracy: 1.0000 - val_loss: 0.1177 - val_accuracy: 0.9915\n",
            "Epoch 988/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.4064e-09 - accuracy: 1.0000 - val_loss: 0.1180 - val_accuracy: 0.9915\n",
            "Epoch 989/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.4042e-09 - accuracy: 1.0000 - val_loss: 0.1180 - val_accuracy: 0.9915\n",
            "Epoch 990/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.3962e-09 - accuracy: 1.0000 - val_loss: 0.1180 - val_accuracy: 0.9915\n",
            "Epoch 991/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.3898e-09 - accuracy: 1.0000 - val_loss: 0.1180 - val_accuracy: 0.9915\n",
            "Epoch 992/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.3820e-09 - accuracy: 1.0000 - val_loss: 0.1180 - val_accuracy: 0.9915\n",
            "Epoch 993/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.3685e-09 - accuracy: 1.0000 - val_loss: 0.1184 - val_accuracy: 0.9915\n",
            "Epoch 994/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.3684e-09 - accuracy: 1.0000 - val_loss: 0.1186 - val_accuracy: 0.9915\n",
            "Epoch 995/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.3627e-09 - accuracy: 1.0000 - val_loss: 0.1183 - val_accuracy: 0.9915\n",
            "Epoch 996/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.3533e-09 - accuracy: 1.0000 - val_loss: 0.1186 - val_accuracy: 0.9915\n",
            "Epoch 997/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.3477e-09 - accuracy: 1.0000 - val_loss: 0.1186 - val_accuracy: 0.9915\n",
            "Epoch 998/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.3397e-09 - accuracy: 1.0000 - val_loss: 0.1187 - val_accuracy: 0.9915\n",
            "Epoch 999/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.3370e-09 - accuracy: 1.0000 - val_loss: 0.1187 - val_accuracy: 0.9915\n",
            "Epoch 1000/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.3294e-09 - accuracy: 1.0000 - val_loss: 0.1187 - val_accuracy: 0.9915\n",
            "Epoch 1001/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.3224e-09 - accuracy: 1.0000 - val_loss: 0.1187 - val_accuracy: 0.9915\n",
            "Epoch 1002/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.3144e-09 - accuracy: 1.0000 - val_loss: 0.1187 - val_accuracy: 0.9915\n",
            "Epoch 1003/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.3101e-09 - accuracy: 1.0000 - val_loss: 0.1186 - val_accuracy: 0.9915\n",
            "Epoch 1004/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.3042e-09 - accuracy: 1.0000 - val_loss: 0.1186 - val_accuracy: 0.9915\n",
            "Epoch 1005/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.2929e-09 - accuracy: 1.0000 - val_loss: 0.1189 - val_accuracy: 0.9915\n",
            "Epoch 1006/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.2914e-09 - accuracy: 1.0000 - val_loss: 0.1184 - val_accuracy: 0.9915\n",
            "Epoch 1007/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.2843e-09 - accuracy: 1.0000 - val_loss: 0.1186 - val_accuracy: 0.9915\n",
            "Epoch 1008/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.2772e-09 - accuracy: 1.0000 - val_loss: 0.1188 - val_accuracy: 0.9915\n",
            "Epoch 1009/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.2694e-09 - accuracy: 1.0000 - val_loss: 0.1188 - val_accuracy: 0.9915\n",
            "Epoch 1010/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.2658e-09 - accuracy: 1.0000 - val_loss: 0.1187 - val_accuracy: 0.9915\n",
            "Epoch 1011/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.2587e-09 - accuracy: 1.0000 - val_loss: 0.1190 - val_accuracy: 0.9915\n",
            "Epoch 1012/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.2486e-09 - accuracy: 1.0000 - val_loss: 0.1192 - val_accuracy: 0.9915\n",
            "Epoch 1013/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.2472e-09 - accuracy: 1.0000 - val_loss: 0.1193 - val_accuracy: 0.9915\n",
            "Epoch 1014/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.2426e-09 - accuracy: 1.0000 - val_loss: 0.1191 - val_accuracy: 0.9915\n",
            "Epoch 1015/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.2335e-09 - accuracy: 1.0000 - val_loss: 0.1194 - val_accuracy: 0.9915\n",
            "Epoch 1016/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.2284e-09 - accuracy: 1.0000 - val_loss: 0.1195 - val_accuracy: 0.9915\n",
            "Epoch 1017/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.2266e-09 - accuracy: 1.0000 - val_loss: 0.1193 - val_accuracy: 0.9915\n",
            "Epoch 1018/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.2186e-09 - accuracy: 1.0000 - val_loss: 0.1194 - val_accuracy: 0.9915\n",
            "Epoch 1019/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.2137e-09 - accuracy: 1.0000 - val_loss: 0.1193 - val_accuracy: 0.9915\n",
            "Epoch 1020/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.2076e-09 - accuracy: 1.0000 - val_loss: 0.1194 - val_accuracy: 0.9915\n",
            "Epoch 1021/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.2008e-09 - accuracy: 1.0000 - val_loss: 0.1196 - val_accuracy: 0.9915\n",
            "Epoch 1022/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.1977e-09 - accuracy: 1.0000 - val_loss: 0.1196 - val_accuracy: 0.9915\n",
            "Epoch 1023/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.1918e-09 - accuracy: 1.0000 - val_loss: 0.1196 - val_accuracy: 0.9915\n",
            "Epoch 1024/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.1822e-09 - accuracy: 1.0000 - val_loss: 0.1200 - val_accuracy: 0.9915\n",
            "Epoch 1025/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.1822e-09 - accuracy: 1.0000 - val_loss: 0.1198 - val_accuracy: 0.9915\n",
            "Epoch 1026/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.1742e-09 - accuracy: 1.0000 - val_loss: 0.1200 - val_accuracy: 0.9915\n",
            "Epoch 1027/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.1716e-09 - accuracy: 1.0000 - val_loss: 0.1200 - val_accuracy: 0.9915\n",
            "Epoch 1028/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.1666e-09 - accuracy: 1.0000 - val_loss: 0.1200 - val_accuracy: 0.9915\n",
            "Epoch 1029/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.1603e-09 - accuracy: 1.0000 - val_loss: 0.1199 - val_accuracy: 0.9915\n",
            "Epoch 1030/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.1554e-09 - accuracy: 1.0000 - val_loss: 0.1199 - val_accuracy: 0.9915\n",
            "Epoch 1031/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.1496e-09 - accuracy: 1.0000 - val_loss: 0.1199 - val_accuracy: 0.9915\n",
            "Epoch 1032/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.1439e-09 - accuracy: 1.0000 - val_loss: 0.1201 - val_accuracy: 0.9915\n",
            "Epoch 1033/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.1410e-09 - accuracy: 1.0000 - val_loss: 0.1200 - val_accuracy: 0.9915\n",
            "Epoch 1034/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.1348e-09 - accuracy: 1.0000 - val_loss: 0.1201 - val_accuracy: 0.9915\n",
            "Epoch 1035/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.1293e-09 - accuracy: 1.0000 - val_loss: 0.1201 - val_accuracy: 0.9915\n",
            "Epoch 1036/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.1252e-09 - accuracy: 1.0000 - val_loss: 0.1201 - val_accuracy: 0.9915\n",
            "Epoch 1037/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.1218e-09 - accuracy: 1.0000 - val_loss: 0.1202 - val_accuracy: 0.9915\n",
            "Epoch 1038/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.1155e-09 - accuracy: 1.0000 - val_loss: 0.1203 - val_accuracy: 0.9915\n",
            "Epoch 1039/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.1112e-09 - accuracy: 1.0000 - val_loss: 0.1203 - val_accuracy: 0.9915\n",
            "Epoch 1040/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.1064e-09 - accuracy: 1.0000 - val_loss: 0.1201 - val_accuracy: 0.9915\n",
            "Epoch 1041/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.1034e-09 - accuracy: 1.0000 - val_loss: 0.1204 - val_accuracy: 0.9915\n",
            "Epoch 1042/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.0980e-09 - accuracy: 1.0000 - val_loss: 0.1205 - val_accuracy: 0.9915\n",
            "Epoch 1043/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.0925e-09 - accuracy: 1.0000 - val_loss: 0.1204 - val_accuracy: 0.9915\n",
            "Epoch 1044/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.0885e-09 - accuracy: 1.0000 - val_loss: 0.1205 - val_accuracy: 0.9915\n",
            "Epoch 1045/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.0842e-09 - accuracy: 1.0000 - val_loss: 0.1204 - val_accuracy: 0.9915\n",
            "Epoch 1046/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.0783e-09 - accuracy: 1.0000 - val_loss: 0.1204 - val_accuracy: 0.9915\n",
            "Epoch 1047/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.0739e-09 - accuracy: 1.0000 - val_loss: 0.1206 - val_accuracy: 0.9915\n",
            "Epoch 1048/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.0694e-09 - accuracy: 1.0000 - val_loss: 0.1206 - val_accuracy: 0.9915\n",
            "Epoch 1049/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.0659e-09 - accuracy: 1.0000 - val_loss: 0.1206 - val_accuracy: 0.9915\n",
            "Epoch 1050/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.0599e-09 - accuracy: 1.0000 - val_loss: 0.1208 - val_accuracy: 0.9915\n",
            "Epoch 1051/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.0577e-09 - accuracy: 1.0000 - val_loss: 0.1207 - val_accuracy: 0.9915\n",
            "Epoch 1052/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.0527e-09 - accuracy: 1.0000 - val_loss: 0.1207 - val_accuracy: 0.9915\n",
            "Epoch 1053/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.0467e-09 - accuracy: 1.0000 - val_loss: 0.1209 - val_accuracy: 0.9915\n",
            "Epoch 1054/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.0422e-09 - accuracy: 1.0000 - val_loss: 0.1209 - val_accuracy: 0.9915\n",
            "Epoch 1055/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.0395e-09 - accuracy: 1.0000 - val_loss: 0.1209 - val_accuracy: 0.9915\n",
            "Epoch 1056/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.0334e-09 - accuracy: 1.0000 - val_loss: 0.1210 - val_accuracy: 0.9915\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4TKRFMNZB8g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "108efa77-6317-4792-f1c9-a51e767a6ff3"
      },
      "source": [
        "prediction = model.predict(XVALID)\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "accuracy = accuracy_score(YVALID, prediction.round())\n",
        "precision = precision_score(YVALID, prediction.round())\n",
        "recall = recall_score(YVALID, prediction.round())\n",
        "f1score = f1_score(YVALID, prediction.round())\n",
        "print(\"Multilayer NN Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
        "print(\"Precision: %.2f%%\" % (precision * 100.0))\n",
        "print(\"Recall: %.2f%%\" % (recall * 100.0))\n",
        "print(\"F1-score: %.2f\" % (f1score*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Multilayer NN Accuracy: 99.15%\n",
            "Precision: 97.38%\n",
            "Recall: 97.38%\n",
            "F1-score: 97.38\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7h3vISI7O4d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "4863b996-9be9-461c-9a3b-b3ef392e29fd"
      },
      "source": [
        "prediction=model.predict(XTRAIN)\n",
        "accuracy = accuracy_score(YTRAIN, prediction.round())\n",
        "precision = precision_score(YTRAIN, prediction.round())\n",
        "recall = recall_score(YTRAIN, prediction.round())\n",
        "f1score = f1_score(YTRAIN, prediction.round())\n",
        "print(\"Training Set Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
        "print(\"Precision: %.2f%%\" % (precision * 100.0))\n",
        "print(\"Recall: %.2f%%\" % (recall * 100.0))\n",
        "print(\"F1-score: %.2f\" % (f1score))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Set Accuracy: 100.00%\n",
            "Precision: 100.00%\n",
            "Recall: 100.00%\n",
            "F1-score: 1.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btLBuSDL-tgj",
        "colab_type": "text"
      },
      "source": [
        "#A neural Network with 5 layers and increased number of neurons:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiNnVQhy-4cB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "38ab6e0e-a0dc-4319-e4c3-c77a8ad548b5"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(132, input_dim = len(XTRAIN[0, :]), activation='sigmoid'))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(40, activation='sigmoid'))\n",
        "model.add(Dense(16, activation='sigmoid'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer = 'rmsprop', metrics=['accuracy'])\n",
        "history = model.fit(XTRAIN, YTRAIN, validation_data=(XVALID, YVALID),epochs = 1056, verbose = 1 )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1056\n",
            "103/103 [==============================] - 0s 3ms/step - loss: 0.4709 - accuracy: 0.8171 - val_loss: 0.4081 - val_accuracy: 0.8371\n",
            "Epoch 2/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.3525 - accuracy: 0.8473 - val_loss: 0.2810 - val_accuracy: 0.8976\n",
            "Epoch 3/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.2590 - accuracy: 0.8894 - val_loss: 0.2074 - val_accuracy: 0.9125\n",
            "Epoch 4/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.1960 - accuracy: 0.9269 - val_loss: 0.1469 - val_accuracy: 0.9459\n",
            "Epoch 5/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.1610 - accuracy: 0.9357 - val_loss: 0.1365 - val_accuracy: 0.9452\n",
            "Epoch 6/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.1437 - accuracy: 0.9433 - val_loss: 0.1089 - val_accuracy: 0.9602\n",
            "Epoch 7/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.1331 - accuracy: 0.9418 - val_loss: 0.0950 - val_accuracy: 0.9687\n",
            "Epoch 8/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.1295 - accuracy: 0.9494 - val_loss: 0.0988 - val_accuracy: 0.9566\n",
            "Epoch 9/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.1268 - accuracy: 0.9479 - val_loss: 0.1007 - val_accuracy: 0.9552\n",
            "Epoch 10/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.1191 - accuracy: 0.9491 - val_loss: 0.1044 - val_accuracy: 0.9552\n",
            "Epoch 11/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.1113 - accuracy: 0.9549 - val_loss: 0.1023 - val_accuracy: 0.9580\n",
            "Epoch 12/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.1136 - accuracy: 0.9515 - val_loss: 0.0696 - val_accuracy: 0.9772\n",
            "Epoch 13/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.1090 - accuracy: 0.9543 - val_loss: 0.0726 - val_accuracy: 0.9744\n",
            "Epoch 14/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.1013 - accuracy: 0.9573 - val_loss: 0.0649 - val_accuracy: 0.9751\n",
            "Epoch 15/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.1008 - accuracy: 0.9564 - val_loss: 0.0574 - val_accuracy: 0.9815\n",
            "Epoch 16/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0969 - accuracy: 0.9610 - val_loss: 0.1051 - val_accuracy: 0.9531\n",
            "Epoch 17/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0889 - accuracy: 0.9659 - val_loss: 0.0751 - val_accuracy: 0.9651\n",
            "Epoch 18/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0827 - accuracy: 0.9649 - val_loss: 0.0930 - val_accuracy: 0.9559\n",
            "Epoch 19/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0809 - accuracy: 0.9677 - val_loss: 0.0725 - val_accuracy: 0.9644\n",
            "Epoch 20/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0766 - accuracy: 0.9723 - val_loss: 0.0570 - val_accuracy: 0.9787\n",
            "Epoch 21/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0744 - accuracy: 0.9701 - val_loss: 0.0420 - val_accuracy: 0.9858\n",
            "Epoch 22/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0692 - accuracy: 0.9738 - val_loss: 0.0696 - val_accuracy: 0.9701\n",
            "Epoch 23/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0648 - accuracy: 0.9732 - val_loss: 0.0374 - val_accuracy: 0.9858\n",
            "Epoch 24/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0647 - accuracy: 0.9784 - val_loss: 0.0397 - val_accuracy: 0.9879\n",
            "Epoch 25/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0614 - accuracy: 0.9750 - val_loss: 0.0392 - val_accuracy: 0.9829\n",
            "Epoch 26/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0574 - accuracy: 0.9811 - val_loss: 0.0286 - val_accuracy: 0.9936\n",
            "Epoch 27/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0541 - accuracy: 0.9802 - val_loss: 0.0318 - val_accuracy: 0.9886\n",
            "Epoch 28/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0539 - accuracy: 0.9756 - val_loss: 0.0365 - val_accuracy: 0.9836\n",
            "Epoch 29/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0481 - accuracy: 0.9814 - val_loss: 0.0712 - val_accuracy: 0.9723\n",
            "Epoch 30/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0503 - accuracy: 0.9799 - val_loss: 0.0298 - val_accuracy: 0.9879\n",
            "Epoch 31/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0482 - accuracy: 0.9820 - val_loss: 0.0442 - val_accuracy: 0.9794\n",
            "Epoch 32/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0451 - accuracy: 0.9799 - val_loss: 0.0223 - val_accuracy: 0.9936\n",
            "Epoch 33/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0420 - accuracy: 0.9838 - val_loss: 0.0310 - val_accuracy: 0.9865\n",
            "Epoch 34/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0457 - accuracy: 0.9820 - val_loss: 0.0487 - val_accuracy: 0.9787\n",
            "Epoch 35/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0401 - accuracy: 0.9860 - val_loss: 0.0195 - val_accuracy: 0.9929\n",
            "Epoch 36/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0392 - accuracy: 0.9854 - val_loss: 0.0234 - val_accuracy: 0.9936\n",
            "Epoch 37/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0374 - accuracy: 0.9857 - val_loss: 0.0220 - val_accuracy: 0.9929\n",
            "Epoch 38/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0388 - accuracy: 0.9845 - val_loss: 0.0228 - val_accuracy: 0.9915\n",
            "Epoch 39/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0343 - accuracy: 0.9863 - val_loss: 0.1072 - val_accuracy: 0.9538\n",
            "Epoch 40/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0420 - accuracy: 0.9832 - val_loss: 0.0273 - val_accuracy: 0.9872\n",
            "Epoch 41/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0364 - accuracy: 0.9857 - val_loss: 0.0263 - val_accuracy: 0.9879\n",
            "Epoch 42/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0331 - accuracy: 0.9869 - val_loss: 0.0205 - val_accuracy: 0.9922\n",
            "Epoch 43/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0338 - accuracy: 0.9851 - val_loss: 0.0336 - val_accuracy: 0.9872\n",
            "Epoch 44/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0320 - accuracy: 0.9896 - val_loss: 0.0249 - val_accuracy: 0.9872\n",
            "Epoch 45/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0308 - accuracy: 0.9887 - val_loss: 0.0275 - val_accuracy: 0.9879\n",
            "Epoch 46/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0347 - accuracy: 0.9854 - val_loss: 0.0830 - val_accuracy: 0.9673\n",
            "Epoch 47/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0341 - accuracy: 0.9835 - val_loss: 0.0372 - val_accuracy: 0.9808\n",
            "Epoch 48/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0334 - accuracy: 0.9857 - val_loss: 0.0190 - val_accuracy: 0.9929\n",
            "Epoch 49/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0322 - accuracy: 0.9866 - val_loss: 0.0217 - val_accuracy: 0.9922\n",
            "Epoch 50/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0324 - accuracy: 0.9875 - val_loss: 0.0393 - val_accuracy: 0.9844\n",
            "Epoch 51/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0291 - accuracy: 0.9869 - val_loss: 0.0172 - val_accuracy: 0.9950\n",
            "Epoch 52/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0322 - accuracy: 0.9869 - val_loss: 0.0210 - val_accuracy: 0.9915\n",
            "Epoch 53/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0326 - accuracy: 0.9872 - val_loss: 0.0170 - val_accuracy: 0.9950\n",
            "Epoch 54/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0303 - accuracy: 0.9884 - val_loss: 0.0445 - val_accuracy: 0.9844\n",
            "Epoch 55/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0275 - accuracy: 0.9887 - val_loss: 0.0222 - val_accuracy: 0.9908\n",
            "Epoch 56/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0237 - accuracy: 0.9918 - val_loss: 0.0543 - val_accuracy: 0.9801\n",
            "Epoch 57/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0299 - accuracy: 0.9872 - val_loss: 0.0169 - val_accuracy: 0.9943\n",
            "Epoch 58/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0288 - accuracy: 0.9887 - val_loss: 0.0137 - val_accuracy: 0.9950\n",
            "Epoch 59/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0280 - accuracy: 0.9887 - val_loss: 0.0235 - val_accuracy: 0.9893\n",
            "Epoch 60/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0303 - accuracy: 0.9881 - val_loss: 0.0179 - val_accuracy: 0.9936\n",
            "Epoch 61/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0280 - accuracy: 0.9875 - val_loss: 0.0165 - val_accuracy: 0.9957\n",
            "Epoch 62/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0277 - accuracy: 0.9884 - val_loss: 0.0203 - val_accuracy: 0.9922\n",
            "Epoch 63/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.9890 - val_loss: 0.0173 - val_accuracy: 0.9922\n",
            "Epoch 64/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0272 - accuracy: 0.9896 - val_loss: 0.0158 - val_accuracy: 0.9950\n",
            "Epoch 65/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0274 - accuracy: 0.9909 - val_loss: 0.0249 - val_accuracy: 0.9893\n",
            "Epoch 66/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0253 - accuracy: 0.9893 - val_loss: 0.0118 - val_accuracy: 0.9979\n",
            "Epoch 67/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0272 - accuracy: 0.9887 - val_loss: 0.0237 - val_accuracy: 0.9908\n",
            "Epoch 68/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0262 - accuracy: 0.9884 - val_loss: 0.0254 - val_accuracy: 0.9915\n",
            "Epoch 69/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0264 - accuracy: 0.9893 - val_loss: 0.0197 - val_accuracy: 0.9900\n",
            "Epoch 70/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0253 - accuracy: 0.9909 - val_loss: 0.0132 - val_accuracy: 0.9964\n",
            "Epoch 71/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0285 - accuracy: 0.9896 - val_loss: 0.0221 - val_accuracy: 0.9893\n",
            "Epoch 72/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0248 - accuracy: 0.9899 - val_loss: 0.0256 - val_accuracy: 0.9893\n",
            "Epoch 73/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0254 - accuracy: 0.9909 - val_loss: 0.0163 - val_accuracy: 0.9957\n",
            "Epoch 74/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0224 - accuracy: 0.9924 - val_loss: 0.0299 - val_accuracy: 0.9879\n",
            "Epoch 75/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0261 - accuracy: 0.9896 - val_loss: 0.0166 - val_accuracy: 0.9943\n",
            "Epoch 76/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0240 - accuracy: 0.9896 - val_loss: 0.0234 - val_accuracy: 0.9908\n",
            "Epoch 77/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0213 - accuracy: 0.9912 - val_loss: 0.0205 - val_accuracy: 0.9908\n",
            "Epoch 78/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0234 - accuracy: 0.9918 - val_loss: 0.0432 - val_accuracy: 0.9836\n",
            "Epoch 79/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0246 - accuracy: 0.9902 - val_loss: 0.0205 - val_accuracy: 0.9915\n",
            "Epoch 80/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0210 - accuracy: 0.9915 - val_loss: 0.0153 - val_accuracy: 0.9943\n",
            "Epoch 81/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0236 - accuracy: 0.9890 - val_loss: 0.0228 - val_accuracy: 0.9922\n",
            "Epoch 82/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0239 - accuracy: 0.9918 - val_loss: 0.0224 - val_accuracy: 0.9915\n",
            "Epoch 83/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0238 - accuracy: 0.9915 - val_loss: 0.0299 - val_accuracy: 0.9879\n",
            "Epoch 84/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0236 - accuracy: 0.9893 - val_loss: 0.0321 - val_accuracy: 0.9872\n",
            "Epoch 85/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0188 - accuracy: 0.9924 - val_loss: 0.0178 - val_accuracy: 0.9957\n",
            "Epoch 86/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0232 - accuracy: 0.9906 - val_loss: 0.0200 - val_accuracy: 0.9922\n",
            "Epoch 87/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0217 - accuracy: 0.9918 - val_loss: 0.0303 - val_accuracy: 0.9865\n",
            "Epoch 88/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0214 - accuracy: 0.9915 - val_loss: 0.0178 - val_accuracy: 0.9936\n",
            "Epoch 89/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0184 - accuracy: 0.9930 - val_loss: 0.0275 - val_accuracy: 0.9893\n",
            "Epoch 90/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0210 - accuracy: 0.9909 - val_loss: 0.0393 - val_accuracy: 0.9844\n",
            "Epoch 91/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0170 - accuracy: 0.9924 - val_loss: 0.0307 - val_accuracy: 0.9879\n",
            "Epoch 92/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0202 - accuracy: 0.9930 - val_loss: 0.0280 - val_accuracy: 0.9922\n",
            "Epoch 93/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0205 - accuracy: 0.9918 - val_loss: 0.0257 - val_accuracy: 0.9915\n",
            "Epoch 94/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0221 - accuracy: 0.9912 - val_loss: 0.0352 - val_accuracy: 0.9886\n",
            "Epoch 95/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0211 - accuracy: 0.9927 - val_loss: 0.0202 - val_accuracy: 0.9929\n",
            "Epoch 96/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0198 - accuracy: 0.9927 - val_loss: 0.0184 - val_accuracy: 0.9936\n",
            "Epoch 97/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0192 - accuracy: 0.9927 - val_loss: 0.0215 - val_accuracy: 0.9908\n",
            "Epoch 98/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0191 - accuracy: 0.9927 - val_loss: 0.0276 - val_accuracy: 0.9886\n",
            "Epoch 99/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0198 - accuracy: 0.9930 - val_loss: 0.0168 - val_accuracy: 0.9950\n",
            "Epoch 100/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0169 - accuracy: 0.9933 - val_loss: 0.0302 - val_accuracy: 0.9879\n",
            "Epoch 101/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0186 - accuracy: 0.9927 - val_loss: 0.0224 - val_accuracy: 0.9922\n",
            "Epoch 102/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0183 - accuracy: 0.9927 - val_loss: 0.0298 - val_accuracy: 0.9893\n",
            "Epoch 103/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0217 - accuracy: 0.9918 - val_loss: 0.0178 - val_accuracy: 0.9950\n",
            "Epoch 104/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0195 - accuracy: 0.9927 - val_loss: 0.0186 - val_accuracy: 0.9964\n",
            "Epoch 105/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0234 - accuracy: 0.9899 - val_loss: 0.0221 - val_accuracy: 0.9922\n",
            "Epoch 106/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0194 - accuracy: 0.9915 - val_loss: 0.0327 - val_accuracy: 0.9893\n",
            "Epoch 107/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0165 - accuracy: 0.9936 - val_loss: 0.0232 - val_accuracy: 0.9908\n",
            "Epoch 108/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0197 - accuracy: 0.9921 - val_loss: 0.0233 - val_accuracy: 0.9922\n",
            "Epoch 109/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0193 - accuracy: 0.9927 - val_loss: 0.0160 - val_accuracy: 0.9943\n",
            "Epoch 110/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0201 - accuracy: 0.9909 - val_loss: 0.0180 - val_accuracy: 0.9943\n",
            "Epoch 111/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0206 - accuracy: 0.9918 - val_loss: 0.0386 - val_accuracy: 0.9872\n",
            "Epoch 112/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0168 - accuracy: 0.9939 - val_loss: 0.0157 - val_accuracy: 0.9943\n",
            "Epoch 113/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0176 - accuracy: 0.9942 - val_loss: 0.0358 - val_accuracy: 0.9851\n",
            "Epoch 114/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0163 - accuracy: 0.9939 - val_loss: 0.0213 - val_accuracy: 0.9936\n",
            "Epoch 115/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0161 - accuracy: 0.9936 - val_loss: 0.0383 - val_accuracy: 0.9858\n",
            "Epoch 116/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0192 - accuracy: 0.9927 - val_loss: 0.0221 - val_accuracy: 0.9936\n",
            "Epoch 117/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0155 - accuracy: 0.9948 - val_loss: 0.0193 - val_accuracy: 0.9922\n",
            "Epoch 118/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0165 - accuracy: 0.9936 - val_loss: 0.0228 - val_accuracy: 0.9936\n",
            "Epoch 119/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0172 - accuracy: 0.9936 - val_loss: 0.0549 - val_accuracy: 0.9808\n",
            "Epoch 120/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0162 - accuracy: 0.9945 - val_loss: 0.0333 - val_accuracy: 0.9893\n",
            "Epoch 121/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0175 - accuracy: 0.9933 - val_loss: 0.0244 - val_accuracy: 0.9915\n",
            "Epoch 122/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0154 - accuracy: 0.9942 - val_loss: 0.0177 - val_accuracy: 0.9915\n",
            "Epoch 123/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0148 - accuracy: 0.9939 - val_loss: 0.0220 - val_accuracy: 0.9929\n",
            "Epoch 124/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0145 - accuracy: 0.9948 - val_loss: 0.0488 - val_accuracy: 0.9844\n",
            "Epoch 125/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0153 - accuracy: 0.9942 - val_loss: 0.0302 - val_accuracy: 0.9915\n",
            "Epoch 126/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0167 - accuracy: 0.9948 - val_loss: 0.0304 - val_accuracy: 0.9908\n",
            "Epoch 127/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0161 - accuracy: 0.9939 - val_loss: 0.0231 - val_accuracy: 0.9936\n",
            "Epoch 128/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0154 - accuracy: 0.9942 - val_loss: 0.0448 - val_accuracy: 0.9829\n",
            "Epoch 129/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0143 - accuracy: 0.9942 - val_loss: 0.0232 - val_accuracy: 0.9922\n",
            "Epoch 130/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0188 - accuracy: 0.9927 - val_loss: 0.0223 - val_accuracy: 0.9922\n",
            "Epoch 131/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0148 - accuracy: 0.9945 - val_loss: 0.0195 - val_accuracy: 0.9950\n",
            "Epoch 132/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0163 - accuracy: 0.9927 - val_loss: 0.0185 - val_accuracy: 0.9957\n",
            "Epoch 133/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0144 - accuracy: 0.9945 - val_loss: 0.0214 - val_accuracy: 0.9929\n",
            "Epoch 134/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0151 - accuracy: 0.9939 - val_loss: 0.0188 - val_accuracy: 0.9950\n",
            "Epoch 135/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0134 - accuracy: 0.9945 - val_loss: 0.0278 - val_accuracy: 0.9922\n",
            "Epoch 136/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0157 - accuracy: 0.9936 - val_loss: 0.0241 - val_accuracy: 0.9936\n",
            "Epoch 137/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0142 - accuracy: 0.9942 - val_loss: 0.0284 - val_accuracy: 0.9908\n",
            "Epoch 138/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0161 - accuracy: 0.9939 - val_loss: 0.0282 - val_accuracy: 0.9929\n",
            "Epoch 139/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0152 - accuracy: 0.9957 - val_loss: 0.0192 - val_accuracy: 0.9950\n",
            "Epoch 140/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0159 - accuracy: 0.9942 - val_loss: 0.0221 - val_accuracy: 0.9936\n",
            "Epoch 141/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0130 - accuracy: 0.9954 - val_loss: 0.0315 - val_accuracy: 0.9908\n",
            "Epoch 142/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0154 - accuracy: 0.9951 - val_loss: 0.0361 - val_accuracy: 0.9900\n",
            "Epoch 143/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0115 - accuracy: 0.9960 - val_loss: 0.0327 - val_accuracy: 0.9893\n",
            "Epoch 144/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0156 - accuracy: 0.9936 - val_loss: 0.0267 - val_accuracy: 0.9908\n",
            "Epoch 145/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0169 - accuracy: 0.9927 - val_loss: 0.0179 - val_accuracy: 0.9964\n",
            "Epoch 146/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0130 - accuracy: 0.9951 - val_loss: 0.0326 - val_accuracy: 0.9908\n",
            "Epoch 147/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0148 - accuracy: 0.9939 - val_loss: 0.0224 - val_accuracy: 0.9915\n",
            "Epoch 148/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0145 - accuracy: 0.9951 - val_loss: 0.0195 - val_accuracy: 0.9943\n",
            "Epoch 149/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0116 - accuracy: 0.9970 - val_loss: 0.0250 - val_accuracy: 0.9922\n",
            "Epoch 150/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0119 - accuracy: 0.9954 - val_loss: 0.0289 - val_accuracy: 0.9922\n",
            "Epoch 151/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0111 - accuracy: 0.9966 - val_loss: 0.0204 - val_accuracy: 0.9964\n",
            "Epoch 152/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0159 - accuracy: 0.9939 - val_loss: 0.0284 - val_accuracy: 0.9922\n",
            "Epoch 153/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0118 - accuracy: 0.9948 - val_loss: 0.0195 - val_accuracy: 0.9957\n",
            "Epoch 154/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0171 - accuracy: 0.9939 - val_loss: 0.0143 - val_accuracy: 0.9957\n",
            "Epoch 155/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0150 - accuracy: 0.9942 - val_loss: 0.0242 - val_accuracy: 0.9922\n",
            "Epoch 156/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0097 - accuracy: 0.9970 - val_loss: 0.0216 - val_accuracy: 0.9950\n",
            "Epoch 157/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0123 - accuracy: 0.9963 - val_loss: 0.0185 - val_accuracy: 0.9950\n",
            "Epoch 158/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0136 - accuracy: 0.9951 - val_loss: 0.0279 - val_accuracy: 0.9922\n",
            "Epoch 159/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0135 - accuracy: 0.9954 - val_loss: 0.0181 - val_accuracy: 0.9950\n",
            "Epoch 160/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0162 - accuracy: 0.9939 - val_loss: 0.0217 - val_accuracy: 0.9950\n",
            "Epoch 161/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0111 - accuracy: 0.9957 - val_loss: 0.0239 - val_accuracy: 0.9943\n",
            "Epoch 162/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0132 - accuracy: 0.9954 - val_loss: 0.0337 - val_accuracy: 0.9908\n",
            "Epoch 163/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0141 - accuracy: 0.9942 - val_loss: 0.0190 - val_accuracy: 0.9943\n",
            "Epoch 164/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0121 - accuracy: 0.9960 - val_loss: 0.0280 - val_accuracy: 0.9943\n",
            "Epoch 165/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0155 - accuracy: 0.9951 - val_loss: 0.0346 - val_accuracy: 0.9900\n",
            "Epoch 166/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0101 - accuracy: 0.9963 - val_loss: 0.0216 - val_accuracy: 0.9957\n",
            "Epoch 167/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0119 - accuracy: 0.9960 - val_loss: 0.0284 - val_accuracy: 0.9929\n",
            "Epoch 168/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0127 - accuracy: 0.9957 - val_loss: 0.0259 - val_accuracy: 0.9936\n",
            "Epoch 169/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0109 - accuracy: 0.9970 - val_loss: 0.0557 - val_accuracy: 0.9865\n",
            "Epoch 170/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0140 - accuracy: 0.9945 - val_loss: 0.0262 - val_accuracy: 0.9943\n",
            "Epoch 171/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0136 - accuracy: 0.9966 - val_loss: 0.0216 - val_accuracy: 0.9943\n",
            "Epoch 172/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0128 - accuracy: 0.9966 - val_loss: 0.0198 - val_accuracy: 0.9964\n",
            "Epoch 173/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0104 - accuracy: 0.9960 - val_loss: 0.0350 - val_accuracy: 0.9915\n",
            "Epoch 174/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0111 - accuracy: 0.9957 - val_loss: 0.0264 - val_accuracy: 0.9936\n",
            "Epoch 175/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0094 - accuracy: 0.9966 - val_loss: 0.0281 - val_accuracy: 0.9929\n",
            "Epoch 176/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0129 - accuracy: 0.9954 - val_loss: 0.0333 - val_accuracy: 0.9900\n",
            "Epoch 177/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0127 - accuracy: 0.9954 - val_loss: 0.0265 - val_accuracy: 0.9936\n",
            "Epoch 178/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0100 - accuracy: 0.9970 - val_loss: 0.0304 - val_accuracy: 0.9915\n",
            "Epoch 179/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0102 - accuracy: 0.9966 - val_loss: 0.0251 - val_accuracy: 0.9943\n",
            "Epoch 180/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0111 - accuracy: 0.9963 - val_loss: 0.0194 - val_accuracy: 0.9943\n",
            "Epoch 181/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0098 - accuracy: 0.9957 - val_loss: 0.0249 - val_accuracy: 0.9950\n",
            "Epoch 182/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0078 - accuracy: 0.9979 - val_loss: 0.0538 - val_accuracy: 0.9879\n",
            "Epoch 183/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0104 - accuracy: 0.9970 - val_loss: 0.0285 - val_accuracy: 0.9929\n",
            "Epoch 184/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0084 - accuracy: 0.9966 - val_loss: 0.0529 - val_accuracy: 0.9886\n",
            "Epoch 185/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0089 - accuracy: 0.9976 - val_loss: 0.0222 - val_accuracy: 0.9964\n",
            "Epoch 186/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0095 - accuracy: 0.9970 - val_loss: 0.0315 - val_accuracy: 0.9922\n",
            "Epoch 187/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0103 - accuracy: 0.9963 - val_loss: 0.0448 - val_accuracy: 0.9886\n",
            "Epoch 188/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0091 - accuracy: 0.9966 - val_loss: 0.0333 - val_accuracy: 0.9915\n",
            "Epoch 189/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0095 - accuracy: 0.9970 - val_loss: 0.0335 - val_accuracy: 0.9915\n",
            "Epoch 190/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0089 - accuracy: 0.9973 - val_loss: 0.0266 - val_accuracy: 0.9929\n",
            "Epoch 191/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0123 - accuracy: 0.9963 - val_loss: 0.0271 - val_accuracy: 0.9936\n",
            "Epoch 192/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0097 - accuracy: 0.9970 - val_loss: 0.0443 - val_accuracy: 0.9893\n",
            "Epoch 193/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0093 - accuracy: 0.9973 - val_loss: 0.0303 - val_accuracy: 0.9936\n",
            "Epoch 194/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0097 - accuracy: 0.9954 - val_loss: 0.0290 - val_accuracy: 0.9943\n",
            "Epoch 195/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0089 - accuracy: 0.9979 - val_loss: 0.0240 - val_accuracy: 0.9957\n",
            "Epoch 196/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0088 - accuracy: 0.9979 - val_loss: 0.0298 - val_accuracy: 0.9936\n",
            "Epoch 197/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0105 - accuracy: 0.9951 - val_loss: 0.0263 - val_accuracy: 0.9943\n",
            "Epoch 198/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0071 - accuracy: 0.9976 - val_loss: 0.0300 - val_accuracy: 0.9936\n",
            "Epoch 199/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0122 - accuracy: 0.9957 - val_loss: 0.0256 - val_accuracy: 0.9943\n",
            "Epoch 200/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0132 - accuracy: 0.9957 - val_loss: 0.0302 - val_accuracy: 0.9929\n",
            "Epoch 201/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0107 - accuracy: 0.9976 - val_loss: 0.0286 - val_accuracy: 0.9936\n",
            "Epoch 202/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0115 - accuracy: 0.9966 - val_loss: 0.0352 - val_accuracy: 0.9929\n",
            "Epoch 203/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0078 - accuracy: 0.9976 - val_loss: 0.0383 - val_accuracy: 0.9900\n",
            "Epoch 204/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0084 - accuracy: 0.9976 - val_loss: 0.0275 - val_accuracy: 0.9943\n",
            "Epoch 205/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0128 - accuracy: 0.9973 - val_loss: 0.0306 - val_accuracy: 0.9929\n",
            "Epoch 206/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0075 - accuracy: 0.9982 - val_loss: 0.0366 - val_accuracy: 0.9893\n",
            "Epoch 207/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0102 - accuracy: 0.9966 - val_loss: 0.0223 - val_accuracy: 0.9950\n",
            "Epoch 208/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0074 - accuracy: 0.9976 - val_loss: 0.0300 - val_accuracy: 0.9922\n",
            "Epoch 209/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0115 - accuracy: 0.9976 - val_loss: 0.0264 - val_accuracy: 0.9943\n",
            "Epoch 210/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0082 - accuracy: 0.9976 - val_loss: 0.0369 - val_accuracy: 0.9908\n",
            "Epoch 211/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0125 - accuracy: 0.9970 - val_loss: 0.0314 - val_accuracy: 0.9929\n",
            "Epoch 212/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0077 - accuracy: 0.9982 - val_loss: 0.0438 - val_accuracy: 0.9908\n",
            "Epoch 213/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0109 - accuracy: 0.9970 - val_loss: 0.0303 - val_accuracy: 0.9936\n",
            "Epoch 214/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0097 - accuracy: 0.9970 - val_loss: 0.0263 - val_accuracy: 0.9936\n",
            "Epoch 215/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0071 - accuracy: 0.9973 - val_loss: 0.0334 - val_accuracy: 0.9915\n",
            "Epoch 216/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0095 - accuracy: 0.9973 - val_loss: 0.0346 - val_accuracy: 0.9929\n",
            "Epoch 217/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0102 - accuracy: 0.9973 - val_loss: 0.0266 - val_accuracy: 0.9950\n",
            "Epoch 218/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0111 - accuracy: 0.9960 - val_loss: 0.0323 - val_accuracy: 0.9929\n",
            "Epoch 219/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0069 - accuracy: 0.9985 - val_loss: 0.0436 - val_accuracy: 0.9922\n",
            "Epoch 220/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0120 - accuracy: 0.9960 - val_loss: 0.0410 - val_accuracy: 0.9915\n",
            "Epoch 221/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0072 - accuracy: 0.9982 - val_loss: 0.0327 - val_accuracy: 0.9936\n",
            "Epoch 222/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0083 - accuracy: 0.9966 - val_loss: 0.0333 - val_accuracy: 0.9922\n",
            "Epoch 223/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0071 - accuracy: 0.9982 - val_loss: 0.0342 - val_accuracy: 0.9929\n",
            "Epoch 224/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0073 - accuracy: 0.9976 - val_loss: 0.0325 - val_accuracy: 0.9936\n",
            "Epoch 225/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0089 - accuracy: 0.9970 - val_loss: 0.0381 - val_accuracy: 0.9915\n",
            "Epoch 226/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0064 - accuracy: 0.9979 - val_loss: 0.0396 - val_accuracy: 0.9915\n",
            "Epoch 227/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0077 - accuracy: 0.9976 - val_loss: 0.0354 - val_accuracy: 0.9943\n",
            "Epoch 228/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0078 - accuracy: 0.9976 - val_loss: 0.0354 - val_accuracy: 0.9915\n",
            "Epoch 229/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0109 - accuracy: 0.9960 - val_loss: 0.0311 - val_accuracy: 0.9936\n",
            "Epoch 230/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0087 - accuracy: 0.9973 - val_loss: 0.0250 - val_accuracy: 0.9957\n",
            "Epoch 231/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0055 - accuracy: 0.9976 - val_loss: 0.0275 - val_accuracy: 0.9957\n",
            "Epoch 232/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0092 - accuracy: 0.9973 - val_loss: 0.0349 - val_accuracy: 0.9936\n",
            "Epoch 233/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0077 - accuracy: 0.9973 - val_loss: 0.0289 - val_accuracy: 0.9936\n",
            "Epoch 234/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0066 - accuracy: 0.9985 - val_loss: 0.0451 - val_accuracy: 0.9922\n",
            "Epoch 235/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0069 - accuracy: 0.9976 - val_loss: 0.0380 - val_accuracy: 0.9915\n",
            "Epoch 236/1056\n",
            "103/103 [==============================] - 0s 5ms/step - loss: 0.0068 - accuracy: 0.9982 - val_loss: 0.0254 - val_accuracy: 0.9943\n",
            "Epoch 237/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0101 - accuracy: 0.9973 - val_loss: 0.0274 - val_accuracy: 0.9943\n",
            "Epoch 238/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0074 - accuracy: 0.9979 - val_loss: 0.0343 - val_accuracy: 0.9922\n",
            "Epoch 239/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0096 - accuracy: 0.9970 - val_loss: 0.0229 - val_accuracy: 0.9964\n",
            "Epoch 240/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0060 - accuracy: 0.9976 - val_loss: 0.0332 - val_accuracy: 0.9929\n",
            "Epoch 241/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0088 - accuracy: 0.9970 - val_loss: 0.0309 - val_accuracy: 0.9936\n",
            "Epoch 242/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0080 - accuracy: 0.9976 - val_loss: 0.0316 - val_accuracy: 0.9943\n",
            "Epoch 243/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0071 - accuracy: 0.9982 - val_loss: 0.0264 - val_accuracy: 0.9950\n",
            "Epoch 244/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0068 - accuracy: 0.9976 - val_loss: 0.0276 - val_accuracy: 0.9936\n",
            "Epoch 245/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0068 - accuracy: 0.9976 - val_loss: 0.0292 - val_accuracy: 0.9943\n",
            "Epoch 246/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0093 - accuracy: 0.9979 - val_loss: 0.0358 - val_accuracy: 0.9929\n",
            "Epoch 247/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0047 - accuracy: 0.9982 - val_loss: 0.0356 - val_accuracy: 0.9922\n",
            "Epoch 248/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0101 - accuracy: 0.9970 - val_loss: 0.0324 - val_accuracy: 0.9943\n",
            "Epoch 249/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0045 - accuracy: 0.9988 - val_loss: 0.0397 - val_accuracy: 0.9936\n",
            "Epoch 250/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0080 - accuracy: 0.9973 - val_loss: 0.0419 - val_accuracy: 0.9908\n",
            "Epoch 251/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0065 - accuracy: 0.9976 - val_loss: 0.0390 - val_accuracy: 0.9922\n",
            "Epoch 252/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0048 - accuracy: 0.9994 - val_loss: 0.0247 - val_accuracy: 0.9964\n",
            "Epoch 253/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0080 - accuracy: 0.9979 - val_loss: 0.0386 - val_accuracy: 0.9936\n",
            "Epoch 254/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0100 - accuracy: 0.9976 - val_loss: 0.0212 - val_accuracy: 0.9972\n",
            "Epoch 255/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0036 - accuracy: 0.9994 - val_loss: 0.0759 - val_accuracy: 0.9851\n",
            "Epoch 256/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0039 - accuracy: 0.9991 - val_loss: 0.0328 - val_accuracy: 0.9936\n",
            "Epoch 257/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0081 - accuracy: 0.9985 - val_loss: 0.0311 - val_accuracy: 0.9943\n",
            "Epoch 258/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0062 - accuracy: 0.9988 - val_loss: 0.0339 - val_accuracy: 0.9922\n",
            "Epoch 259/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0073 - accuracy: 0.9982 - val_loss: 0.0341 - val_accuracy: 0.9943\n",
            "Epoch 260/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0094 - accuracy: 0.9979 - val_loss: 0.0300 - val_accuracy: 0.9936\n",
            "Epoch 261/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0037 - accuracy: 0.9988 - val_loss: 0.0423 - val_accuracy: 0.9915\n",
            "Epoch 262/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0048 - accuracy: 0.9982 - val_loss: 0.0265 - val_accuracy: 0.9964\n",
            "Epoch 263/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0073 - accuracy: 0.9976 - val_loss: 0.0322 - val_accuracy: 0.9943\n",
            "Epoch 264/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0043 - accuracy: 0.9988 - val_loss: 0.0545 - val_accuracy: 0.9908\n",
            "Epoch 265/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0053 - accuracy: 0.9988 - val_loss: 0.0295 - val_accuracy: 0.9936\n",
            "Epoch 266/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0048 - accuracy: 0.9985 - val_loss: 0.0327 - val_accuracy: 0.9950\n",
            "Epoch 267/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0085 - accuracy: 0.9976 - val_loss: 0.0362 - val_accuracy: 0.9950\n",
            "Epoch 268/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0077 - accuracy: 0.9979 - val_loss: 0.0415 - val_accuracy: 0.9929\n",
            "Epoch 269/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0038 - accuracy: 0.9994 - val_loss: 0.0391 - val_accuracy: 0.9929\n",
            "Epoch 270/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0093 - accuracy: 0.9976 - val_loss: 0.0381 - val_accuracy: 0.9929\n",
            "Epoch 271/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0063 - accuracy: 0.9985 - val_loss: 0.0383 - val_accuracy: 0.9922\n",
            "Epoch 272/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0050 - accuracy: 0.9976 - val_loss: 0.0259 - val_accuracy: 0.9950\n",
            "Epoch 273/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0073 - accuracy: 0.9970 - val_loss: 0.0387 - val_accuracy: 0.9936\n",
            "Epoch 274/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0065 - accuracy: 0.9979 - val_loss: 0.0406 - val_accuracy: 0.9929\n",
            "Epoch 275/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0094 - accuracy: 0.9966 - val_loss: 0.0402 - val_accuracy: 0.9922\n",
            "Epoch 276/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0073 - accuracy: 0.9976 - val_loss: 0.0318 - val_accuracy: 0.9943\n",
            "Epoch 277/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0043 - accuracy: 0.9991 - val_loss: 0.0560 - val_accuracy: 0.9900\n",
            "Epoch 278/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0093 - accuracy: 0.9982 - val_loss: 0.0338 - val_accuracy: 0.9943\n",
            "Epoch 279/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0044 - accuracy: 0.9994 - val_loss: 0.0442 - val_accuracy: 0.9915\n",
            "Epoch 280/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0075 - accuracy: 0.9979 - val_loss: 0.0356 - val_accuracy: 0.9943\n",
            "Epoch 281/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0063 - accuracy: 0.9982 - val_loss: 0.0315 - val_accuracy: 0.9957\n",
            "Epoch 282/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0080 - accuracy: 0.9979 - val_loss: 0.0415 - val_accuracy: 0.9929\n",
            "Epoch 283/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0049 - accuracy: 0.9982 - val_loss: 0.0318 - val_accuracy: 0.9936\n",
            "Epoch 284/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0058 - accuracy: 0.9985 - val_loss: 0.0377 - val_accuracy: 0.9943\n",
            "Epoch 285/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0063 - accuracy: 0.9988 - val_loss: 0.0390 - val_accuracy: 0.9929\n",
            "Epoch 286/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0064 - accuracy: 0.9985 - val_loss: 0.0290 - val_accuracy: 0.9950\n",
            "Epoch 287/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0058 - accuracy: 0.9991 - val_loss: 0.0375 - val_accuracy: 0.9943\n",
            "Epoch 288/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0060 - accuracy: 0.9982 - val_loss: 0.0283 - val_accuracy: 0.9936\n",
            "Epoch 289/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0067 - accuracy: 0.9985 - val_loss: 0.0466 - val_accuracy: 0.9915\n",
            "Epoch 290/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0054 - accuracy: 0.9982 - val_loss: 0.0507 - val_accuracy: 0.9915\n",
            "Epoch 291/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0048 - accuracy: 0.9985 - val_loss: 0.0397 - val_accuracy: 0.9922\n",
            "Epoch 292/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0044 - accuracy: 0.9982 - val_loss: 0.0434 - val_accuracy: 0.9936\n",
            "Epoch 293/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0059 - accuracy: 0.9979 - val_loss: 0.0462 - val_accuracy: 0.9915\n",
            "Epoch 294/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0083 - accuracy: 0.9982 - val_loss: 0.0375 - val_accuracy: 0.9922\n",
            "Epoch 295/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0044 - accuracy: 0.9985 - val_loss: 0.0418 - val_accuracy: 0.9929\n",
            "Epoch 296/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.0388 - val_accuracy: 0.9915\n",
            "Epoch 297/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0073 - accuracy: 0.9988 - val_loss: 0.0478 - val_accuracy: 0.9929\n",
            "Epoch 298/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0029 - accuracy: 0.9997 - val_loss: 0.0415 - val_accuracy: 0.9943\n",
            "Epoch 299/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0054 - accuracy: 0.9985 - val_loss: 0.0360 - val_accuracy: 0.9943\n",
            "Epoch 300/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0055 - accuracy: 0.9988 - val_loss: 0.0440 - val_accuracy: 0.9943\n",
            "Epoch 301/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0054 - accuracy: 0.9985 - val_loss: 0.0412 - val_accuracy: 0.9929\n",
            "Epoch 302/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0058 - accuracy: 0.9988 - val_loss: 0.0396 - val_accuracy: 0.9943\n",
            "Epoch 303/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0056 - accuracy: 0.9988 - val_loss: 0.0504 - val_accuracy: 0.9936\n",
            "Epoch 304/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0062 - accuracy: 0.9982 - val_loss: 0.0449 - val_accuracy: 0.9929\n",
            "Epoch 305/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0033 - accuracy: 0.9994 - val_loss: 0.0381 - val_accuracy: 0.9950\n",
            "Epoch 306/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0081 - accuracy: 0.9985 - val_loss: 0.0333 - val_accuracy: 0.9950\n",
            "Epoch 307/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0043 - accuracy: 0.9988 - val_loss: 0.0480 - val_accuracy: 0.9915\n",
            "Epoch 308/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0089 - accuracy: 0.9973 - val_loss: 0.0494 - val_accuracy: 0.9929\n",
            "Epoch 309/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0058 - accuracy: 0.9979 - val_loss: 0.0322 - val_accuracy: 0.9943\n",
            "Epoch 310/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0069 - accuracy: 0.9979 - val_loss: 0.0345 - val_accuracy: 0.9943\n",
            "Epoch 311/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0019 - accuracy: 0.9994 - val_loss: 0.0324 - val_accuracy: 0.9943\n",
            "Epoch 312/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0066 - accuracy: 0.9988 - val_loss: 0.0382 - val_accuracy: 0.9929\n",
            "Epoch 313/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0048 - accuracy: 0.9988 - val_loss: 0.0492 - val_accuracy: 0.9922\n",
            "Epoch 314/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0045 - accuracy: 0.9988 - val_loss: 0.0626 - val_accuracy: 0.9908\n",
            "Epoch 315/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0021 - accuracy: 0.9994 - val_loss: 0.0723 - val_accuracy: 0.9886\n",
            "Epoch 316/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0076 - accuracy: 0.9979 - val_loss: 0.0534 - val_accuracy: 0.9922\n",
            "Epoch 317/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0049 - accuracy: 0.9988 - val_loss: 0.0469 - val_accuracy: 0.9929\n",
            "Epoch 318/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0030 - accuracy: 0.9991 - val_loss: 0.0346 - val_accuracy: 0.9943\n",
            "Epoch 319/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0045 - accuracy: 0.9988 - val_loss: 0.0396 - val_accuracy: 0.9922\n",
            "Epoch 320/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0026 - accuracy: 0.9997 - val_loss: 0.0366 - val_accuracy: 0.9943\n",
            "Epoch 321/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0045 - accuracy: 0.9982 - val_loss: 0.0423 - val_accuracy: 0.9943\n",
            "Epoch 322/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0073 - accuracy: 0.9985 - val_loss: 0.0437 - val_accuracy: 0.9943\n",
            "Epoch 323/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0039 - accuracy: 0.9991 - val_loss: 0.0471 - val_accuracy: 0.9936\n",
            "Epoch 324/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0057 - accuracy: 0.9985 - val_loss: 0.0460 - val_accuracy: 0.9922\n",
            "Epoch 325/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0033 - accuracy: 0.9988 - val_loss: 0.0414 - val_accuracy: 0.9929\n",
            "Epoch 326/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0053 - accuracy: 0.9979 - val_loss: 0.0474 - val_accuracy: 0.9936\n",
            "Epoch 327/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0048 - accuracy: 0.9985 - val_loss: 0.0370 - val_accuracy: 0.9950\n",
            "Epoch 328/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0045 - accuracy: 0.9991 - val_loss: 0.0420 - val_accuracy: 0.9929\n",
            "Epoch 329/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0042 - accuracy: 0.9979 - val_loss: 0.0454 - val_accuracy: 0.9908\n",
            "Epoch 330/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0067 - accuracy: 0.9985 - val_loss: 0.0495 - val_accuracy: 0.9915\n",
            "Epoch 331/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0055 - accuracy: 0.9988 - val_loss: 0.0448 - val_accuracy: 0.9929\n",
            "Epoch 332/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0046 - accuracy: 0.9985 - val_loss: 0.0408 - val_accuracy: 0.9929\n",
            "Epoch 333/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0049 - accuracy: 0.9985 - val_loss: 0.0603 - val_accuracy: 0.9900\n",
            "Epoch 334/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0039 - accuracy: 0.9985 - val_loss: 0.0488 - val_accuracy: 0.9929\n",
            "Epoch 335/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 0.0471 - val_accuracy: 0.9929\n",
            "Epoch 336/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0064 - accuracy: 0.9979 - val_loss: 0.0549 - val_accuracy: 0.9922\n",
            "Epoch 337/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0024 - accuracy: 0.9988 - val_loss: 0.0445 - val_accuracy: 0.9929\n",
            "Epoch 338/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0027 - accuracy: 0.9988 - val_loss: 0.0537 - val_accuracy: 0.9936\n",
            "Epoch 339/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0043 - accuracy: 0.9988 - val_loss: 0.0539 - val_accuracy: 0.9922\n",
            "Epoch 340/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0039 - accuracy: 0.9991 - val_loss: 0.0538 - val_accuracy: 0.9900\n",
            "Epoch 341/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0087 - accuracy: 0.9973 - val_loss: 0.0693 - val_accuracy: 0.9900\n",
            "Epoch 342/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0031 - accuracy: 0.9991 - val_loss: 0.0457 - val_accuracy: 0.9922\n",
            "Epoch 343/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0039 - accuracy: 0.9985 - val_loss: 0.0604 - val_accuracy: 0.9922\n",
            "Epoch 344/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0063 - accuracy: 0.9985 - val_loss: 0.0590 - val_accuracy: 0.9922\n",
            "Epoch 345/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0049 - accuracy: 0.9985 - val_loss: 0.0578 - val_accuracy: 0.9908\n",
            "Epoch 346/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0019 - accuracy: 0.9994 - val_loss: 0.0547 - val_accuracy: 0.9922\n",
            "Epoch 347/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0044 - accuracy: 0.9991 - val_loss: 0.0474 - val_accuracy: 0.9936\n",
            "Epoch 348/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0020 - accuracy: 0.9988 - val_loss: 0.0460 - val_accuracy: 0.9922\n",
            "Epoch 349/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0049 - accuracy: 0.9988 - val_loss: 0.0504 - val_accuracy: 0.9900\n",
            "Epoch 350/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0077 - accuracy: 0.9979 - val_loss: 0.0561 - val_accuracy: 0.9922\n",
            "Epoch 351/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0076 - accuracy: 0.9976 - val_loss: 0.0664 - val_accuracy: 0.9886\n",
            "Epoch 352/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0038 - accuracy: 0.9988 - val_loss: 0.0453 - val_accuracy: 0.9936\n",
            "Epoch 353/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0018 - accuracy: 0.9991 - val_loss: 0.0616 - val_accuracy: 0.9900\n",
            "Epoch 354/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0060 - accuracy: 0.9982 - val_loss: 0.0441 - val_accuracy: 0.9929\n",
            "Epoch 355/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0020 - accuracy: 0.9994 - val_loss: 0.0463 - val_accuracy: 0.9929\n",
            "Epoch 356/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0068 - accuracy: 0.9982 - val_loss: 0.0438 - val_accuracy: 0.9929\n",
            "Epoch 357/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0048 - accuracy: 0.9991 - val_loss: 0.0353 - val_accuracy: 0.9950\n",
            "Epoch 358/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0058 - accuracy: 0.9988 - val_loss: 0.0435 - val_accuracy: 0.9936\n",
            "Epoch 359/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0040 - accuracy: 0.9991 - val_loss: 0.0426 - val_accuracy: 0.9929\n",
            "Epoch 360/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0097 - accuracy: 0.9982 - val_loss: 0.0488 - val_accuracy: 0.9929\n",
            "Epoch 361/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.6151e-04 - accuracy: 1.0000 - val_loss: 0.0498 - val_accuracy: 0.9936\n",
            "Epoch 362/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0019 - accuracy: 0.9994 - val_loss: 0.0420 - val_accuracy: 0.9922\n",
            "Epoch 363/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0025 - accuracy: 0.9994 - val_loss: 0.0510 - val_accuracy: 0.9929\n",
            "Epoch 364/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0063 - accuracy: 0.9982 - val_loss: 0.0451 - val_accuracy: 0.9936\n",
            "Epoch 365/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0033 - accuracy: 0.9994 - val_loss: 0.0485 - val_accuracy: 0.9943\n",
            "Epoch 366/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0040 - accuracy: 0.9991 - val_loss: 0.0433 - val_accuracy: 0.9936\n",
            "Epoch 367/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0082 - accuracy: 0.9988 - val_loss: 0.0519 - val_accuracy: 0.9929\n",
            "Epoch 368/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0129 - accuracy: 0.9966 - val_loss: 0.0529 - val_accuracy: 0.9922\n",
            "Epoch 369/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0042 - accuracy: 0.9985 - val_loss: 0.0553 - val_accuracy: 0.9915\n",
            "Epoch 370/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0018 - accuracy: 0.9994 - val_loss: 0.0463 - val_accuracy: 0.9929\n",
            "Epoch 371/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0072 - accuracy: 0.9979 - val_loss: 0.0439 - val_accuracy: 0.9943\n",
            "Epoch 372/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0033 - accuracy: 0.9994 - val_loss: 0.0469 - val_accuracy: 0.9936\n",
            "Epoch 373/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0048 - accuracy: 0.9994 - val_loss: 0.0499 - val_accuracy: 0.9915\n",
            "Epoch 374/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0059 - accuracy: 0.9985 - val_loss: 0.0450 - val_accuracy: 0.9936\n",
            "Epoch 375/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0044 - accuracy: 0.9991 - val_loss: 0.0416 - val_accuracy: 0.9936\n",
            "Epoch 376/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.9614e-04 - accuracy: 0.9997 - val_loss: 0.0348 - val_accuracy: 0.9950\n",
            "Epoch 377/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0041 - accuracy: 0.9991 - val_loss: 0.0519 - val_accuracy: 0.9936\n",
            "Epoch 378/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0069 - accuracy: 0.9988 - val_loss: 0.0447 - val_accuracy: 0.9936\n",
            "Epoch 379/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0023 - accuracy: 0.9997 - val_loss: 0.0526 - val_accuracy: 0.9915\n",
            "Epoch 380/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0056 - accuracy: 0.9991 - val_loss: 0.0520 - val_accuracy: 0.9922\n",
            "Epoch 381/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0035 - accuracy: 0.9985 - val_loss: 0.0528 - val_accuracy: 0.9908\n",
            "Epoch 382/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0027 - accuracy: 0.9991 - val_loss: 0.0516 - val_accuracy: 0.9943\n",
            "Epoch 383/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0056 - accuracy: 0.9994 - val_loss: 0.0422 - val_accuracy: 0.9936\n",
            "Epoch 384/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0044 - accuracy: 0.9985 - val_loss: 0.0559 - val_accuracy: 0.9908\n",
            "Epoch 385/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0017 - accuracy: 0.9994 - val_loss: 0.0512 - val_accuracy: 0.9929\n",
            "Epoch 386/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.3054e-04 - accuracy: 0.9997 - val_loss: 0.0459 - val_accuracy: 0.9936\n",
            "Epoch 387/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0031 - accuracy: 0.9988 - val_loss: 0.0528 - val_accuracy: 0.9929\n",
            "Epoch 388/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.4617e-04 - accuracy: 0.9997 - val_loss: 0.0546 - val_accuracy: 0.9929\n",
            "Epoch 389/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0055 - accuracy: 0.9988 - val_loss: 0.0414 - val_accuracy: 0.9929\n",
            "Epoch 390/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0068 - accuracy: 0.9985 - val_loss: 0.0424 - val_accuracy: 0.9943\n",
            "Epoch 391/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0016 - accuracy: 0.9994 - val_loss: 0.0472 - val_accuracy: 0.9936\n",
            "Epoch 392/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0011 - accuracy: 0.9994 - val_loss: 0.0465 - val_accuracy: 0.9936\n",
            "Epoch 393/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0042 - accuracy: 0.9988 - val_loss: 0.0388 - val_accuracy: 0.9936\n",
            "Epoch 394/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0020 - accuracy: 0.9991 - val_loss: 0.0507 - val_accuracy: 0.9936\n",
            "Epoch 395/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0040 - accuracy: 0.9994 - val_loss: 0.0448 - val_accuracy: 0.9929\n",
            "Epoch 396/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0046 - accuracy: 0.9988 - val_loss: 0.0427 - val_accuracy: 0.9943\n",
            "Epoch 397/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0054 - accuracy: 0.9988 - val_loss: 0.0511 - val_accuracy: 0.9936\n",
            "Epoch 398/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0044 - accuracy: 0.9991 - val_loss: 0.0431 - val_accuracy: 0.9943\n",
            "Epoch 399/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0010 - accuracy: 0.9991 - val_loss: 0.0521 - val_accuracy: 0.9936\n",
            "Epoch 400/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.0619 - val_accuracy: 0.9922\n",
            "Epoch 401/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0031 - accuracy: 0.9988 - val_loss: 0.0533 - val_accuracy: 0.9929\n",
            "Epoch 402/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0075 - accuracy: 0.9985 - val_loss: 0.0451 - val_accuracy: 0.9936\n",
            "Epoch 403/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0020 - accuracy: 0.9997 - val_loss: 0.0482 - val_accuracy: 0.9936\n",
            "Epoch 404/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0038 - accuracy: 0.9985 - val_loss: 0.0628 - val_accuracy: 0.9915\n",
            "Epoch 405/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0084 - accuracy: 0.9982 - val_loss: 0.0480 - val_accuracy: 0.9922\n",
            "Epoch 406/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0034 - accuracy: 0.9997 - val_loss: 0.0574 - val_accuracy: 0.9929\n",
            "Epoch 407/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0023 - accuracy: 0.9994 - val_loss: 0.0475 - val_accuracy: 0.9915\n",
            "Epoch 408/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.7117e-04 - accuracy: 0.9997 - val_loss: 0.0820 - val_accuracy: 0.9915\n",
            "Epoch 409/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0050 - accuracy: 0.9985 - val_loss: 0.0544 - val_accuracy: 0.9922\n",
            "Epoch 410/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0051 - accuracy: 0.9988 - val_loss: 0.0713 - val_accuracy: 0.9915\n",
            "Epoch 411/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0024 - accuracy: 0.9997 - val_loss: 0.0630 - val_accuracy: 0.9929\n",
            "Epoch 412/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0013 - accuracy: 0.9994 - val_loss: 0.0603 - val_accuracy: 0.9936\n",
            "Epoch 413/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 0.0625 - val_accuracy: 0.9922\n",
            "Epoch 414/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0063 - accuracy: 0.9985 - val_loss: 0.0878 - val_accuracy: 0.9893\n",
            "Epoch 415/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0058 - accuracy: 0.9988 - val_loss: 0.0588 - val_accuracy: 0.9922\n",
            "Epoch 416/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0057 - accuracy: 0.9976 - val_loss: 0.0572 - val_accuracy: 0.9929\n",
            "Epoch 417/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.5992e-05 - accuracy: 1.0000 - val_loss: 0.0581 - val_accuracy: 0.9915\n",
            "Epoch 418/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0051 - accuracy: 0.9985 - val_loss: 0.0478 - val_accuracy: 0.9929\n",
            "Epoch 419/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.8089e-05 - accuracy: 1.0000 - val_loss: 0.0525 - val_accuracy: 0.9936\n",
            "Epoch 420/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0034 - accuracy: 0.9988 - val_loss: 0.0569 - val_accuracy: 0.9922\n",
            "Epoch 421/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0081 - accuracy: 0.9991 - val_loss: 0.0577 - val_accuracy: 0.9922\n",
            "Epoch 422/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0042 - accuracy: 0.9994 - val_loss: 0.0556 - val_accuracy: 0.9929\n",
            "Epoch 423/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 0.0758 - val_accuracy: 0.9900\n",
            "Epoch 424/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0031 - accuracy: 0.9991 - val_loss: 0.0659 - val_accuracy: 0.9900\n",
            "Epoch 425/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0029 - accuracy: 0.9988 - val_loss: 0.0605 - val_accuracy: 0.9922\n",
            "Epoch 426/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0040 - accuracy: 0.9994 - val_loss: 0.0513 - val_accuracy: 0.9922\n",
            "Epoch 427/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0017 - accuracy: 0.9988 - val_loss: 0.0587 - val_accuracy: 0.9915\n",
            "Epoch 428/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0023 - accuracy: 0.9997 - val_loss: 0.0636 - val_accuracy: 0.9922\n",
            "Epoch 429/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0037 - accuracy: 0.9991 - val_loss: 0.0681 - val_accuracy: 0.9915\n",
            "Epoch 430/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0061 - accuracy: 0.9991 - val_loss: 0.0646 - val_accuracy: 0.9922\n",
            "Epoch 431/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0035 - accuracy: 0.9994 - val_loss: 0.0704 - val_accuracy: 0.9915\n",
            "Epoch 432/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0049 - accuracy: 0.9982 - val_loss: 0.0671 - val_accuracy: 0.9900\n",
            "Epoch 433/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0066 - accuracy: 0.9985 - val_loss: 0.0611 - val_accuracy: 0.9929\n",
            "Epoch 434/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0060 - accuracy: 0.9982 - val_loss: 0.0630 - val_accuracy: 0.9929\n",
            "Epoch 435/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0044 - accuracy: 0.9991 - val_loss: 0.0505 - val_accuracy: 0.9929\n",
            "Epoch 436/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.4325e-05 - accuracy: 1.0000 - val_loss: 0.0507 - val_accuracy: 0.9929\n",
            "Epoch 437/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0058 - accuracy: 0.9991 - val_loss: 0.0573 - val_accuracy: 0.9922\n",
            "Epoch 438/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.9297e-04 - accuracy: 0.9994 - val_loss: 0.0507 - val_accuracy: 0.9922\n",
            "Epoch 439/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0012 - accuracy: 0.9991 - val_loss: 0.0528 - val_accuracy: 0.9915\n",
            "Epoch 440/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.4707e-04 - accuracy: 0.9994 - val_loss: 0.0732 - val_accuracy: 0.9908\n",
            "Epoch 441/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0053 - accuracy: 0.9991 - val_loss: 0.0541 - val_accuracy: 0.9915\n",
            "Epoch 442/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.7105e-04 - accuracy: 1.0000 - val_loss: 0.0618 - val_accuracy: 0.9929\n",
            "Epoch 443/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0038 - accuracy: 0.9988 - val_loss: 0.0761 - val_accuracy: 0.9915\n",
            "Epoch 444/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0025 - accuracy: 0.9991 - val_loss: 0.0617 - val_accuracy: 0.9929\n",
            "Epoch 445/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.8202e-04 - accuracy: 0.9997 - val_loss: 0.0577 - val_accuracy: 0.9936\n",
            "Epoch 446/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.3550e-04 - accuracy: 1.0000 - val_loss: 0.0504 - val_accuracy: 0.9929\n",
            "Epoch 447/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0017 - accuracy: 0.9994 - val_loss: 0.0642 - val_accuracy: 0.9922\n",
            "Epoch 448/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0039 - accuracy: 0.9994 - val_loss: 0.0720 - val_accuracy: 0.9915\n",
            "Epoch 449/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0070 - accuracy: 0.9991 - val_loss: 0.0585 - val_accuracy: 0.9929\n",
            "Epoch 450/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0037 - accuracy: 0.9991 - val_loss: 0.0506 - val_accuracy: 0.9922\n",
            "Epoch 451/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0056 - accuracy: 0.9982 - val_loss: 0.0669 - val_accuracy: 0.9929\n",
            "Epoch 452/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.6889e-05 - accuracy: 1.0000 - val_loss: 0.0735 - val_accuracy: 0.9908\n",
            "Epoch 453/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0022 - accuracy: 0.9991 - val_loss: 0.0684 - val_accuracy: 0.9915\n",
            "Epoch 454/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0020 - accuracy: 0.9994 - val_loss: 0.0646 - val_accuracy: 0.9929\n",
            "Epoch 455/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0011 - accuracy: 0.9994 - val_loss: 0.0717 - val_accuracy: 0.9922\n",
            "Epoch 456/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0025 - accuracy: 0.9994 - val_loss: 0.0562 - val_accuracy: 0.9936\n",
            "Epoch 457/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0018 - accuracy: 0.9997 - val_loss: 0.0546 - val_accuracy: 0.9936\n",
            "Epoch 458/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0047 - accuracy: 0.9988 - val_loss: 0.0577 - val_accuracy: 0.9922\n",
            "Epoch 459/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.7249e-06 - accuracy: 1.0000 - val_loss: 0.0643 - val_accuracy: 0.9908\n",
            "Epoch 460/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0046 - accuracy: 0.9988 - val_loss: 0.0569 - val_accuracy: 0.9929\n",
            "Epoch 461/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.8577e-05 - accuracy: 1.0000 - val_loss: 0.0666 - val_accuracy: 0.9922\n",
            "Epoch 462/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0039 - accuracy: 0.9985 - val_loss: 0.0919 - val_accuracy: 0.9886\n",
            "Epoch 463/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0029 - accuracy: 0.9997 - val_loss: 0.0785 - val_accuracy: 0.9908\n",
            "Epoch 464/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0058 - accuracy: 0.9991 - val_loss: 0.0762 - val_accuracy: 0.9900\n",
            "Epoch 465/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.1090e-04 - accuracy: 0.9994 - val_loss: 0.0731 - val_accuracy: 0.9922\n",
            "Epoch 466/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0041 - accuracy: 0.9994 - val_loss: 0.0604 - val_accuracy: 0.9929\n",
            "Epoch 467/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.7258e-06 - accuracy: 1.0000 - val_loss: 0.0627 - val_accuracy: 0.9922\n",
            "Epoch 468/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0017 - accuracy: 0.9994 - val_loss: 0.0649 - val_accuracy: 0.9915\n",
            "Epoch 469/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.1242e-04 - accuracy: 0.9997 - val_loss: 0.0867 - val_accuracy: 0.9908\n",
            "Epoch 470/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0035 - accuracy: 0.9997 - val_loss: 0.0619 - val_accuracy: 0.9936\n",
            "Epoch 471/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0022 - accuracy: 0.9988 - val_loss: 0.0787 - val_accuracy: 0.9929\n",
            "Epoch 472/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 0.0817 - val_accuracy: 0.9915\n",
            "Epoch 473/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.2669e-05 - accuracy: 1.0000 - val_loss: 0.0670 - val_accuracy: 0.9915\n",
            "Epoch 474/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0072 - accuracy: 0.9985 - val_loss: 0.0687 - val_accuracy: 0.9915\n",
            "Epoch 475/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.4030e-04 - accuracy: 0.9994 - val_loss: 0.0685 - val_accuracy: 0.9908\n",
            "Epoch 476/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0028 - accuracy: 0.9997 - val_loss: 0.0653 - val_accuracy: 0.9922\n",
            "Epoch 477/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0023 - accuracy: 0.9997 - val_loss: 0.0645 - val_accuracy: 0.9936\n",
            "Epoch 478/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0021 - accuracy: 0.9994 - val_loss: 0.0731 - val_accuracy: 0.9908\n",
            "Epoch 479/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0041 - accuracy: 0.9994 - val_loss: 0.0867 - val_accuracy: 0.9900\n",
            "Epoch 480/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0052 - accuracy: 0.9988 - val_loss: 0.0922 - val_accuracy: 0.9893\n",
            "Epoch 481/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0022 - accuracy: 0.9994 - val_loss: 0.0646 - val_accuracy: 0.9936\n",
            "Epoch 482/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0092 - accuracy: 0.9979 - val_loss: 0.0707 - val_accuracy: 0.9929\n",
            "Epoch 483/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.2172e-05 - accuracy: 1.0000 - val_loss: 0.0748 - val_accuracy: 0.9922\n",
            "Epoch 484/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 0.0679 - val_accuracy: 0.9936\n",
            "Epoch 485/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0032 - accuracy: 0.9991 - val_loss: 0.0672 - val_accuracy: 0.9936\n",
            "Epoch 486/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0019 - accuracy: 0.9994 - val_loss: 0.0653 - val_accuracy: 0.9943\n",
            "Epoch 487/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.6432e-04 - accuracy: 0.9997 - val_loss: 0.0798 - val_accuracy: 0.9908\n",
            "Epoch 488/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0037 - accuracy: 0.9994 - val_loss: 0.0560 - val_accuracy: 0.9943\n",
            "Epoch 489/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0029 - accuracy: 0.9991 - val_loss: 0.0647 - val_accuracy: 0.9929\n",
            "Epoch 490/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.9271e-05 - accuracy: 1.0000 - val_loss: 0.0730 - val_accuracy: 0.9915\n",
            "Epoch 491/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0040 - accuracy: 0.9994 - val_loss: 0.0856 - val_accuracy: 0.9900\n",
            "Epoch 492/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.1233e-05 - accuracy: 1.0000 - val_loss: 0.0681 - val_accuracy: 0.9929\n",
            "Epoch 493/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.3341e-04 - accuracy: 0.9997 - val_loss: 0.0751 - val_accuracy: 0.9915\n",
            "Epoch 494/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0021 - accuracy: 0.9997 - val_loss: 0.0650 - val_accuracy: 0.9915\n",
            "Epoch 495/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.6628e-06 - accuracy: 1.0000 - val_loss: 0.0682 - val_accuracy: 0.9929\n",
            "Epoch 496/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0045 - accuracy: 0.9994 - val_loss: 0.0842 - val_accuracy: 0.9915\n",
            "Epoch 497/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.3234e-04 - accuracy: 1.0000 - val_loss: 0.1155 - val_accuracy: 0.9879\n",
            "Epoch 498/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0044 - accuracy: 0.9994 - val_loss: 0.0759 - val_accuracy: 0.9922\n",
            "Epoch 499/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0019 - accuracy: 0.9994 - val_loss: 0.0826 - val_accuracy: 0.9900\n",
            "Epoch 500/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0013 - accuracy: 0.9997 - val_loss: 0.0804 - val_accuracy: 0.9922\n",
            "Epoch 501/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0019 - accuracy: 0.9997 - val_loss: 0.0699 - val_accuracy: 0.9929\n",
            "Epoch 502/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.0259e-04 - accuracy: 0.9997 - val_loss: 0.0992 - val_accuracy: 0.9900\n",
            "Epoch 503/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0040 - accuracy: 0.9994 - val_loss: 0.1089 - val_accuracy: 0.9886\n",
            "Epoch 504/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0048 - accuracy: 0.9991 - val_loss: 0.0760 - val_accuracy: 0.9929\n",
            "Epoch 505/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0097 - accuracy: 0.9979 - val_loss: 0.0678 - val_accuracy: 0.9922\n",
            "Epoch 506/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.1284e-04 - accuracy: 0.9994 - val_loss: 0.0650 - val_accuracy: 0.9936\n",
            "Epoch 507/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0038 - accuracy: 0.9994 - val_loss: 0.0898 - val_accuracy: 0.9908\n",
            "Epoch 508/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0036 - accuracy: 0.9997 - val_loss: 0.0724 - val_accuracy: 0.9929\n",
            "Epoch 509/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.6285e-04 - accuracy: 0.9997 - val_loss: 0.0680 - val_accuracy: 0.9936\n",
            "Epoch 510/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.0695e-04 - accuracy: 0.9997 - val_loss: 0.0646 - val_accuracy: 0.9936\n",
            "Epoch 511/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0055 - accuracy: 0.9988 - val_loss: 0.0703 - val_accuracy: 0.9922\n",
            "Epoch 512/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0040 - accuracy: 0.9991 - val_loss: 0.0721 - val_accuracy: 0.9922\n",
            "Epoch 513/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.2753e-05 - accuracy: 1.0000 - val_loss: 0.0849 - val_accuracy: 0.9900\n",
            "Epoch 514/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.5209e-04 - accuracy: 1.0000 - val_loss: 0.0857 - val_accuracy: 0.9908\n",
            "Epoch 515/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0040 - accuracy: 0.9991 - val_loss: 0.0823 - val_accuracy: 0.9915\n",
            "Epoch 516/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0031 - accuracy: 0.9994 - val_loss: 0.0708 - val_accuracy: 0.9922\n",
            "Epoch 517/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.2103e-04 - accuracy: 1.0000 - val_loss: 0.0672 - val_accuracy: 0.9929\n",
            "Epoch 518/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0040 - accuracy: 0.9991 - val_loss: 0.0838 - val_accuracy: 0.9915\n",
            "Epoch 519/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0034 - accuracy: 0.9994 - val_loss: 0.0855 - val_accuracy: 0.9929\n",
            "Epoch 520/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0047 - accuracy: 0.9988 - val_loss: 0.0859 - val_accuracy: 0.9922\n",
            "Epoch 521/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.2126e-05 - accuracy: 1.0000 - val_loss: 0.0806 - val_accuracy: 0.9929\n",
            "Epoch 522/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.2850e-04 - accuracy: 0.9997 - val_loss: 0.0881 - val_accuracy: 0.9893\n",
            "Epoch 523/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.3888e-05 - accuracy: 1.0000 - val_loss: 0.0822 - val_accuracy: 0.9922\n",
            "Epoch 524/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0035 - accuracy: 0.9994 - val_loss: 0.0778 - val_accuracy: 0.9929\n",
            "Epoch 525/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0033 - accuracy: 0.9991 - val_loss: 0.0858 - val_accuracy: 0.9900\n",
            "Epoch 526/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.4753e-04 - accuracy: 0.9997 - val_loss: 0.1042 - val_accuracy: 0.9872\n",
            "Epoch 527/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.2447e-05 - accuracy: 1.0000 - val_loss: 0.0723 - val_accuracy: 0.9915\n",
            "Epoch 528/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0032 - accuracy: 0.9994 - val_loss: 0.0563 - val_accuracy: 0.9929\n",
            "Epoch 529/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.0644e-06 - accuracy: 1.0000 - val_loss: 0.0597 - val_accuracy: 0.9929\n",
            "Epoch 530/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0625 - val_accuracy: 0.9922\n",
            "Epoch 531/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.8917e-04 - accuracy: 0.9997 - val_loss: 0.0693 - val_accuracy: 0.9908\n",
            "Epoch 532/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0025 - accuracy: 0.9991 - val_loss: 0.0674 - val_accuracy: 0.9922\n",
            "Epoch 533/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.3556e-06 - accuracy: 1.0000 - val_loss: 0.0751 - val_accuracy: 0.9922\n",
            "Epoch 534/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0020 - accuracy: 0.9997 - val_loss: 0.0764 - val_accuracy: 0.9915\n",
            "Epoch 535/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0081 - accuracy: 0.9982 - val_loss: 0.0819 - val_accuracy: 0.9922\n",
            "Epoch 536/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0025 - accuracy: 0.9997 - val_loss: 0.0848 - val_accuracy: 0.9922\n",
            "Epoch 537/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.5021e-05 - accuracy: 1.0000 - val_loss: 0.0853 - val_accuracy: 0.9900\n",
            "Epoch 538/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0014 - accuracy: 0.9994 - val_loss: 0.0762 - val_accuracy: 0.9922\n",
            "Epoch 539/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0011 - accuracy: 0.9997 - val_loss: 0.0860 - val_accuracy: 0.9922\n",
            "Epoch 540/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0030 - accuracy: 0.9988 - val_loss: 0.0768 - val_accuracy: 0.9922\n",
            "Epoch 541/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0060 - accuracy: 0.9991 - val_loss: 0.0679 - val_accuracy: 0.9922\n",
            "Epoch 542/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0029 - accuracy: 0.9991 - val_loss: 0.0677 - val_accuracy: 0.9929\n",
            "Epoch 543/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.1682e-06 - accuracy: 1.0000 - val_loss: 0.0702 - val_accuracy: 0.9922\n",
            "Epoch 544/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.4622e-04 - accuracy: 0.9997 - val_loss: 0.0692 - val_accuracy: 0.9922\n",
            "Epoch 545/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0033 - accuracy: 0.9991 - val_loss: 0.0737 - val_accuracy: 0.9929\n",
            "Epoch 546/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.0884e-06 - accuracy: 1.0000 - val_loss: 0.0741 - val_accuracy: 0.9929\n",
            "Epoch 547/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0024 - accuracy: 0.9994 - val_loss: 0.0748 - val_accuracy: 0.9929\n",
            "Epoch 548/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0029 - accuracy: 0.9991 - val_loss: 0.0912 - val_accuracy: 0.9900\n",
            "Epoch 549/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0029 - accuracy: 0.9991 - val_loss: 0.0877 - val_accuracy: 0.9915\n",
            "Epoch 550/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.7115e-04 - accuracy: 1.0000 - val_loss: 0.0843 - val_accuracy: 0.9922\n",
            "Epoch 551/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0058 - accuracy: 0.9988 - val_loss: 0.0644 - val_accuracy: 0.9943\n",
            "Epoch 552/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0027 - accuracy: 0.9991 - val_loss: 0.0732 - val_accuracy: 0.9929\n",
            "Epoch 553/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.9733e-04 - accuracy: 0.9994 - val_loss: 0.0799 - val_accuracy: 0.9922\n",
            "Epoch 554/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0025 - accuracy: 0.9997 - val_loss: 0.0911 - val_accuracy: 0.9908\n",
            "Epoch 555/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.1024e-06 - accuracy: 1.0000 - val_loss: 0.0792 - val_accuracy: 0.9915\n",
            "Epoch 556/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.9535e-04 - accuracy: 1.0000 - val_loss: 0.0924 - val_accuracy: 0.9915\n",
            "Epoch 557/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.1601e-04 - accuracy: 1.0000 - val_loss: 0.0997 - val_accuracy: 0.9900\n",
            "Epoch 558/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.3353e-05 - accuracy: 1.0000 - val_loss: 0.0747 - val_accuracy: 0.9936\n",
            "Epoch 559/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0035 - accuracy: 0.9997 - val_loss: 0.0695 - val_accuracy: 0.9936\n",
            "Epoch 560/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0088 - accuracy: 0.9985 - val_loss: 0.0787 - val_accuracy: 0.9922\n",
            "Epoch 561/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0034 - accuracy: 0.9997 - val_loss: 0.0919 - val_accuracy: 0.9900\n",
            "Epoch 562/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0045 - accuracy: 0.9991 - val_loss: 0.0838 - val_accuracy: 0.9908\n",
            "Epoch 563/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0067 - accuracy: 0.9994 - val_loss: 0.0845 - val_accuracy: 0.9900\n",
            "Epoch 564/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0025 - accuracy: 0.9997 - val_loss: 0.0869 - val_accuracy: 0.9915\n",
            "Epoch 565/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.9942e-04 - accuracy: 0.9997 - val_loss: 0.0899 - val_accuracy: 0.9893\n",
            "Epoch 566/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.3045e-05 - accuracy: 1.0000 - val_loss: 0.0633 - val_accuracy: 0.9936\n",
            "Epoch 567/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0078 - accuracy: 0.9985 - val_loss: 0.0979 - val_accuracy: 0.9908\n",
            "Epoch 568/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.8132e-04 - accuracy: 1.0000 - val_loss: 0.0816 - val_accuracy: 0.9915\n",
            "Epoch 569/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.7039e-07 - accuracy: 1.0000 - val_loss: 0.0898 - val_accuracy: 0.9908\n",
            "Epoch 570/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0082 - accuracy: 0.9979 - val_loss: 0.0985 - val_accuracy: 0.9908\n",
            "Epoch 571/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.9014e-07 - accuracy: 1.0000 - val_loss: 0.0973 - val_accuracy: 0.9908\n",
            "Epoch 572/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0064 - accuracy: 0.9994 - val_loss: 0.1095 - val_accuracy: 0.9879\n",
            "Epoch 573/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0042 - accuracy: 0.9994 - val_loss: 0.1079 - val_accuracy: 0.9879\n",
            "Epoch 574/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0046 - accuracy: 0.9991 - val_loss: 0.0785 - val_accuracy: 0.9922\n",
            "Epoch 575/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 0.1065 - val_accuracy: 0.9893\n",
            "Epoch 576/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0011 - accuracy: 0.9997 - val_loss: 0.0965 - val_accuracy: 0.9922\n",
            "Epoch 577/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0038 - accuracy: 0.9994 - val_loss: 0.0936 - val_accuracy: 0.9915\n",
            "Epoch 578/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.8966e-07 - accuracy: 1.0000 - val_loss: 0.0941 - val_accuracy: 0.9915\n",
            "Epoch 579/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0068 - accuracy: 0.9994 - val_loss: 0.0953 - val_accuracy: 0.9908\n",
            "Epoch 580/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.1538e-06 - accuracy: 1.0000 - val_loss: 0.0905 - val_accuracy: 0.9922\n",
            "Epoch 581/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0014 - accuracy: 0.9991 - val_loss: 0.1432 - val_accuracy: 0.9858\n",
            "Epoch 582/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.3696e-04 - accuracy: 0.9997 - val_loss: 0.0835 - val_accuracy: 0.9908\n",
            "Epoch 583/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.6713e-07 - accuracy: 1.0000 - val_loss: 0.0909 - val_accuracy: 0.9915\n",
            "Epoch 584/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0110 - accuracy: 0.9982 - val_loss: 0.1019 - val_accuracy: 0.9922\n",
            "Epoch 585/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0055 - accuracy: 0.9988 - val_loss: 0.0888 - val_accuracy: 0.9922\n",
            "Epoch 586/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.7153e-07 - accuracy: 1.0000 - val_loss: 0.0882 - val_accuracy: 0.9922\n",
            "Epoch 587/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.0607e-04 - accuracy: 0.9997 - val_loss: 0.0854 - val_accuracy: 0.9915\n",
            "Epoch 588/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0026 - accuracy: 0.9991 - val_loss: 0.0767 - val_accuracy: 0.9936\n",
            "Epoch 589/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.4368e-05 - accuracy: 1.0000 - val_loss: 0.0740 - val_accuracy: 0.9936\n",
            "Epoch 590/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0016 - accuracy: 0.9994 - val_loss: 0.0872 - val_accuracy: 0.9922\n",
            "Epoch 591/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.0427e-05 - accuracy: 1.0000 - val_loss: 0.0883 - val_accuracy: 0.9922\n",
            "Epoch 592/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0114 - accuracy: 0.9973 - val_loss: 0.0855 - val_accuracy: 0.9900\n",
            "Epoch 593/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0010 - accuracy: 0.9997 - val_loss: 0.0854 - val_accuracy: 0.9922\n",
            "Epoch 594/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0036 - accuracy: 0.9997 - val_loss: 0.0872 - val_accuracy: 0.9922\n",
            "Epoch 595/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.5669e-06 - accuracy: 1.0000 - val_loss: 0.1466 - val_accuracy: 0.9886\n",
            "Epoch 596/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0022 - accuracy: 0.9994 - val_loss: 0.0840 - val_accuracy: 0.9922\n",
            "Epoch 597/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0012 - accuracy: 0.9994 - val_loss: 0.0887 - val_accuracy: 0.9929\n",
            "Epoch 598/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.0543e-06 - accuracy: 1.0000 - val_loss: 0.0886 - val_accuracy: 0.9900\n",
            "Epoch 599/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0047 - accuracy: 0.9988 - val_loss: 0.0757 - val_accuracy: 0.9908\n",
            "Epoch 600/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0026 - accuracy: 0.9997 - val_loss: 0.0667 - val_accuracy: 0.9936\n",
            "Epoch 601/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0029 - accuracy: 0.9997 - val_loss: 0.0678 - val_accuracy: 0.9936\n",
            "Epoch 602/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.6055e-06 - accuracy: 1.0000 - val_loss: 0.0940 - val_accuracy: 0.9908\n",
            "Epoch 603/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0011 - accuracy: 0.9997 - val_loss: 0.0750 - val_accuracy: 0.9936\n",
            "Epoch 604/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0054 - accuracy: 0.9988 - val_loss: 0.0902 - val_accuracy: 0.9922\n",
            "Epoch 605/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0092 - accuracy: 0.9985 - val_loss: 0.1305 - val_accuracy: 0.9893\n",
            "Epoch 606/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0084 - accuracy: 0.9988 - val_loss: 0.0889 - val_accuracy: 0.9922\n",
            "Epoch 607/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0036 - accuracy: 0.9994 - val_loss: 0.0897 - val_accuracy: 0.9915\n",
            "Epoch 608/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0023 - accuracy: 0.9994 - val_loss: 0.0909 - val_accuracy: 0.9915\n",
            "Epoch 609/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.6876e-07 - accuracy: 1.0000 - val_loss: 0.0887 - val_accuracy: 0.9908\n",
            "Epoch 610/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0052 - accuracy: 0.9988 - val_loss: 0.0723 - val_accuracy: 0.9929\n",
            "Epoch 611/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.2922e-05 - accuracy: 1.0000 - val_loss: 0.0667 - val_accuracy: 0.9929\n",
            "Epoch 612/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0026 - accuracy: 0.9991 - val_loss: 0.0829 - val_accuracy: 0.9915\n",
            "Epoch 613/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.4682e-07 - accuracy: 1.0000 - val_loss: 0.0813 - val_accuracy: 0.9915\n",
            "Epoch 614/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.5916e-05 - accuracy: 1.0000 - val_loss: 0.1163 - val_accuracy: 0.9908\n",
            "Epoch 615/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0126 - accuracy: 0.9988 - val_loss: 0.1091 - val_accuracy: 0.9886\n",
            "Epoch 616/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0058 - accuracy: 0.9994 - val_loss: 0.0822 - val_accuracy: 0.9922\n",
            "Epoch 617/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0042 - accuracy: 0.9994 - val_loss: 0.0841 - val_accuracy: 0.9915\n",
            "Epoch 618/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0061 - accuracy: 0.9991 - val_loss: 0.0951 - val_accuracy: 0.9915\n",
            "Epoch 619/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.8639e-05 - accuracy: 1.0000 - val_loss: 0.0889 - val_accuracy: 0.9915\n",
            "Epoch 620/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0035 - accuracy: 0.9994 - val_loss: 0.1076 - val_accuracy: 0.9908\n",
            "Epoch 621/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0106 - accuracy: 0.9988 - val_loss: 0.1144 - val_accuracy: 0.9893\n",
            "Epoch 622/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 0.1057 - val_accuracy: 0.9915\n",
            "Epoch 623/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.9247e-04 - accuracy: 0.9997 - val_loss: 0.1053 - val_accuracy: 0.9900\n",
            "Epoch 624/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0039 - accuracy: 0.9997 - val_loss: 0.1157 - val_accuracy: 0.9872\n",
            "Epoch 625/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0035 - accuracy: 0.9994 - val_loss: 0.1123 - val_accuracy: 0.9893\n",
            "Epoch 626/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0078 - accuracy: 0.9982 - val_loss: 0.0845 - val_accuracy: 0.9929\n",
            "Epoch 627/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.3662e-07 - accuracy: 1.0000 - val_loss: 0.0878 - val_accuracy: 0.9915\n",
            "Epoch 628/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0146 - accuracy: 0.9979 - val_loss: 0.0946 - val_accuracy: 0.9908\n",
            "Epoch 629/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0010 - accuracy: 0.9994 - val_loss: 0.0857 - val_accuracy: 0.9922\n",
            "Epoch 630/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.8283e-07 - accuracy: 1.0000 - val_loss: 0.0869 - val_accuracy: 0.9922\n",
            "Epoch 631/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0067 - accuracy: 0.9994 - val_loss: 0.0875 - val_accuracy: 0.9929\n",
            "Epoch 632/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.7446e-05 - accuracy: 1.0000 - val_loss: 0.0947 - val_accuracy: 0.9915\n",
            "Epoch 633/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0026 - accuracy: 0.9997 - val_loss: 0.1026 - val_accuracy: 0.9893\n",
            "Epoch 634/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.4241e-06 - accuracy: 1.0000 - val_loss: 0.0945 - val_accuracy: 0.9922\n",
            "Epoch 635/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.5277e-07 - accuracy: 1.0000 - val_loss: 0.0883 - val_accuracy: 0.9929\n",
            "Epoch 636/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0022 - accuracy: 0.9997 - val_loss: 0.0995 - val_accuracy: 0.9908\n",
            "Epoch 637/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0023 - accuracy: 0.9994 - val_loss: 0.1011 - val_accuracy: 0.9915\n",
            "Epoch 638/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.0153e-06 - accuracy: 1.0000 - val_loss: 0.1060 - val_accuracy: 0.9908\n",
            "Epoch 639/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.8146e-05 - accuracy: 1.0000 - val_loss: 0.0994 - val_accuracy: 0.9908\n",
            "Epoch 640/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.1670e-05 - accuracy: 1.0000 - val_loss: 0.0914 - val_accuracy: 0.9922\n",
            "Epoch 641/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.7343e-04 - accuracy: 0.9997 - val_loss: 0.0880 - val_accuracy: 0.9922\n",
            "Epoch 642/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.7281e-07 - accuracy: 1.0000 - val_loss: 0.0880 - val_accuracy: 0.9922\n",
            "Epoch 643/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.8639e-04 - accuracy: 0.9994 - val_loss: 0.0945 - val_accuracy: 0.9922\n",
            "Epoch 644/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.4306e-07 - accuracy: 1.0000 - val_loss: 0.1013 - val_accuracy: 0.9915\n",
            "Epoch 645/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0029 - accuracy: 0.9991 - val_loss: 0.1177 - val_accuracy: 0.9900\n",
            "Epoch 646/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0050 - accuracy: 0.9988 - val_loss: 0.0973 - val_accuracy: 0.9922\n",
            "Epoch 647/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0051 - accuracy: 0.9991 - val_loss: 0.1127 - val_accuracy: 0.9893\n",
            "Epoch 648/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0039 - accuracy: 0.9997 - val_loss: 0.0944 - val_accuracy: 0.9922\n",
            "Epoch 649/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0086 - accuracy: 0.9991 - val_loss: 0.0983 - val_accuracy: 0.9908\n",
            "Epoch 650/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.1138e-05 - accuracy: 1.0000 - val_loss: 0.0973 - val_accuracy: 0.9915\n",
            "Epoch 651/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0017 - accuracy: 0.9994 - val_loss: 0.1106 - val_accuracy: 0.9900\n",
            "Epoch 652/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0035 - accuracy: 0.9994 - val_loss: 0.0841 - val_accuracy: 0.9922\n",
            "Epoch 653/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.7054e-07 - accuracy: 1.0000 - val_loss: 0.1391 - val_accuracy: 0.9858\n",
            "Epoch 654/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0044 - accuracy: 0.9994 - val_loss: 0.0916 - val_accuracy: 0.9908\n",
            "Epoch 655/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0035 - accuracy: 0.9997 - val_loss: 0.1079 - val_accuracy: 0.9908\n",
            "Epoch 656/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.8080e-07 - accuracy: 1.0000 - val_loss: 0.1024 - val_accuracy: 0.9915\n",
            "Epoch 657/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.8761e-04 - accuracy: 0.9997 - val_loss: 0.0920 - val_accuracy: 0.9908\n",
            "Epoch 658/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.2646e-07 - accuracy: 1.0000 - val_loss: 0.0926 - val_accuracy: 0.9915\n",
            "Epoch 659/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.5365e-04 - accuracy: 0.9997 - val_loss: 0.1016 - val_accuracy: 0.9893\n",
            "Epoch 660/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.4399e-06 - accuracy: 1.0000 - val_loss: 0.0970 - val_accuracy: 0.9929\n",
            "Epoch 661/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0087 - accuracy: 0.9988 - val_loss: 0.1190 - val_accuracy: 0.9908\n",
            "Epoch 662/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0029 - accuracy: 0.9991 - val_loss: 0.1129 - val_accuracy: 0.9900\n",
            "Epoch 663/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0040 - accuracy: 0.9991 - val_loss: 0.0998 - val_accuracy: 0.9922\n",
            "Epoch 664/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.5956e-04 - accuracy: 0.9997 - val_loss: 0.0925 - val_accuracy: 0.9915\n",
            "Epoch 665/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0031 - accuracy: 0.9994 - val_loss: 0.1026 - val_accuracy: 0.9900\n",
            "Epoch 666/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0037 - accuracy: 0.9991 - val_loss: 0.1270 - val_accuracy: 0.9879\n",
            "Epoch 667/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0083 - accuracy: 0.9991 - val_loss: 0.1063 - val_accuracy: 0.9908\n",
            "Epoch 668/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0039 - accuracy: 0.9991 - val_loss: 0.1076 - val_accuracy: 0.9915\n",
            "Epoch 669/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0065 - accuracy: 0.9985 - val_loss: 0.0880 - val_accuracy: 0.9922\n",
            "Epoch 670/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.8184e-04 - accuracy: 0.9997 - val_loss: 0.1032 - val_accuracy: 0.9908\n",
            "Epoch 671/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0043 - accuracy: 0.9994 - val_loss: 0.0982 - val_accuracy: 0.9915\n",
            "Epoch 672/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.7608e-07 - accuracy: 1.0000 - val_loss: 0.1080 - val_accuracy: 0.9893\n",
            "Epoch 673/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.2099e-04 - accuracy: 0.9997 - val_loss: 0.0938 - val_accuracy: 0.9915\n",
            "Epoch 674/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.8814e-07 - accuracy: 1.0000 - val_loss: 0.0918 - val_accuracy: 0.9915\n",
            "Epoch 675/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.8252e-06 - accuracy: 1.0000 - val_loss: 0.0846 - val_accuracy: 0.9915\n",
            "Epoch 676/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.3921e-04 - accuracy: 0.9997 - val_loss: 0.1217 - val_accuracy: 0.9879\n",
            "Epoch 677/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0101 - accuracy: 0.9985 - val_loss: 0.1199 - val_accuracy: 0.9886\n",
            "Epoch 678/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0086 - accuracy: 0.9991 - val_loss: 0.0946 - val_accuracy: 0.9915\n",
            "Epoch 679/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.5884e-07 - accuracy: 1.0000 - val_loss: 0.0931 - val_accuracy: 0.9922\n",
            "Epoch 680/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0011 - accuracy: 0.9997 - val_loss: 0.0979 - val_accuracy: 0.9922\n",
            "Epoch 681/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0102 - accuracy: 0.9982 - val_loss: 0.0942 - val_accuracy: 0.9929\n",
            "Epoch 682/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0052 - accuracy: 0.9988 - val_loss: 0.1031 - val_accuracy: 0.9922\n",
            "Epoch 683/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0102 - accuracy: 0.9991 - val_loss: 0.0865 - val_accuracy: 0.9915\n",
            "Epoch 684/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0092 - accuracy: 0.9988 - val_loss: 0.0911 - val_accuracy: 0.9908\n",
            "Epoch 685/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.3456e-07 - accuracy: 1.0000 - val_loss: 0.0924 - val_accuracy: 0.9908\n",
            "Epoch 686/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0017 - accuracy: 0.9997 - val_loss: 0.1334 - val_accuracy: 0.9893\n",
            "Epoch 687/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0051 - accuracy: 0.9985 - val_loss: 0.0794 - val_accuracy: 0.9936\n",
            "Epoch 688/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.7897e-07 - accuracy: 1.0000 - val_loss: 0.0758 - val_accuracy: 0.9929\n",
            "Epoch 689/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.0984 - val_accuracy: 0.9915\n",
            "Epoch 690/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0048 - accuracy: 0.9982 - val_loss: 0.0847 - val_accuracy: 0.9929\n",
            "Epoch 691/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0024 - accuracy: 0.9997 - val_loss: 0.0988 - val_accuracy: 0.9908\n",
            "Epoch 692/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0035 - accuracy: 0.9991 - val_loss: 0.0996 - val_accuracy: 0.9929\n",
            "Epoch 693/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.2346e-07 - accuracy: 1.0000 - val_loss: 0.0990 - val_accuracy: 0.9915\n",
            "Epoch 694/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0029 - accuracy: 0.9991 - val_loss: 0.0875 - val_accuracy: 0.9915\n",
            "Epoch 695/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.2691e-07 - accuracy: 1.0000 - val_loss: 0.0949 - val_accuracy: 0.9915\n",
            "Epoch 696/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.1167 - val_accuracy: 0.9922\n",
            "Epoch 697/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0034 - accuracy: 0.9997 - val_loss: 0.0926 - val_accuracy: 0.9922\n",
            "Epoch 698/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0035 - accuracy: 0.9997 - val_loss: 0.0783 - val_accuracy: 0.9922\n",
            "Epoch 699/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.3705e-07 - accuracy: 1.0000 - val_loss: 0.0848 - val_accuracy: 0.9929\n",
            "Epoch 700/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.6899e-08 - accuracy: 1.0000 - val_loss: 0.0855 - val_accuracy: 0.9929\n",
            "Epoch 701/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0078 - accuracy: 0.9988 - val_loss: 0.0988 - val_accuracy: 0.9922\n",
            "Epoch 702/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0020 - accuracy: 0.9994 - val_loss: 0.0859 - val_accuracy: 0.9915\n",
            "Epoch 703/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0083 - accuracy: 0.9988 - val_loss: 0.1134 - val_accuracy: 0.9908\n",
            "Epoch 704/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0050 - accuracy: 0.9982 - val_loss: 0.1042 - val_accuracy: 0.9915\n",
            "Epoch 705/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0045 - accuracy: 0.9988 - val_loss: 0.1066 - val_accuracy: 0.9915\n",
            "Epoch 706/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.7955e-04 - accuracy: 0.9997 - val_loss: 0.0813 - val_accuracy: 0.9943\n",
            "Epoch 707/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.1902e-07 - accuracy: 1.0000 - val_loss: 0.0865 - val_accuracy: 0.9929\n",
            "Epoch 708/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.1410e-07 - accuracy: 1.0000 - val_loss: 0.1090 - val_accuracy: 0.9908\n",
            "Epoch 709/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0144 - accuracy: 0.9979 - val_loss: 0.1123 - val_accuracy: 0.9900\n",
            "Epoch 710/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0036 - accuracy: 0.9997 - val_loss: 0.1041 - val_accuracy: 0.9908\n",
            "Epoch 711/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0044 - accuracy: 0.9994 - val_loss: 0.1087 - val_accuracy: 0.9908\n",
            "Epoch 712/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.3532e-05 - accuracy: 1.0000 - val_loss: 0.1714 - val_accuracy: 0.9879\n",
            "Epoch 713/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0046 - accuracy: 0.9997 - val_loss: 0.0932 - val_accuracy: 0.9929\n",
            "Epoch 714/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.0143e-07 - accuracy: 1.0000 - val_loss: 0.1015 - val_accuracy: 0.9908\n",
            "Epoch 715/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.8235e-06 - accuracy: 1.0000 - val_loss: 0.0985 - val_accuracy: 0.9922\n",
            "Epoch 716/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0042 - accuracy: 0.9997 - val_loss: 0.1069 - val_accuracy: 0.9922\n",
            "Epoch 717/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.1602e-04 - accuracy: 0.9997 - val_loss: 0.1056 - val_accuracy: 0.9915\n",
            "Epoch 718/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0039 - accuracy: 0.9991 - val_loss: 0.1183 - val_accuracy: 0.9922\n",
            "Epoch 719/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0028 - accuracy: 0.9997 - val_loss: 0.1152 - val_accuracy: 0.9908\n",
            "Epoch 720/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0045 - accuracy: 0.9997 - val_loss: 0.1057 - val_accuracy: 0.9929\n",
            "Epoch 721/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0048 - accuracy: 0.9991 - val_loss: 0.1193 - val_accuracy: 0.9900\n",
            "Epoch 722/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0049 - accuracy: 0.9988 - val_loss: 0.1046 - val_accuracy: 0.9908\n",
            "Epoch 723/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0048 - accuracy: 0.9994 - val_loss: 0.1084 - val_accuracy: 0.9915\n",
            "Epoch 724/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.7605e-07 - accuracy: 1.0000 - val_loss: 0.1117 - val_accuracy: 0.9908\n",
            "Epoch 725/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0069 - accuracy: 0.9991 - val_loss: 0.1012 - val_accuracy: 0.9929\n",
            "Epoch 726/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.6459e-07 - accuracy: 1.0000 - val_loss: 0.1015 - val_accuracy: 0.9922\n",
            "Epoch 727/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0020 - accuracy: 0.9994 - val_loss: 0.1377 - val_accuracy: 0.9879\n",
            "Epoch 728/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0056 - accuracy: 0.9991 - val_loss: 0.1205 - val_accuracy: 0.9893\n",
            "Epoch 729/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.1154 - val_accuracy: 0.9908\n",
            "Epoch 730/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.2234e-07 - accuracy: 1.0000 - val_loss: 0.1822 - val_accuracy: 0.9872\n",
            "Epoch 731/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0058 - accuracy: 0.9991 - val_loss: 0.0954 - val_accuracy: 0.9922\n",
            "Epoch 732/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.8087e-08 - accuracy: 1.0000 - val_loss: 0.1003 - val_accuracy: 0.9922\n",
            "Epoch 733/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0042 - accuracy: 0.9994 - val_loss: 0.1118 - val_accuracy: 0.9900\n",
            "Epoch 734/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0086 - accuracy: 0.9991 - val_loss: 0.1090 - val_accuracy: 0.9915\n",
            "Epoch 735/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.2016e-07 - accuracy: 1.0000 - val_loss: 0.1114 - val_accuracy: 0.9915\n",
            "Epoch 736/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0070 - accuracy: 0.9991 - val_loss: 0.1128 - val_accuracy: 0.9900\n",
            "Epoch 737/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0056 - accuracy: 0.9994 - val_loss: 0.1022 - val_accuracy: 0.9915\n",
            "Epoch 738/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0062 - accuracy: 0.9994 - val_loss: 0.1134 - val_accuracy: 0.9908\n",
            "Epoch 739/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.9404e-08 - accuracy: 1.0000 - val_loss: 0.1106 - val_accuracy: 0.9900\n",
            "Epoch 740/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0044 - accuracy: 0.9988 - val_loss: 0.1210 - val_accuracy: 0.9900\n",
            "Epoch 741/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.7705e-08 - accuracy: 1.0000 - val_loss: 0.1171 - val_accuracy: 0.9908\n",
            "Epoch 742/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0098 - accuracy: 0.9985 - val_loss: 0.1069 - val_accuracy: 0.9900\n",
            "Epoch 743/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0065 - accuracy: 0.9988 - val_loss: 0.1510 - val_accuracy: 0.9872\n",
            "Epoch 744/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0090 - accuracy: 0.9985 - val_loss: 0.0845 - val_accuracy: 0.9936\n",
            "Epoch 745/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.9459e-05 - accuracy: 1.0000 - val_loss: 0.0931 - val_accuracy: 0.9915\n",
            "Epoch 746/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0019 - accuracy: 0.9997 - val_loss: 0.0883 - val_accuracy: 0.9900\n",
            "Epoch 747/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0047 - accuracy: 0.9994 - val_loss: 0.0957 - val_accuracy: 0.9908\n",
            "Epoch 748/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0052 - accuracy: 0.9994 - val_loss: 0.0783 - val_accuracy: 0.9922\n",
            "Epoch 749/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0058 - accuracy: 0.9994 - val_loss: 0.1022 - val_accuracy: 0.9900\n",
            "Epoch 750/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.8318e-05 - accuracy: 1.0000 - val_loss: 0.0845 - val_accuracy: 0.9922\n",
            "Epoch 751/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.9484e-07 - accuracy: 1.0000 - val_loss: 0.0873 - val_accuracy: 0.9915\n",
            "Epoch 752/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0094 - accuracy: 0.9985 - val_loss: 0.0986 - val_accuracy: 0.9915\n",
            "Epoch 753/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0082 - accuracy: 0.9994 - val_loss: 0.0958 - val_accuracy: 0.9929\n",
            "Epoch 754/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0040 - accuracy: 0.9997 - val_loss: 0.0909 - val_accuracy: 0.9915\n",
            "Epoch 755/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0022 - accuracy: 0.9997 - val_loss: 0.0862 - val_accuracy: 0.9915\n",
            "Epoch 756/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0055 - accuracy: 0.9991 - val_loss: 0.1096 - val_accuracy: 0.9893\n",
            "Epoch 757/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0045 - accuracy: 0.9997 - val_loss: 0.0991 - val_accuracy: 0.9915\n",
            "Epoch 758/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.3223e-06 - accuracy: 1.0000 - val_loss: 0.1277 - val_accuracy: 0.9886\n",
            "Epoch 759/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0050 - accuracy: 0.9997 - val_loss: 0.1051 - val_accuracy: 0.9908\n",
            "Epoch 760/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0063 - accuracy: 0.9994 - val_loss: 0.0971 - val_accuracy: 0.9908\n",
            "Epoch 761/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0046 - accuracy: 0.9994 - val_loss: 0.1008 - val_accuracy: 0.9922\n",
            "Epoch 762/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.5419e-08 - accuracy: 1.0000 - val_loss: 0.1045 - val_accuracy: 0.9915\n",
            "Epoch 763/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0027 - accuracy: 0.9997 - val_loss: 0.1004 - val_accuracy: 0.9922\n",
            "Epoch 764/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0012 - accuracy: 0.9997 - val_loss: 0.1173 - val_accuracy: 0.9893\n",
            "Epoch 765/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.8765e-06 - accuracy: 1.0000 - val_loss: 0.1044 - val_accuracy: 0.9900\n",
            "Epoch 766/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.1289e-05 - accuracy: 1.0000 - val_loss: 0.1432 - val_accuracy: 0.9879\n",
            "Epoch 767/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0042 - accuracy: 0.9994 - val_loss: 0.0975 - val_accuracy: 0.9915\n",
            "Epoch 768/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.8258e-08 - accuracy: 1.0000 - val_loss: 0.0968 - val_accuracy: 0.9915\n",
            "Epoch 769/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.6588e-04 - accuracy: 0.9997 - val_loss: 0.0952 - val_accuracy: 0.9929\n",
            "Epoch 770/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0047 - accuracy: 0.9994 - val_loss: 0.0873 - val_accuracy: 0.9915\n",
            "Epoch 771/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0088 - accuracy: 0.9994 - val_loss: 0.0951 - val_accuracy: 0.9929\n",
            "Epoch 772/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.5505e-06 - accuracy: 1.0000 - val_loss: 0.1076 - val_accuracy: 0.9915\n",
            "Epoch 773/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0032 - accuracy: 0.9997 - val_loss: 0.1127 - val_accuracy: 0.9908\n",
            "Epoch 774/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0067 - accuracy: 0.9991 - val_loss: 0.1420 - val_accuracy: 0.9879\n",
            "Epoch 775/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0043 - accuracy: 0.9988 - val_loss: 0.1002 - val_accuracy: 0.9900\n",
            "Epoch 776/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.0587e-07 - accuracy: 1.0000 - val_loss: 0.1050 - val_accuracy: 0.9893\n",
            "Epoch 777/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0021 - accuracy: 0.9994 - val_loss: 0.1024 - val_accuracy: 0.9915\n",
            "Epoch 778/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0091 - accuracy: 0.9979 - val_loss: 0.0999 - val_accuracy: 0.9922\n",
            "Epoch 779/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.4609e-06 - accuracy: 1.0000 - val_loss: 0.0893 - val_accuracy: 0.9915\n",
            "Epoch 780/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0049 - accuracy: 0.9994 - val_loss: 0.0913 - val_accuracy: 0.9915\n",
            "Epoch 781/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.2645e-07 - accuracy: 1.0000 - val_loss: 0.0917 - val_accuracy: 0.9922\n",
            "Epoch 782/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0023 - accuracy: 0.9994 - val_loss: 0.0972 - val_accuracy: 0.9908\n",
            "Epoch 783/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0071 - accuracy: 0.9991 - val_loss: 0.1081 - val_accuracy: 0.9893\n",
            "Epoch 784/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.0644e-05 - accuracy: 1.0000 - val_loss: 0.1019 - val_accuracy: 0.9900\n",
            "Epoch 785/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0014 - accuracy: 0.9994 - val_loss: 0.1108 - val_accuracy: 0.9908\n",
            "Epoch 786/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0026 - accuracy: 0.9991 - val_loss: 0.0951 - val_accuracy: 0.9936\n",
            "Epoch 787/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0040 - accuracy: 0.9994 - val_loss: 0.0976 - val_accuracy: 0.9915\n",
            "Epoch 788/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0024 - accuracy: 0.9994 - val_loss: 0.0838 - val_accuracy: 0.9929\n",
            "Epoch 789/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0033 - accuracy: 0.9991 - val_loss: 0.1186 - val_accuracy: 0.9915\n",
            "Epoch 790/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0105 - accuracy: 0.9985 - val_loss: 0.1053 - val_accuracy: 0.9900\n",
            "Epoch 791/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.1413e-07 - accuracy: 1.0000 - val_loss: 0.1009 - val_accuracy: 0.9908\n",
            "Epoch 792/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.3050e-08 - accuracy: 1.0000 - val_loss: 0.0823 - val_accuracy: 0.9929\n",
            "Epoch 793/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.1181 - val_accuracy: 0.9915\n",
            "Epoch 794/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0018 - accuracy: 0.9997 - val_loss: 0.0888 - val_accuracy: 0.9922\n",
            "Epoch 795/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.3110e-08 - accuracy: 1.0000 - val_loss: 0.0871 - val_accuracy: 0.9915\n",
            "Epoch 796/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.9948e-08 - accuracy: 1.0000 - val_loss: 0.1081 - val_accuracy: 0.9915\n",
            "Epoch 797/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.0937e-04 - accuracy: 0.9997 - val_loss: 0.1368 - val_accuracy: 0.9900\n",
            "Epoch 798/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0033 - accuracy: 0.9994 - val_loss: 0.1031 - val_accuracy: 0.9915\n",
            "Epoch 799/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.3567e-08 - accuracy: 1.0000 - val_loss: 0.1037 - val_accuracy: 0.9908\n",
            "Epoch 800/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.4968e-08 - accuracy: 1.0000 - val_loss: 0.1108 - val_accuracy: 0.9915\n",
            "Epoch 801/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.5282e-08 - accuracy: 1.0000 - val_loss: 0.1148 - val_accuracy: 0.9915\n",
            "Epoch 802/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.1047 - val_accuracy: 0.9915\n",
            "Epoch 803/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0049 - accuracy: 0.9991 - val_loss: 0.1501 - val_accuracy: 0.9872\n",
            "Epoch 804/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.1722e-07 - accuracy: 1.0000 - val_loss: 0.1379 - val_accuracy: 0.9879\n",
            "Epoch 805/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.8823e-08 - accuracy: 1.0000 - val_loss: 0.1388 - val_accuracy: 0.9886\n",
            "Epoch 806/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.0490e-04 - accuracy: 0.9994 - val_loss: 0.1141 - val_accuracy: 0.9908\n",
            "Epoch 807/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.2044e-07 - accuracy: 1.0000 - val_loss: 0.1102 - val_accuracy: 0.9929\n",
            "Epoch 808/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0043 - accuracy: 0.9994 - val_loss: 0.1180 - val_accuracy: 0.9900\n",
            "Epoch 809/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0042 - accuracy: 0.9994 - val_loss: 0.1206 - val_accuracy: 0.9908\n",
            "Epoch 810/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.4027e-08 - accuracy: 1.0000 - val_loss: 0.1210 - val_accuracy: 0.9908\n",
            "Epoch 811/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0029 - accuracy: 0.9997 - val_loss: 0.1089 - val_accuracy: 0.9915\n",
            "Epoch 812/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0029 - accuracy: 0.9991 - val_loss: 0.1019 - val_accuracy: 0.9915\n",
            "Epoch 813/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.1524e-06 - accuracy: 1.0000 - val_loss: 0.1241 - val_accuracy: 0.9893\n",
            "Epoch 814/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0026 - accuracy: 0.9997 - val_loss: 0.0966 - val_accuracy: 0.9922\n",
            "Epoch 815/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.7219e-06 - accuracy: 1.0000 - val_loss: 0.1246 - val_accuracy: 0.9900\n",
            "Epoch 816/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0039 - accuracy: 0.9997 - val_loss: 0.0916 - val_accuracy: 0.9929\n",
            "Epoch 817/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.6599e-07 - accuracy: 1.0000 - val_loss: 0.1216 - val_accuracy: 0.9886\n",
            "Epoch 818/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.1943e-06 - accuracy: 1.0000 - val_loss: 0.0984 - val_accuracy: 0.9915\n",
            "Epoch 819/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.4079e-04 - accuracy: 0.9997 - val_loss: 0.1309 - val_accuracy: 0.9908\n",
            "Epoch 820/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0075 - accuracy: 0.9994 - val_loss: 0.1331 - val_accuracy: 0.9893\n",
            "Epoch 821/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0065 - accuracy: 0.9991 - val_loss: 0.1001 - val_accuracy: 0.9900\n",
            "Epoch 822/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.1947e-08 - accuracy: 1.0000 - val_loss: 0.1027 - val_accuracy: 0.9900\n",
            "Epoch 823/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0030 - accuracy: 0.9997 - val_loss: 0.1177 - val_accuracy: 0.9900\n",
            "Epoch 824/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.9206e-08 - accuracy: 1.0000 - val_loss: 0.1124 - val_accuracy: 0.9900\n",
            "Epoch 825/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.6836e-08 - accuracy: 1.0000 - val_loss: 0.1299 - val_accuracy: 0.9893\n",
            "Epoch 826/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0065 - accuracy: 0.9991 - val_loss: 0.1076 - val_accuracy: 0.9908\n",
            "Epoch 827/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.1141e-06 - accuracy: 1.0000 - val_loss: 0.1183 - val_accuracy: 0.9893\n",
            "Epoch 828/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0118 - accuracy: 0.9988 - val_loss: 0.1362 - val_accuracy: 0.9893\n",
            "Epoch 829/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.2729e-06 - accuracy: 1.0000 - val_loss: 0.1264 - val_accuracy: 0.9900\n",
            "Epoch 830/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.6347e-08 - accuracy: 1.0000 - val_loss: 0.1147 - val_accuracy: 0.9915\n",
            "Epoch 831/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.7704e-08 - accuracy: 1.0000 - val_loss: 0.1021 - val_accuracy: 0.9915\n",
            "Epoch 832/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0033 - accuracy: 0.9997 - val_loss: 0.1037 - val_accuracy: 0.9915\n",
            "Epoch 833/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0048 - accuracy: 0.9997 - val_loss: 0.1080 - val_accuracy: 0.9915\n",
            "Epoch 834/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.3103e-08 - accuracy: 1.0000 - val_loss: 0.1130 - val_accuracy: 0.9908\n",
            "Epoch 835/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0015 - accuracy: 0.9997 - val_loss: 0.1099 - val_accuracy: 0.9915\n",
            "Epoch 836/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0037 - accuracy: 0.9997 - val_loss: 0.1186 - val_accuracy: 0.9915\n",
            "Epoch 837/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0030 - accuracy: 0.9997 - val_loss: 0.1482 - val_accuracy: 0.9886\n",
            "Epoch 838/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0044 - accuracy: 0.9997 - val_loss: 0.1368 - val_accuracy: 0.9886\n",
            "Epoch 839/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0128 - accuracy: 0.9985 - val_loss: 0.1441 - val_accuracy: 0.9886\n",
            "Epoch 840/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.9074e-06 - accuracy: 1.0000 - val_loss: 0.1955 - val_accuracy: 0.9836\n",
            "Epoch 841/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0099 - accuracy: 0.9988 - val_loss: 0.1365 - val_accuracy: 0.9900\n",
            "Epoch 842/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.1629e-08 - accuracy: 1.0000 - val_loss: 0.1313 - val_accuracy: 0.9893\n",
            "Epoch 843/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0154 - accuracy: 0.9985 - val_loss: 0.1443 - val_accuracy: 0.9886\n",
            "Epoch 844/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.6729e-08 - accuracy: 1.0000 - val_loss: 0.1447 - val_accuracy: 0.9886\n",
            "Epoch 845/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0032 - accuracy: 0.9997 - val_loss: 0.1518 - val_accuracy: 0.9893\n",
            "Epoch 846/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.8877e-07 - accuracy: 1.0000 - val_loss: 0.1803 - val_accuracy: 0.9879\n",
            "Epoch 847/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0069 - accuracy: 0.9988 - val_loss: 0.1443 - val_accuracy: 0.9893\n",
            "Epoch 848/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0149 - accuracy: 0.9982 - val_loss: 0.1623 - val_accuracy: 0.9893\n",
            "Epoch 849/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0072 - accuracy: 0.9991 - val_loss: 0.1316 - val_accuracy: 0.9886\n",
            "Epoch 850/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.4086e-04 - accuracy: 0.9994 - val_loss: 0.1020 - val_accuracy: 0.9915\n",
            "Epoch 851/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0109 - accuracy: 0.9991 - val_loss: 0.1220 - val_accuracy: 0.9908\n",
            "Epoch 852/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.3295e-05 - accuracy: 1.0000 - val_loss: 0.1091 - val_accuracy: 0.9908\n",
            "Epoch 853/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.8428e-05 - accuracy: 1.0000 - val_loss: 0.1247 - val_accuracy: 0.9900\n",
            "Epoch 854/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0275 - accuracy: 0.9970 - val_loss: 0.1163 - val_accuracy: 0.9908\n",
            "Epoch 855/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0150 - accuracy: 0.9982 - val_loss: 0.1574 - val_accuracy: 0.9886\n",
            "Epoch 856/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0122 - accuracy: 0.9988 - val_loss: 0.1220 - val_accuracy: 0.9900\n",
            "Epoch 857/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0048 - accuracy: 0.9994 - val_loss: 0.0966 - val_accuracy: 0.9922\n",
            "Epoch 858/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0062 - accuracy: 0.9991 - val_loss: 0.0831 - val_accuracy: 0.9915\n",
            "Epoch 859/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.5697e-07 - accuracy: 1.0000 - val_loss: 0.1097 - val_accuracy: 0.9900\n",
            "Epoch 860/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0084 - accuracy: 0.9988 - val_loss: 0.1153 - val_accuracy: 0.9908\n",
            "Epoch 861/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0045 - accuracy: 0.9994 - val_loss: 0.0913 - val_accuracy: 0.9915\n",
            "Epoch 862/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0023 - accuracy: 0.9997 - val_loss: 0.1090 - val_accuracy: 0.9900\n",
            "Epoch 863/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0087 - accuracy: 0.9991 - val_loss: 0.0927 - val_accuracy: 0.9929\n",
            "Epoch 864/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0040 - accuracy: 0.9997 - val_loss: 0.1465 - val_accuracy: 0.9886\n",
            "Epoch 865/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.2518e-06 - accuracy: 1.0000 - val_loss: 0.1240 - val_accuracy: 0.9893\n",
            "Epoch 866/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0077 - accuracy: 0.9982 - val_loss: 0.1196 - val_accuracy: 0.9893\n",
            "Epoch 867/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.2255e-06 - accuracy: 1.0000 - val_loss: 0.1127 - val_accuracy: 0.9915\n",
            "Epoch 868/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0050 - accuracy: 0.9994 - val_loss: 0.0983 - val_accuracy: 0.9915\n",
            "Epoch 869/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0023 - accuracy: 0.9994 - val_loss: 0.1190 - val_accuracy: 0.9908\n",
            "Epoch 870/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0088 - accuracy: 0.9991 - val_loss: 0.0959 - val_accuracy: 0.9908\n",
            "Epoch 871/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.6400e-08 - accuracy: 1.0000 - val_loss: 0.0965 - val_accuracy: 0.9908\n",
            "Epoch 872/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.0827e-04 - accuracy: 1.0000 - val_loss: 0.1456 - val_accuracy: 0.9900\n",
            "Epoch 873/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0105 - accuracy: 0.9985 - val_loss: 0.1233 - val_accuracy: 0.9886\n",
            "Epoch 874/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0085 - accuracy: 0.9982 - val_loss: 0.1056 - val_accuracy: 0.9915\n",
            "Epoch 875/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.6664e-05 - accuracy: 1.0000 - val_loss: 0.1068 - val_accuracy: 0.9900\n",
            "Epoch 876/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.3905e-04 - accuracy: 0.9997 - val_loss: 0.1069 - val_accuracy: 0.9900\n",
            "Epoch 877/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0039 - accuracy: 0.9997 - val_loss: 0.1595 - val_accuracy: 0.9886\n",
            "Epoch 878/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0129 - accuracy: 0.9988 - val_loss: 0.1412 - val_accuracy: 0.9893\n",
            "Epoch 879/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.1242 - val_accuracy: 0.9893\n",
            "Epoch 880/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.3354e-08 - accuracy: 1.0000 - val_loss: 0.1171 - val_accuracy: 0.9893\n",
            "Epoch 881/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0043 - accuracy: 0.9994 - val_loss: 0.1259 - val_accuracy: 0.9893\n",
            "Epoch 882/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.5577e-05 - accuracy: 1.0000 - val_loss: 0.1255 - val_accuracy: 0.9886\n",
            "Epoch 883/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.9276e-05 - accuracy: 1.0000 - val_loss: 0.1253 - val_accuracy: 0.9893\n",
            "Epoch 884/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.0284e-08 - accuracy: 1.0000 - val_loss: 0.1160 - val_accuracy: 0.9908\n",
            "Epoch 885/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0052 - accuracy: 0.9991 - val_loss: 0.1271 - val_accuracy: 0.9900\n",
            "Epoch 886/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.4376e-07 - accuracy: 1.0000 - val_loss: 0.1247 - val_accuracy: 0.9893\n",
            "Epoch 887/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0022 - accuracy: 0.9997 - val_loss: 0.0860 - val_accuracy: 0.9936\n",
            "Epoch 888/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0085 - accuracy: 0.9985 - val_loss: 0.1384 - val_accuracy: 0.9900\n",
            "Epoch 889/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0052 - accuracy: 0.9991 - val_loss: 0.0953 - val_accuracy: 0.9908\n",
            "Epoch 890/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0012 - accuracy: 0.9997 - val_loss: 0.0999 - val_accuracy: 0.9922\n",
            "Epoch 891/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.6578e-07 - accuracy: 1.0000 - val_loss: 0.1016 - val_accuracy: 0.9915\n",
            "Epoch 892/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.7362e-07 - accuracy: 1.0000 - val_loss: 0.1305 - val_accuracy: 0.9893\n",
            "Epoch 893/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0023 - accuracy: 0.9997 - val_loss: 0.1105 - val_accuracy: 0.9893\n",
            "Epoch 894/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.7578e-08 - accuracy: 1.0000 - val_loss: 0.1111 - val_accuracy: 0.9900\n",
            "Epoch 895/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0063 - accuracy: 0.9994 - val_loss: 0.1162 - val_accuracy: 0.9915\n",
            "Epoch 896/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0068 - accuracy: 0.9994 - val_loss: 0.0827 - val_accuracy: 0.9943\n",
            "Epoch 897/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.4822e-08 - accuracy: 1.0000 - val_loss: 0.0875 - val_accuracy: 0.9922\n",
            "Epoch 898/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0030 - accuracy: 0.9991 - val_loss: 0.0975 - val_accuracy: 0.9915\n",
            "Epoch 899/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.0729e-07 - accuracy: 1.0000 - val_loss: 0.0980 - val_accuracy: 0.9908\n",
            "Epoch 900/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0032 - accuracy: 0.9997 - val_loss: 0.0897 - val_accuracy: 0.9922\n",
            "Epoch 901/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.7896e-08 - accuracy: 1.0000 - val_loss: 0.0887 - val_accuracy: 0.9915\n",
            "Epoch 902/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0092 - accuracy: 0.9985 - val_loss: 0.1093 - val_accuracy: 0.9915\n",
            "Epoch 903/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.9292e-07 - accuracy: 1.0000 - val_loss: 0.1064 - val_accuracy: 0.9915\n",
            "Epoch 904/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0050 - accuracy: 0.9991 - val_loss: 0.0824 - val_accuracy: 0.9936\n",
            "Epoch 905/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.3175e-07 - accuracy: 1.0000 - val_loss: 0.0873 - val_accuracy: 0.9922\n",
            "Epoch 906/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0045 - accuracy: 0.9994 - val_loss: 0.1279 - val_accuracy: 0.9900\n",
            "Epoch 907/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.5445e-07 - accuracy: 1.0000 - val_loss: 0.1175 - val_accuracy: 0.9900\n",
            "Epoch 908/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.4321e-08 - accuracy: 1.0000 - val_loss: 0.1150 - val_accuracy: 0.9900\n",
            "Epoch 909/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0048 - accuracy: 0.9997 - val_loss: 0.1021 - val_accuracy: 0.9922\n",
            "Epoch 910/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0054 - accuracy: 0.9991 - val_loss: 0.0895 - val_accuracy: 0.9929\n",
            "Epoch 911/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.8357e-08 - accuracy: 1.0000 - val_loss: 0.0904 - val_accuracy: 0.9936\n",
            "Epoch 912/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.7684e-08 - accuracy: 1.0000 - val_loss: 0.1133 - val_accuracy: 0.9908\n",
            "Epoch 913/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 0.0089 - accuracy: 0.9985 - val_loss: 0.1038 - val_accuracy: 0.9893\n",
            "Epoch 914/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.3017e-08 - accuracy: 1.0000 - val_loss: 0.1076 - val_accuracy: 0.9900\n",
            "Epoch 915/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 2.2452e-08 - accuracy: 1.0000 - val_loss: 0.1242 - val_accuracy: 0.9900\n",
            "Epoch 916/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.3356e-08 - accuracy: 1.0000 - val_loss: 0.1278 - val_accuracy: 0.9900\n",
            "Epoch 917/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.2610e-08 - accuracy: 1.0000 - val_loss: 0.1172 - val_accuracy: 0.9908\n",
            "Epoch 918/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.1818e-08 - accuracy: 1.0000 - val_loss: 0.1216 - val_accuracy: 0.9900\n",
            "Epoch 919/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.1472e-08 - accuracy: 1.0000 - val_loss: 0.1200 - val_accuracy: 0.9908\n",
            "Epoch 920/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.0947e-08 - accuracy: 1.0000 - val_loss: 0.1187 - val_accuracy: 0.9908\n",
            "Epoch 921/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.0554e-08 - accuracy: 1.0000 - val_loss: 0.1172 - val_accuracy: 0.9908\n",
            "Epoch 922/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 1.0255e-08 - accuracy: 1.0000 - val_loss: 0.1197 - val_accuracy: 0.9908\n",
            "Epoch 923/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.9319e-09 - accuracy: 1.0000 - val_loss: 0.1184 - val_accuracy: 0.9908\n",
            "Epoch 924/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.6755e-09 - accuracy: 1.0000 - val_loss: 0.1197 - val_accuracy: 0.9908\n",
            "Epoch 925/1056\n",
            "103/103 [==============================] - 0s 4ms/step - loss: 9.4089e-09 - accuracy: 1.0000 - val_loss: 0.1195 - val_accuracy: 0.9908\n",
            "Epoch 926/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 9.1776e-09 - accuracy: 1.0000 - val_loss: 0.1217 - val_accuracy: 0.9908\n",
            "Epoch 927/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.9602e-09 - accuracy: 1.0000 - val_loss: 0.1206 - val_accuracy: 0.9908\n",
            "Epoch 928/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.7427e-09 - accuracy: 1.0000 - val_loss: 0.1197 - val_accuracy: 0.9908\n",
            "Epoch 929/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.5749e-09 - accuracy: 1.0000 - val_loss: 0.1214 - val_accuracy: 0.9908\n",
            "Epoch 930/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.3686e-09 - accuracy: 1.0000 - val_loss: 0.1226 - val_accuracy: 0.9908\n",
            "Epoch 931/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.2212e-09 - accuracy: 1.0000 - val_loss: 0.1225 - val_accuracy: 0.9908\n",
            "Epoch 932/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 8.0557e-09 - accuracy: 1.0000 - val_loss: 0.1215 - val_accuracy: 0.9908\n",
            "Epoch 933/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.8870e-09 - accuracy: 1.0000 - val_loss: 0.1224 - val_accuracy: 0.9908\n",
            "Epoch 934/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.7485e-09 - accuracy: 1.0000 - val_loss: 0.1232 - val_accuracy: 0.9908\n",
            "Epoch 935/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.6155e-09 - accuracy: 1.0000 - val_loss: 0.1250 - val_accuracy: 0.9908\n",
            "Epoch 936/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.4860e-09 - accuracy: 1.0000 - val_loss: 0.1237 - val_accuracy: 0.9908\n",
            "Epoch 937/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.3587e-09 - accuracy: 1.0000 - val_loss: 0.1244 - val_accuracy: 0.9908\n",
            "Epoch 938/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.2422e-09 - accuracy: 1.0000 - val_loss: 0.1255 - val_accuracy: 0.9908\n",
            "Epoch 939/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.1367e-09 - accuracy: 1.0000 - val_loss: 0.1245 - val_accuracy: 0.9908\n",
            "Epoch 940/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 7.0233e-09 - accuracy: 1.0000 - val_loss: 0.1248 - val_accuracy: 0.9908\n",
            "Epoch 941/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.9198e-09 - accuracy: 1.0000 - val_loss: 0.1261 - val_accuracy: 0.9908\n",
            "Epoch 942/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.8181e-09 - accuracy: 1.0000 - val_loss: 0.1245 - val_accuracy: 0.9908\n",
            "Epoch 943/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.7349e-09 - accuracy: 1.0000 - val_loss: 0.1254 - val_accuracy: 0.9908\n",
            "Epoch 944/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.6339e-09 - accuracy: 1.0000 - val_loss: 0.1257 - val_accuracy: 0.9908\n",
            "Epoch 945/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.5479e-09 - accuracy: 1.0000 - val_loss: 0.1265 - val_accuracy: 0.9908\n",
            "Epoch 946/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.4595e-09 - accuracy: 1.0000 - val_loss: 0.1258 - val_accuracy: 0.9908\n",
            "Epoch 947/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.3844e-09 - accuracy: 1.0000 - val_loss: 0.1263 - val_accuracy: 0.9908\n",
            "Epoch 948/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.2980e-09 - accuracy: 1.0000 - val_loss: 0.1269 - val_accuracy: 0.9908\n",
            "Epoch 949/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.2252e-09 - accuracy: 1.0000 - val_loss: 0.1265 - val_accuracy: 0.9908\n",
            "Epoch 950/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.1615e-09 - accuracy: 1.0000 - val_loss: 0.1264 - val_accuracy: 0.9908\n",
            "Epoch 951/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.0874e-09 - accuracy: 1.0000 - val_loss: 0.1267 - val_accuracy: 0.9908\n",
            "Epoch 952/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 6.0217e-09 - accuracy: 1.0000 - val_loss: 0.1266 - val_accuracy: 0.9908\n",
            "Epoch 953/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.9586e-09 - accuracy: 1.0000 - val_loss: 0.1265 - val_accuracy: 0.9908\n",
            "Epoch 954/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.8969e-09 - accuracy: 1.0000 - val_loss: 0.1272 - val_accuracy: 0.9908\n",
            "Epoch 955/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.8386e-09 - accuracy: 1.0000 - val_loss: 0.1268 - val_accuracy: 0.9908\n",
            "Epoch 956/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.7809e-09 - accuracy: 1.0000 - val_loss: 0.1274 - val_accuracy: 0.9908\n",
            "Epoch 957/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.7237e-09 - accuracy: 1.0000 - val_loss: 0.1266 - val_accuracy: 0.9908\n",
            "Epoch 958/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.6696e-09 - accuracy: 1.0000 - val_loss: 0.1260 - val_accuracy: 0.9908\n",
            "Epoch 959/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.6165e-09 - accuracy: 1.0000 - val_loss: 0.1264 - val_accuracy: 0.9908\n",
            "Epoch 960/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.5680e-09 - accuracy: 1.0000 - val_loss: 0.1274 - val_accuracy: 0.9908\n",
            "Epoch 961/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.5179e-09 - accuracy: 1.0000 - val_loss: 0.1288 - val_accuracy: 0.9908\n",
            "Epoch 962/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.4736e-09 - accuracy: 1.0000 - val_loss: 0.1284 - val_accuracy: 0.9908\n",
            "Epoch 963/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.4276e-09 - accuracy: 1.0000 - val_loss: 0.1279 - val_accuracy: 0.9908\n",
            "Epoch 964/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.3801e-09 - accuracy: 1.0000 - val_loss: 0.1282 - val_accuracy: 0.9908\n",
            "Epoch 965/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.3426e-09 - accuracy: 1.0000 - val_loss: 0.1280 - val_accuracy: 0.9908\n",
            "Epoch 966/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.2972e-09 - accuracy: 1.0000 - val_loss: 0.1278 - val_accuracy: 0.9908\n",
            "Epoch 967/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.2585e-09 - accuracy: 1.0000 - val_loss: 0.1285 - val_accuracy: 0.9908\n",
            "Epoch 968/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.2166e-09 - accuracy: 1.0000 - val_loss: 0.1286 - val_accuracy: 0.9908\n",
            "Epoch 969/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.1756e-09 - accuracy: 1.0000 - val_loss: 0.1286 - val_accuracy: 0.9908\n",
            "Epoch 970/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.1388e-09 - accuracy: 1.0000 - val_loss: 0.1291 - val_accuracy: 0.9908\n",
            "Epoch 971/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.1012e-09 - accuracy: 1.0000 - val_loss: 0.1290 - val_accuracy: 0.9908\n",
            "Epoch 972/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.0705e-09 - accuracy: 1.0000 - val_loss: 0.1289 - val_accuracy: 0.9908\n",
            "Epoch 973/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 5.0358e-09 - accuracy: 1.0000 - val_loss: 0.1287 - val_accuracy: 0.9908\n",
            "Epoch 974/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.9982e-09 - accuracy: 1.0000 - val_loss: 0.1288 - val_accuracy: 0.9908\n",
            "Epoch 975/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.9658e-09 - accuracy: 1.0000 - val_loss: 0.1291 - val_accuracy: 0.9908\n",
            "Epoch 976/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.9327e-09 - accuracy: 1.0000 - val_loss: 0.1297 - val_accuracy: 0.9908\n",
            "Epoch 977/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.9020e-09 - accuracy: 1.0000 - val_loss: 0.1292 - val_accuracy: 0.9908\n",
            "Epoch 978/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.8706e-09 - accuracy: 1.0000 - val_loss: 0.1293 - val_accuracy: 0.9908\n",
            "Epoch 979/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.8402e-09 - accuracy: 1.0000 - val_loss: 0.1291 - val_accuracy: 0.9908\n",
            "Epoch 980/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.8106e-09 - accuracy: 1.0000 - val_loss: 0.1291 - val_accuracy: 0.9908\n",
            "Epoch 981/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.7833e-09 - accuracy: 1.0000 - val_loss: 0.1287 - val_accuracy: 0.9908\n",
            "Epoch 982/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.7542e-09 - accuracy: 1.0000 - val_loss: 0.1289 - val_accuracy: 0.9908\n",
            "Epoch 983/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.7274e-09 - accuracy: 1.0000 - val_loss: 0.1291 - val_accuracy: 0.9908\n",
            "Epoch 984/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.6955e-09 - accuracy: 1.0000 - val_loss: 0.1294 - val_accuracy: 0.9908\n",
            "Epoch 985/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.6714e-09 - accuracy: 1.0000 - val_loss: 0.1291 - val_accuracy: 0.9908\n",
            "Epoch 986/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.6439e-09 - accuracy: 1.0000 - val_loss: 0.1298 - val_accuracy: 0.9908\n",
            "Epoch 987/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.6228e-09 - accuracy: 1.0000 - val_loss: 0.1294 - val_accuracy: 0.9908\n",
            "Epoch 988/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.5957e-09 - accuracy: 1.0000 - val_loss: 0.1296 - val_accuracy: 0.9908\n",
            "Epoch 989/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.5725e-09 - accuracy: 1.0000 - val_loss: 0.1297 - val_accuracy: 0.9908\n",
            "Epoch 990/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.5489e-09 - accuracy: 1.0000 - val_loss: 0.1296 - val_accuracy: 0.9908\n",
            "Epoch 991/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.5254e-09 - accuracy: 1.0000 - val_loss: 0.1295 - val_accuracy: 0.9908\n",
            "Epoch 992/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.4981e-09 - accuracy: 1.0000 - val_loss: 0.1303 - val_accuracy: 0.9908\n",
            "Epoch 993/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.4797e-09 - accuracy: 1.0000 - val_loss: 0.1299 - val_accuracy: 0.9908\n",
            "Epoch 994/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.4604e-09 - accuracy: 1.0000 - val_loss: 0.1293 - val_accuracy: 0.9908\n",
            "Epoch 995/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.4351e-09 - accuracy: 1.0000 - val_loss: 0.1300 - val_accuracy: 0.9908\n",
            "Epoch 996/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.4140e-09 - accuracy: 1.0000 - val_loss: 0.1295 - val_accuracy: 0.9908\n",
            "Epoch 997/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.3953e-09 - accuracy: 1.0000 - val_loss: 0.1293 - val_accuracy: 0.9908\n",
            "Epoch 998/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.3744e-09 - accuracy: 1.0000 - val_loss: 0.1295 - val_accuracy: 0.9908\n",
            "Epoch 999/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.3535e-09 - accuracy: 1.0000 - val_loss: 0.1296 - val_accuracy: 0.9908\n",
            "Epoch 1000/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.3353e-09 - accuracy: 1.0000 - val_loss: 0.1294 - val_accuracy: 0.9908\n",
            "Epoch 1001/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.3128e-09 - accuracy: 1.0000 - val_loss: 0.1302 - val_accuracy: 0.9908\n",
            "Epoch 1002/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.2957e-09 - accuracy: 1.0000 - val_loss: 0.1303 - val_accuracy: 0.9908\n",
            "Epoch 1003/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.2793e-09 - accuracy: 1.0000 - val_loss: 0.1301 - val_accuracy: 0.9908\n",
            "Epoch 1004/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.2591e-09 - accuracy: 1.0000 - val_loss: 0.1309 - val_accuracy: 0.9908\n",
            "Epoch 1005/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.2454e-09 - accuracy: 1.0000 - val_loss: 0.1307 - val_accuracy: 0.9908\n",
            "Epoch 1006/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.2225e-09 - accuracy: 1.0000 - val_loss: 0.1303 - val_accuracy: 0.9908\n",
            "Epoch 1007/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.2071e-09 - accuracy: 1.0000 - val_loss: 0.1306 - val_accuracy: 0.9908\n",
            "Epoch 1008/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.1873e-09 - accuracy: 1.0000 - val_loss: 0.1303 - val_accuracy: 0.9908\n",
            "Epoch 1009/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.1719e-09 - accuracy: 1.0000 - val_loss: 0.1304 - val_accuracy: 0.9908\n",
            "Epoch 1010/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.1558e-09 - accuracy: 1.0000 - val_loss: 0.1303 - val_accuracy: 0.9908\n",
            "Epoch 1011/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.1398e-09 - accuracy: 1.0000 - val_loss: 0.1305 - val_accuracy: 0.9908\n",
            "Epoch 1012/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.1228e-09 - accuracy: 1.0000 - val_loss: 0.1306 - val_accuracy: 0.9908\n",
            "Epoch 1013/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.1077e-09 - accuracy: 1.0000 - val_loss: 0.1306 - val_accuracy: 0.9908\n",
            "Epoch 1014/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.0922e-09 - accuracy: 1.0000 - val_loss: 0.1308 - val_accuracy: 0.9908\n",
            "Epoch 1015/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.0765e-09 - accuracy: 1.0000 - val_loss: 0.1308 - val_accuracy: 0.9908\n",
            "Epoch 1016/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.0612e-09 - accuracy: 1.0000 - val_loss: 0.1308 - val_accuracy: 0.9908\n",
            "Epoch 1017/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.0437e-09 - accuracy: 1.0000 - val_loss: 0.1304 - val_accuracy: 0.9908\n",
            "Epoch 1018/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.0285e-09 - accuracy: 1.0000 - val_loss: 0.1309 - val_accuracy: 0.9908\n",
            "Epoch 1019/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 4.0140e-09 - accuracy: 1.0000 - val_loss: 0.1308 - val_accuracy: 0.9908\n",
            "Epoch 1020/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.9988e-09 - accuracy: 1.0000 - val_loss: 0.1307 - val_accuracy: 0.9908\n",
            "Epoch 1021/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.9842e-09 - accuracy: 1.0000 - val_loss: 0.1308 - val_accuracy: 0.9908\n",
            "Epoch 1022/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.9712e-09 - accuracy: 1.0000 - val_loss: 0.1310 - val_accuracy: 0.9908\n",
            "Epoch 1023/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.9574e-09 - accuracy: 1.0000 - val_loss: 0.1311 - val_accuracy: 0.9908\n",
            "Epoch 1024/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.9410e-09 - accuracy: 1.0000 - val_loss: 0.1307 - val_accuracy: 0.9908\n",
            "Epoch 1025/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.9277e-09 - accuracy: 1.0000 - val_loss: 0.1307 - val_accuracy: 0.9908\n",
            "Epoch 1026/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.9134e-09 - accuracy: 1.0000 - val_loss: 0.1312 - val_accuracy: 0.9908\n",
            "Epoch 1027/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.9005e-09 - accuracy: 1.0000 - val_loss: 0.1311 - val_accuracy: 0.9908\n",
            "Epoch 1028/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.8877e-09 - accuracy: 1.0000 - val_loss: 0.1314 - val_accuracy: 0.9908\n",
            "Epoch 1029/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.8770e-09 - accuracy: 1.0000 - val_loss: 0.1313 - val_accuracy: 0.9908\n",
            "Epoch 1030/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.8640e-09 - accuracy: 1.0000 - val_loss: 0.1314 - val_accuracy: 0.9908\n",
            "Epoch 1031/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.8515e-09 - accuracy: 1.0000 - val_loss: 0.1316 - val_accuracy: 0.9908\n",
            "Epoch 1032/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.8428e-09 - accuracy: 1.0000 - val_loss: 0.1321 - val_accuracy: 0.9908\n",
            "Epoch 1033/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.8289e-09 - accuracy: 1.0000 - val_loss: 0.1320 - val_accuracy: 0.9908\n",
            "Epoch 1034/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.8180e-09 - accuracy: 1.0000 - val_loss: 0.1315 - val_accuracy: 0.9908\n",
            "Epoch 1035/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.8064e-09 - accuracy: 1.0000 - val_loss: 0.1313 - val_accuracy: 0.9908\n",
            "Epoch 1036/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.7901e-09 - accuracy: 1.0000 - val_loss: 0.1318 - val_accuracy: 0.9908\n",
            "Epoch 1037/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.7842e-09 - accuracy: 1.0000 - val_loss: 0.1316 - val_accuracy: 0.9908\n",
            "Epoch 1038/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.7687e-09 - accuracy: 1.0000 - val_loss: 0.1319 - val_accuracy: 0.9908\n",
            "Epoch 1039/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.7600e-09 - accuracy: 1.0000 - val_loss: 0.1320 - val_accuracy: 0.9908\n",
            "Epoch 1040/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.7492e-09 - accuracy: 1.0000 - val_loss: 0.1318 - val_accuracy: 0.9908\n",
            "Epoch 1041/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.7402e-09 - accuracy: 1.0000 - val_loss: 0.1319 - val_accuracy: 0.9908\n",
            "Epoch 1042/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.7306e-09 - accuracy: 1.0000 - val_loss: 0.1314 - val_accuracy: 0.9908\n",
            "Epoch 1043/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.7174e-09 - accuracy: 1.0000 - val_loss: 0.1318 - val_accuracy: 0.9908\n",
            "Epoch 1044/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.7050e-09 - accuracy: 1.0000 - val_loss: 0.1317 - val_accuracy: 0.9908\n",
            "Epoch 1045/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.6958e-09 - accuracy: 1.0000 - val_loss: 0.1323 - val_accuracy: 0.9908\n",
            "Epoch 1046/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.6877e-09 - accuracy: 1.0000 - val_loss: 0.1322 - val_accuracy: 0.9908\n",
            "Epoch 1047/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.6750e-09 - accuracy: 1.0000 - val_loss: 0.1324 - val_accuracy: 0.9908\n",
            "Epoch 1048/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.6692e-09 - accuracy: 1.0000 - val_loss: 0.1327 - val_accuracy: 0.9908\n",
            "Epoch 1049/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.6575e-09 - accuracy: 1.0000 - val_loss: 0.1325 - val_accuracy: 0.9908\n",
            "Epoch 1050/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.6475e-09 - accuracy: 1.0000 - val_loss: 0.1324 - val_accuracy: 0.9908\n",
            "Epoch 1051/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.6356e-09 - accuracy: 1.0000 - val_loss: 0.1327 - val_accuracy: 0.9908\n",
            "Epoch 1052/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.6318e-09 - accuracy: 1.0000 - val_loss: 0.1325 - val_accuracy: 0.9908\n",
            "Epoch 1053/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.6173e-09 - accuracy: 1.0000 - val_loss: 0.1328 - val_accuracy: 0.9908\n",
            "Epoch 1054/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.6173e-09 - accuracy: 1.0000 - val_loss: 0.1326 - val_accuracy: 0.9908\n",
            "Epoch 1055/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.6025e-09 - accuracy: 1.0000 - val_loss: 0.1328 - val_accuracy: 0.9908\n",
            "Epoch 1056/1056\n",
            "103/103 [==============================] - 0s 2ms/step - loss: 3.5933e-09 - accuracy: 1.0000 - val_loss: 0.1328 - val_accuracy: 0.9908\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0Ox61tdZk49",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "294d1c43-006a-4f5b-c462-661573d9ea50"
      },
      "source": [
        "prediction = model.predict(XVALID)\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "accuracy = accuracy_score(YVALID, prediction.round())\n",
        "precision = precision_score(YVALID, prediction.round())\n",
        "recall = recall_score(YVALID, prediction.round())\n",
        "f1score = f1_score(YVALID, prediction.round())\n",
        "print(\"Multilayer NN Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
        "print(\"Precision: %.2f%%\" % (precision * 100.0))\n",
        "print(\"Recall: %.2f%%\" % (recall * 100.0))\n",
        "print(\"F1-score: %.2f\" % (f1score*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Multilayer NN Accuracy: 99.08%\n",
            "Precision: 97.37%\n",
            "Recall: 96.94%\n",
            "F1-score: 97.16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_PENgfn_Fuf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "85fcd505-9c3d-40bc-8eb3-dcdd54ddc67c"
      },
      "source": [
        "prediction=model.predict(XTRAIN)\n",
        "accuracy = accuracy_score(YTRAIN, prediction.round())\n",
        "precision = precision_score(YTRAIN, prediction.round())\n",
        "recall = recall_score(YTRAIN, prediction.round())\n",
        "f1score = f1_score(YTRAIN, prediction.round())\n",
        "print(\"Training Set Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
        "print(\"Precision: %.2f%%\" % (precision * 100.0))\n",
        "print(\"Recall: %.2f%%\" % (recall * 100.0))\n",
        "print(\"F1-score: %.2f\" % (f1score))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Set Accuracy: 100.00%\n",
            "Precision: 100.00%\n",
            "Recall: 100.00%\n",
            "F1-score: 1.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hoCrBUQ-lA5",
        "colab_type": "text"
      },
      "source": [
        "##Overfitting\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaOQEYir-qT_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5e471b42-4ca9-49d5-d790-4abeefa8e476"
      },
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "callback_a = ModelCheckpoint(filepath = 'Reduced_Nasa_asteroid.hdf5', monitor='val_loss', save_best_only = True, save_weights_only = True, verbose = 1)\n",
        "callback_b = EarlyStopping(monitor='val_loss', mode='min', patience=500, verbose=1)\n",
        "model = Sequential()\n",
        "model.add(Dense(24, input_dim = len(XTRAIN[0, :]), activation='sigmoid'))\n",
        "model.add(Dense(12, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer = 'rmsprop', metrics=['accuracy'])\n",
        "history = model.fit(XTRAIN, YTRAIN, validation_data=(XVALID, YVALID), epochs=3056, batch_size=4, callbacks = [callback_a, callback_b])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3056\n",
            "796/821 [============================>.] - ETA: 0s - loss: 0.3220 - accuracy: 0.8562\n",
            "Epoch 00001: val_loss improved from inf to 0.21908, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 2s 2ms/step - loss: 0.3195 - accuracy: 0.8574 - val_loss: 0.2191 - val_accuracy: 0.9175\n",
            "Epoch 2/3056\n",
            "803/821 [============================>.] - ETA: 0s - loss: 0.1818 - accuracy: 0.9256\n",
            "Epoch 00002: val_loss improved from 0.21908 to 0.15486, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.1822 - accuracy: 0.9247 - val_loss: 0.1549 - val_accuracy: 0.9403\n",
            "Epoch 3/3056\n",
            "810/821 [============================>.] - ETA: 0s - loss: 0.1330 - accuracy: 0.9438\n",
            "Epoch 00003: val_loss improved from 0.15486 to 0.12970, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.1320 - accuracy: 0.9445 - val_loss: 0.1297 - val_accuracy: 0.9488\n",
            "Epoch 4/3056\n",
            "804/821 [============================>.] - ETA: 0s - loss: 0.1122 - accuracy: 0.9562\n",
            "Epoch 00004: val_loss improved from 0.12970 to 0.11441, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.1128 - accuracy: 0.9552 - val_loss: 0.1144 - val_accuracy: 0.9552\n",
            "Epoch 5/3056\n",
            "803/821 [============================>.] - ETA: 0s - loss: 0.1027 - accuracy: 0.9552\n",
            "Epoch 00005: val_loss did not improve from 0.11441\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.1034 - accuracy: 0.9546 - val_loss: 0.1172 - val_accuracy: 0.9580\n",
            "Epoch 6/3056\n",
            "818/821 [============================>.] - ETA: 0s - loss: 0.1001 - accuracy: 0.9566\n",
            "Epoch 00006: val_loss improved from 0.11441 to 0.11411, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.1000 - accuracy: 0.9564 - val_loss: 0.1141 - val_accuracy: 0.9609\n",
            "Epoch 7/3056\n",
            "794/821 [============================>.] - ETA: 0s - loss: 0.0942 - accuracy: 0.9581\n",
            "Epoch 00007: val_loss did not improve from 0.11411\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0968 - accuracy: 0.9567 - val_loss: 0.1211 - val_accuracy: 0.9523\n",
            "Epoch 8/3056\n",
            "790/821 [===========================>..] - ETA: 0s - loss: 0.0958 - accuracy: 0.9570\n",
            "Epoch 00008: val_loss did not improve from 0.11411\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0956 - accuracy: 0.9576 - val_loss: 0.1156 - val_accuracy: 0.9616\n",
            "Epoch 9/3056\n",
            "809/821 [============================>.] - ETA: 0s - loss: 0.0946 - accuracy: 0.9592\n",
            "Epoch 00009: val_loss improved from 0.11411 to 0.11291, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0951 - accuracy: 0.9592 - val_loss: 0.1129 - val_accuracy: 0.9623\n",
            "Epoch 10/3056\n",
            "806/821 [============================>.] - ETA: 0s - loss: 0.0923 - accuracy: 0.9618\n",
            "Epoch 00010: val_loss did not improve from 0.11291\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0927 - accuracy: 0.9616 - val_loss: 0.1276 - val_accuracy: 0.9516\n",
            "Epoch 11/3056\n",
            "779/821 [===========================>..] - ETA: 0s - loss: 0.0916 - accuracy: 0.9599\n",
            "Epoch 00011: val_loss did not improve from 0.11291\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0915 - accuracy: 0.9595 - val_loss: 0.1148 - val_accuracy: 0.9630\n",
            "Epoch 12/3056\n",
            "806/821 [============================>.] - ETA: 0s - loss: 0.0896 - accuracy: 0.9615\n",
            "Epoch 00012: val_loss did not improve from 0.11291\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0896 - accuracy: 0.9616 - val_loss: 0.1249 - val_accuracy: 0.9609\n",
            "Epoch 13/3056\n",
            "808/821 [============================>.] - ETA: 0s - loss: 0.0910 - accuracy: 0.9619\n",
            "Epoch 00013: val_loss did not improve from 0.11291\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0905 - accuracy: 0.9622 - val_loss: 0.1135 - val_accuracy: 0.9616\n",
            "Epoch 14/3056\n",
            "815/821 [============================>.] - ETA: 0s - loss: 0.0891 - accuracy: 0.9620\n",
            "Epoch 00014: val_loss did not improve from 0.11291\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0889 - accuracy: 0.9619 - val_loss: 0.1164 - val_accuracy: 0.9580\n",
            "Epoch 15/3056\n",
            "778/821 [===========================>..] - ETA: 0s - loss: 0.0889 - accuracy: 0.9621\n",
            "Epoch 00015: val_loss did not improve from 0.11291\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0896 - accuracy: 0.9622 - val_loss: 0.1178 - val_accuracy: 0.9609\n",
            "Epoch 16/3056\n",
            "804/821 [============================>.] - ETA: 0s - loss: 0.0878 - accuracy: 0.9639\n",
            "Epoch 00016: val_loss did not improve from 0.11291\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0881 - accuracy: 0.9640 - val_loss: 0.1221 - val_accuracy: 0.9573\n",
            "Epoch 17/3056\n",
            "821/821 [==============================] - ETA: 0s - loss: 0.0868 - accuracy: 0.9631\n",
            "Epoch 00017: val_loss improved from 0.11291 to 0.10827, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0868 - accuracy: 0.9631 - val_loss: 0.1083 - val_accuracy: 0.9630\n",
            "Epoch 18/3056\n",
            "808/821 [============================>.] - ETA: 0s - loss: 0.0847 - accuracy: 0.9641\n",
            "Epoch 00018: val_loss improved from 0.10827 to 0.10475, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0849 - accuracy: 0.9640 - val_loss: 0.1047 - val_accuracy: 0.9609\n",
            "Epoch 19/3056\n",
            "811/821 [============================>.] - ETA: 0s - loss: 0.0837 - accuracy: 0.9655\n",
            "Epoch 00019: val_loss did not improve from 0.10475\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0833 - accuracy: 0.9656 - val_loss: 0.1162 - val_accuracy: 0.9644\n",
            "Epoch 20/3056\n",
            "820/821 [============================>.] - ETA: 0s - loss: 0.0827 - accuracy: 0.9668\n",
            "Epoch 00020: val_loss did not improve from 0.10475\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0826 - accuracy: 0.9668 - val_loss: 0.1139 - val_accuracy: 0.9616\n",
            "Epoch 21/3056\n",
            "780/821 [===========================>..] - ETA: 0s - loss: 0.0801 - accuracy: 0.9683\n",
            "Epoch 00021: val_loss did not improve from 0.10475\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0833 - accuracy: 0.9671 - val_loss: 0.1095 - val_accuracy: 0.9609\n",
            "Epoch 22/3056\n",
            "787/821 [===========================>..] - ETA: 0s - loss: 0.0827 - accuracy: 0.9654\n",
            "Epoch 00022: val_loss did not improve from 0.10475\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0821 - accuracy: 0.9656 - val_loss: 0.1129 - val_accuracy: 0.9680\n",
            "Epoch 23/3056\n",
            "784/821 [===========================>..] - ETA: 0s - loss: 0.0825 - accuracy: 0.9668\n",
            "Epoch 00023: val_loss did not improve from 0.10475\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0812 - accuracy: 0.9665 - val_loss: 0.1251 - val_accuracy: 0.9595\n",
            "Epoch 24/3056\n",
            "812/821 [============================>.] - ETA: 0s - loss: 0.0832 - accuracy: 0.9667\n",
            "Epoch 00024: val_loss did not improve from 0.10475\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0832 - accuracy: 0.9668 - val_loss: 0.1113 - val_accuracy: 0.9630\n",
            "Epoch 25/3056\n",
            "810/821 [============================>.] - ETA: 0s - loss: 0.0783 - accuracy: 0.9682\n",
            "Epoch 00025: val_loss did not improve from 0.10475\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0788 - accuracy: 0.9683 - val_loss: 0.1258 - val_accuracy: 0.9566\n",
            "Epoch 26/3056\n",
            "809/821 [============================>.] - ETA: 0s - loss: 0.0807 - accuracy: 0.9694\n",
            "Epoch 00026: val_loss did not improve from 0.10475\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0797 - accuracy: 0.9698 - val_loss: 0.1152 - val_accuracy: 0.9623\n",
            "Epoch 27/3056\n",
            "811/821 [============================>.] - ETA: 0s - loss: 0.0782 - accuracy: 0.9701\n",
            "Epoch 00027: val_loss did not improve from 0.10475\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0778 - accuracy: 0.9701 - val_loss: 0.1095 - val_accuracy: 0.9701\n",
            "Epoch 28/3056\n",
            "812/821 [============================>.] - ETA: 0s - loss: 0.0780 - accuracy: 0.9704\n",
            "Epoch 00028: val_loss did not improve from 0.10475\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0780 - accuracy: 0.9704 - val_loss: 0.1106 - val_accuracy: 0.9644\n",
            "Epoch 29/3056\n",
            "813/821 [============================>.] - ETA: 0s - loss: 0.0727 - accuracy: 0.9739\n",
            "Epoch 00029: val_loss did not improve from 0.10475\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0770 - accuracy: 0.9732 - val_loss: 0.1134 - val_accuracy: 0.9644\n",
            "Epoch 30/3056\n",
            "811/821 [============================>.] - ETA: 0s - loss: 0.0761 - accuracy: 0.9707\n",
            "Epoch 00030: val_loss did not improve from 0.10475\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0757 - accuracy: 0.9707 - val_loss: 0.1193 - val_accuracy: 0.9580\n",
            "Epoch 31/3056\n",
            "814/821 [============================>.] - ETA: 0s - loss: 0.0737 - accuracy: 0.9721\n",
            "Epoch 00031: val_loss improved from 0.10475 to 0.10352, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0733 - accuracy: 0.9723 - val_loss: 0.1035 - val_accuracy: 0.9651\n",
            "Epoch 32/3056\n",
            "817/821 [============================>.] - ETA: 0s - loss: 0.0754 - accuracy: 0.9734\n",
            "Epoch 00032: val_loss did not improve from 0.10352\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0756 - accuracy: 0.9732 - val_loss: 0.1175 - val_accuracy: 0.9616\n",
            "Epoch 33/3056\n",
            "810/821 [============================>.] - ETA: 0s - loss: 0.0729 - accuracy: 0.9728\n",
            "Epoch 00033: val_loss improved from 0.10352 to 0.10215, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0726 - accuracy: 0.9729 - val_loss: 0.1022 - val_accuracy: 0.9701\n",
            "Epoch 34/3056\n",
            "819/821 [============================>.] - ETA: 0s - loss: 0.0717 - accuracy: 0.9744\n",
            "Epoch 00034: val_loss did not improve from 0.10215\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0717 - accuracy: 0.9744 - val_loss: 0.1176 - val_accuracy: 0.9666\n",
            "Epoch 35/3056\n",
            "817/821 [============================>.] - ETA: 0s - loss: 0.0709 - accuracy: 0.9722\n",
            "Epoch 00035: val_loss did not improve from 0.10215\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0706 - accuracy: 0.9723 - val_loss: 0.1092 - val_accuracy: 0.9659\n",
            "Epoch 36/3056\n",
            "807/821 [============================>.] - ETA: 0s - loss: 0.0682 - accuracy: 0.9743\n",
            "Epoch 00036: val_loss improved from 0.10215 to 0.09915, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0691 - accuracy: 0.9744 - val_loss: 0.0991 - val_accuracy: 0.9780\n",
            "Epoch 37/3056\n",
            "808/821 [============================>.] - ETA: 0s - loss: 0.0668 - accuracy: 0.9762\n",
            "Epoch 00037: val_loss did not improve from 0.09915\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0663 - accuracy: 0.9762 - val_loss: 0.1264 - val_accuracy: 0.9637\n",
            "Epoch 38/3056\n",
            "818/821 [============================>.] - ETA: 0s - loss: 0.0682 - accuracy: 0.9759\n",
            "Epoch 00038: val_loss did not improve from 0.09915\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0680 - accuracy: 0.9759 - val_loss: 0.1120 - val_accuracy: 0.9651\n",
            "Epoch 39/3056\n",
            "814/821 [============================>.] - ETA: 0s - loss: 0.0646 - accuracy: 0.9770\n",
            "Epoch 00039: val_loss did not improve from 0.09915\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0653 - accuracy: 0.9765 - val_loss: 0.1017 - val_accuracy: 0.9787\n",
            "Epoch 40/3056\n",
            "782/821 [===========================>..] - ETA: 0s - loss: 0.0663 - accuracy: 0.9767\n",
            "Epoch 00040: val_loss did not improve from 0.09915\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0661 - accuracy: 0.9765 - val_loss: 0.1092 - val_accuracy: 0.9730\n",
            "Epoch 41/3056\n",
            "795/821 [============================>.] - ETA: 0s - loss: 0.0610 - accuracy: 0.9780\n",
            "Epoch 00041: val_loss did not improve from 0.09915\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0638 - accuracy: 0.9781 - val_loss: 0.1007 - val_accuracy: 0.9787\n",
            "Epoch 42/3056\n",
            "809/821 [============================>.] - ETA: 0s - loss: 0.0610 - accuracy: 0.9774\n",
            "Epoch 00042: val_loss did not improve from 0.09915\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0605 - accuracy: 0.9774 - val_loss: 0.1034 - val_accuracy: 0.9716\n",
            "Epoch 43/3056\n",
            "802/821 [============================>.] - ETA: 0s - loss: 0.0592 - accuracy: 0.9797\n",
            "Epoch 00043: val_loss did not improve from 0.09915\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0604 - accuracy: 0.9796 - val_loss: 0.1077 - val_accuracy: 0.9801\n",
            "Epoch 44/3056\n",
            "812/821 [============================>.] - ETA: 0s - loss: 0.0622 - accuracy: 0.9791\n",
            "Epoch 00044: val_loss did not improve from 0.09915\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0617 - accuracy: 0.9793 - val_loss: 0.1085 - val_accuracy: 0.9730\n",
            "Epoch 45/3056\n",
            "812/821 [============================>.] - ETA: 0s - loss: 0.0580 - accuracy: 0.9806\n",
            "Epoch 00045: val_loss improved from 0.09915 to 0.09789, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0575 - accuracy: 0.9808 - val_loss: 0.0979 - val_accuracy: 0.9794\n",
            "Epoch 46/3056\n",
            "783/821 [===========================>..] - ETA: 0s - loss: 0.0581 - accuracy: 0.9773\n",
            "Epoch 00046: val_loss did not improve from 0.09789\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0589 - accuracy: 0.9768 - val_loss: 0.1043 - val_accuracy: 0.9801\n",
            "Epoch 47/3056\n",
            "799/821 [============================>.] - ETA: 0s - loss: 0.0560 - accuracy: 0.9825\n",
            "Epoch 00047: val_loss improved from 0.09789 to 0.09780, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0567 - accuracy: 0.9817 - val_loss: 0.0978 - val_accuracy: 0.9815\n",
            "Epoch 48/3056\n",
            "807/821 [============================>.] - ETA: 0s - loss: 0.0550 - accuracy: 0.9830\n",
            "Epoch 00048: val_loss did not improve from 0.09780\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0548 - accuracy: 0.9829 - val_loss: 0.1048 - val_accuracy: 0.9723\n",
            "Epoch 49/3056\n",
            "814/821 [============================>.] - ETA: 0s - loss: 0.0542 - accuracy: 0.9807\n",
            "Epoch 00049: val_loss did not improve from 0.09780\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0540 - accuracy: 0.9808 - val_loss: 0.1044 - val_accuracy: 0.9765\n",
            "Epoch 50/3056\n",
            "816/821 [============================>.] - ETA: 0s - loss: 0.0512 - accuracy: 0.9816\n",
            "Epoch 00050: val_loss did not improve from 0.09780\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0511 - accuracy: 0.9817 - val_loss: 0.1159 - val_accuracy: 0.9708\n",
            "Epoch 51/3056\n",
            "815/821 [============================>.] - ETA: 0s - loss: 0.0538 - accuracy: 0.9819\n",
            "Epoch 00051: val_loss did not improve from 0.09780\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0551 - accuracy: 0.9817 - val_loss: 0.1034 - val_accuracy: 0.9815\n",
            "Epoch 52/3056\n",
            "810/821 [============================>.] - ETA: 0s - loss: 0.0493 - accuracy: 0.9855\n",
            "Epoch 00052: val_loss did not improve from 0.09780\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0534 - accuracy: 0.9848 - val_loss: 0.1161 - val_accuracy: 0.9723\n",
            "Epoch 53/3056\n",
            "803/821 [============================>.] - ETA: 0s - loss: 0.0475 - accuracy: 0.9851\n",
            "Epoch 00053: val_loss did not improve from 0.09780\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0498 - accuracy: 0.9848 - val_loss: 0.1016 - val_accuracy: 0.9794\n",
            "Epoch 54/3056\n",
            "816/821 [============================>.] - ETA: 0s - loss: 0.0481 - accuracy: 0.9835\n",
            "Epoch 00054: val_loss did not improve from 0.09780\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0503 - accuracy: 0.9832 - val_loss: 0.1108 - val_accuracy: 0.9737\n",
            "Epoch 55/3056\n",
            "802/821 [============================>.] - ETA: 0s - loss: 0.0505 - accuracy: 0.9819\n",
            "Epoch 00055: val_loss did not improve from 0.09780\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0496 - accuracy: 0.9823 - val_loss: 0.1004 - val_accuracy: 0.9844\n",
            "Epoch 56/3056\n",
            "811/821 [============================>.] - ETA: 0s - loss: 0.0485 - accuracy: 0.9849\n",
            "Epoch 00056: val_loss improved from 0.09780 to 0.09774, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0484 - accuracy: 0.9848 - val_loss: 0.0977 - val_accuracy: 0.9844\n",
            "Epoch 57/3056\n",
            "803/821 [============================>.] - ETA: 0s - loss: 0.0461 - accuracy: 0.9857\n",
            "Epoch 00057: val_loss improved from 0.09774 to 0.09126, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0463 - accuracy: 0.9854 - val_loss: 0.0913 - val_accuracy: 0.9865\n",
            "Epoch 58/3056\n",
            "808/821 [============================>.] - ETA: 0s - loss: 0.0457 - accuracy: 0.9851\n",
            "Epoch 00058: val_loss did not improve from 0.09126\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0451 - accuracy: 0.9854 - val_loss: 0.0974 - val_accuracy: 0.9822\n",
            "Epoch 59/3056\n",
            "815/821 [============================>.] - ETA: 0s - loss: 0.0441 - accuracy: 0.9837\n",
            "Epoch 00059: val_loss did not improve from 0.09126\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0438 - accuracy: 0.9838 - val_loss: 0.0967 - val_accuracy: 0.9829\n",
            "Epoch 60/3056\n",
            "796/821 [============================>.] - ETA: 0s - loss: 0.0430 - accuracy: 0.9865\n",
            "Epoch 00060: val_loss did not improve from 0.09126\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0421 - accuracy: 0.9869 - val_loss: 0.1018 - val_accuracy: 0.9851\n",
            "Epoch 61/3056\n",
            "806/821 [============================>.] - ETA: 0s - loss: 0.0443 - accuracy: 0.9854\n",
            "Epoch 00061: val_loss did not improve from 0.09126\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0442 - accuracy: 0.9854 - val_loss: 0.0929 - val_accuracy: 0.9858\n",
            "Epoch 62/3056\n",
            "803/821 [============================>.] - ETA: 0s - loss: 0.0401 - accuracy: 0.9875\n",
            "Epoch 00062: val_loss did not improve from 0.09126\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0433 - accuracy: 0.9872 - val_loss: 0.1065 - val_accuracy: 0.9822\n",
            "Epoch 63/3056\n",
            "801/821 [============================>.] - ETA: 0s - loss: 0.0387 - accuracy: 0.9844\n",
            "Epoch 00063: val_loss did not improve from 0.09126\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0403 - accuracy: 0.9845 - val_loss: 0.1238 - val_accuracy: 0.9737\n",
            "Epoch 64/3056\n",
            "793/821 [===========================>..] - ETA: 0s - loss: 0.0427 - accuracy: 0.9846\n",
            "Epoch 00064: val_loss did not improve from 0.09126\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0421 - accuracy: 0.9848 - val_loss: 0.0984 - val_accuracy: 0.9851\n",
            "Epoch 65/3056\n",
            "792/821 [===========================>..] - ETA: 0s - loss: 0.0404 - accuracy: 0.9886\n",
            "Epoch 00065: val_loss did not improve from 0.09126\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0416 - accuracy: 0.9884 - val_loss: 0.0959 - val_accuracy: 0.9879\n",
            "Epoch 66/3056\n",
            "798/821 [============================>.] - ETA: 0s - loss: 0.0354 - accuracy: 0.9881\n",
            "Epoch 00066: val_loss did not improve from 0.09126\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0374 - accuracy: 0.9878 - val_loss: 0.0957 - val_accuracy: 0.9851\n",
            "Epoch 67/3056\n",
            "806/821 [============================>.] - ETA: 0s - loss: 0.0379 - accuracy: 0.9879\n",
            "Epoch 00067: val_loss did not improve from 0.09126\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0381 - accuracy: 0.9878 - val_loss: 0.1004 - val_accuracy: 0.9844\n",
            "Epoch 68/3056\n",
            "805/821 [============================>.] - ETA: 0s - loss: 0.0401 - accuracy: 0.9882\n",
            "Epoch 00068: val_loss did not improve from 0.09126\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0394 - accuracy: 0.9884 - val_loss: 0.0979 - val_accuracy: 0.9865\n",
            "Epoch 69/3056\n",
            "814/821 [============================>.] - ETA: 0s - loss: 0.0384 - accuracy: 0.9874\n",
            "Epoch 00069: val_loss did not improve from 0.09126\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0381 - accuracy: 0.9875 - val_loss: 0.0915 - val_accuracy: 0.9872\n",
            "Epoch 70/3056\n",
            "813/821 [============================>.] - ETA: 0s - loss: 0.0363 - accuracy: 0.9899\n",
            "Epoch 00070: val_loss did not improve from 0.09126\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0370 - accuracy: 0.9896 - val_loss: 0.0943 - val_accuracy: 0.9879\n",
            "Epoch 71/3056\n",
            "815/821 [============================>.] - ETA: 0s - loss: 0.0376 - accuracy: 0.9902\n",
            "Epoch 00071: val_loss did not improve from 0.09126\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0373 - accuracy: 0.9902 - val_loss: 0.1042 - val_accuracy: 0.9815\n",
            "Epoch 72/3056\n",
            "793/821 [===========================>..] - ETA: 0s - loss: 0.0379 - accuracy: 0.9877\n",
            "Epoch 00072: val_loss did not improve from 0.09126\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0374 - accuracy: 0.9878 - val_loss: 0.0993 - val_accuracy: 0.9858\n",
            "Epoch 73/3056\n",
            "783/821 [===========================>..] - ETA: 0s - loss: 0.0382 - accuracy: 0.9888\n",
            "Epoch 00073: val_loss improved from 0.09126 to 0.09107, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0365 - accuracy: 0.9893 - val_loss: 0.0911 - val_accuracy: 0.9879\n",
            "Epoch 74/3056\n",
            "790/821 [===========================>..] - ETA: 0s - loss: 0.0361 - accuracy: 0.9877\n",
            "Epoch 00074: val_loss did not improve from 0.09107\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0350 - accuracy: 0.9881 - val_loss: 0.0933 - val_accuracy: 0.9858\n",
            "Epoch 75/3056\n",
            "800/821 [============================>.] - ETA: 0s - loss: 0.0361 - accuracy: 0.9875\n",
            "Epoch 00075: val_loss did not improve from 0.09107\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0353 - accuracy: 0.9878 - val_loss: 0.0962 - val_accuracy: 0.9858\n",
            "Epoch 76/3056\n",
            "808/821 [============================>.] - ETA: 0s - loss: 0.0343 - accuracy: 0.9892\n",
            "Epoch 00076: val_loss did not improve from 0.09107\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0338 - accuracy: 0.9893 - val_loss: 0.0925 - val_accuracy: 0.9865\n",
            "Epoch 77/3056\n",
            "800/821 [============================>.] - ETA: 0s - loss: 0.0342 - accuracy: 0.9881\n",
            "Epoch 00077: val_loss did not improve from 0.09107\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0339 - accuracy: 0.9881 - val_loss: 0.0963 - val_accuracy: 0.9879\n",
            "Epoch 78/3056\n",
            "793/821 [===========================>..] - ETA: 0s - loss: 0.0318 - accuracy: 0.9896\n",
            "Epoch 00078: val_loss did not improve from 0.09107\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0324 - accuracy: 0.9890 - val_loss: 0.1006 - val_accuracy: 0.9844\n",
            "Epoch 79/3056\n",
            "804/821 [============================>.] - ETA: 0s - loss: 0.0353 - accuracy: 0.9907\n",
            "Epoch 00079: val_loss improved from 0.09107 to 0.08966, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0352 - accuracy: 0.9906 - val_loss: 0.0897 - val_accuracy: 0.9858\n",
            "Epoch 80/3056\n",
            "803/821 [============================>.] - ETA: 0s - loss: 0.0319 - accuracy: 0.9894\n",
            "Epoch 00080: val_loss did not improve from 0.08966\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0347 - accuracy: 0.9893 - val_loss: 0.0918 - val_accuracy: 0.9865\n",
            "Epoch 81/3056\n",
            "808/821 [============================>.] - ETA: 0s - loss: 0.0321 - accuracy: 0.9898\n",
            "Epoch 00081: val_loss did not improve from 0.08966\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0316 - accuracy: 0.9899 - val_loss: 0.1047 - val_accuracy: 0.9872\n",
            "Epoch 82/3056\n",
            "817/821 [============================>.] - ETA: 0s - loss: 0.0332 - accuracy: 0.9896\n",
            "Epoch 00082: val_loss did not improve from 0.08966\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0331 - accuracy: 0.9896 - val_loss: 0.0952 - val_accuracy: 0.9858\n",
            "Epoch 83/3056\n",
            "809/821 [============================>.] - ETA: 0s - loss: 0.0336 - accuracy: 0.9907\n",
            "Epoch 00083: val_loss did not improve from 0.08966\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0332 - accuracy: 0.9909 - val_loss: 0.1011 - val_accuracy: 0.9865\n",
            "Epoch 84/3056\n",
            "810/821 [============================>.] - ETA: 0s - loss: 0.0315 - accuracy: 0.9907\n",
            "Epoch 00084: val_loss did not improve from 0.08966\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0323 - accuracy: 0.9906 - val_loss: 0.0949 - val_accuracy: 0.9851\n",
            "Epoch 85/3056\n",
            "816/821 [============================>.] - ETA: 0s - loss: 0.0324 - accuracy: 0.9899\n",
            "Epoch 00085: val_loss did not improve from 0.08966\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0322 - accuracy: 0.9899 - val_loss: 0.0949 - val_accuracy: 0.9872\n",
            "Epoch 86/3056\n",
            "802/821 [============================>.] - ETA: 0s - loss: 0.0326 - accuracy: 0.9906\n",
            "Epoch 00086: val_loss did not improve from 0.08966\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0320 - accuracy: 0.9909 - val_loss: 0.1066 - val_accuracy: 0.9844\n",
            "Epoch 87/3056\n",
            "814/821 [============================>.] - ETA: 0s - loss: 0.0312 - accuracy: 0.9889\n",
            "Epoch 00087: val_loss did not improve from 0.08966\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0316 - accuracy: 0.9887 - val_loss: 0.0923 - val_accuracy: 0.9865\n",
            "Epoch 88/3056\n",
            "819/821 [============================>.] - ETA: 0s - loss: 0.0278 - accuracy: 0.9911\n",
            "Epoch 00088: val_loss did not improve from 0.08966\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0278 - accuracy: 0.9912 - val_loss: 0.0932 - val_accuracy: 0.9858\n",
            "Epoch 89/3056\n",
            "791/821 [===========================>..] - ETA: 0s - loss: 0.0292 - accuracy: 0.9912\n",
            "Epoch 00089: val_loss did not improve from 0.08966\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0295 - accuracy: 0.9909 - val_loss: 0.0927 - val_accuracy: 0.9879\n",
            "Epoch 90/3056\n",
            "809/821 [============================>.] - ETA: 0s - loss: 0.0299 - accuracy: 0.9920\n",
            "Epoch 00090: val_loss did not improve from 0.08966\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0295 - accuracy: 0.9921 - val_loss: 0.0989 - val_accuracy: 0.9872\n",
            "Epoch 91/3056\n",
            "814/821 [============================>.] - ETA: 0s - loss: 0.0300 - accuracy: 0.9905\n",
            "Epoch 00091: val_loss did not improve from 0.08966\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0298 - accuracy: 0.9906 - val_loss: 0.0964 - val_accuracy: 0.9872\n",
            "Epoch 92/3056\n",
            "811/821 [============================>.] - ETA: 0s - loss: 0.0302 - accuracy: 0.9908\n",
            "Epoch 00092: val_loss did not improve from 0.08966\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0299 - accuracy: 0.9909 - val_loss: 0.0961 - val_accuracy: 0.9872\n",
            "Epoch 93/3056\n",
            "811/821 [============================>.] - ETA: 0s - loss: 0.0280 - accuracy: 0.9908\n",
            "Epoch 00093: val_loss did not improve from 0.08966\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0277 - accuracy: 0.9909 - val_loss: 0.1030 - val_accuracy: 0.9879\n",
            "Epoch 94/3056\n",
            "819/821 [============================>.] - ETA: 0s - loss: 0.0297 - accuracy: 0.9915\n",
            "Epoch 00094: val_loss did not improve from 0.08966\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0297 - accuracy: 0.9915 - val_loss: 0.0988 - val_accuracy: 0.9879\n",
            "Epoch 95/3056\n",
            "807/821 [============================>.] - ETA: 0s - loss: 0.0290 - accuracy: 0.9916\n",
            "Epoch 00095: val_loss did not improve from 0.08966\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0289 - accuracy: 0.9915 - val_loss: 0.0972 - val_accuracy: 0.9865\n",
            "Epoch 96/3056\n",
            "802/821 [============================>.] - ETA: 0s - loss: 0.0296 - accuracy: 0.9919\n",
            "Epoch 00096: val_loss did not improve from 0.08966\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0297 - accuracy: 0.9918 - val_loss: 0.0994 - val_accuracy: 0.9879\n",
            "Epoch 97/3056\n",
            "798/821 [============================>.] - ETA: 0s - loss: 0.0280 - accuracy: 0.9915\n",
            "Epoch 00097: val_loss did not improve from 0.08966\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0274 - accuracy: 0.9918 - val_loss: 0.1030 - val_accuracy: 0.9858\n",
            "Epoch 98/3056\n",
            "821/821 [==============================] - ETA: 0s - loss: 0.0296 - accuracy: 0.9915\n",
            "Epoch 00098: val_loss did not improve from 0.08966\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0296 - accuracy: 0.9915 - val_loss: 0.1013 - val_accuracy: 0.9879\n",
            "Epoch 99/3056\n",
            "777/821 [===========================>..] - ETA: 0s - loss: 0.0295 - accuracy: 0.9910\n",
            "Epoch 00099: val_loss did not improve from 0.08966\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0290 - accuracy: 0.9909 - val_loss: 0.1004 - val_accuracy: 0.9851\n",
            "Epoch 100/3056\n",
            "816/821 [============================>.] - ETA: 0s - loss: 0.0283 - accuracy: 0.9905\n",
            "Epoch 00100: val_loss did not improve from 0.08966\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0282 - accuracy: 0.9906 - val_loss: 0.1034 - val_accuracy: 0.9865\n",
            "Epoch 101/3056\n",
            "816/821 [============================>.] - ETA: 0s - loss: 0.0290 - accuracy: 0.9917\n",
            "Epoch 00101: val_loss did not improve from 0.08966\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0293 - accuracy: 0.9915 - val_loss: 0.0994 - val_accuracy: 0.9851\n",
            "Epoch 102/3056\n",
            "815/821 [============================>.] - ETA: 0s - loss: 0.0260 - accuracy: 0.9926\n",
            "Epoch 00102: val_loss did not improve from 0.08966\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0259 - accuracy: 0.9927 - val_loss: 0.0954 - val_accuracy: 0.9851\n",
            "Epoch 103/3056\n",
            "812/821 [============================>.] - ETA: 0s - loss: 0.0257 - accuracy: 0.9935\n",
            "Epoch 00103: val_loss did not improve from 0.08966\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0271 - accuracy: 0.9930 - val_loss: 0.0967 - val_accuracy: 0.9879\n",
            "Epoch 104/3056\n",
            "809/821 [============================>.] - ETA: 0s - loss: 0.0272 - accuracy: 0.9917\n",
            "Epoch 00104: val_loss did not improve from 0.08966\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0269 - accuracy: 0.9918 - val_loss: 0.0968 - val_accuracy: 0.9893\n",
            "Epoch 105/3056\n",
            "808/821 [============================>.] - ETA: 0s - loss: 0.0267 - accuracy: 0.9929\n",
            "Epoch 00105: val_loss did not improve from 0.08966\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0263 - accuracy: 0.9930 - val_loss: 0.0988 - val_accuracy: 0.9879\n",
            "Epoch 106/3056\n",
            "802/821 [============================>.] - ETA: 0s - loss: 0.0262 - accuracy: 0.9925\n",
            "Epoch 00106: val_loss did not improve from 0.08966\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0262 - accuracy: 0.9921 - val_loss: 0.0911 - val_accuracy: 0.9879\n",
            "Epoch 107/3056\n",
            "783/821 [===========================>..] - ETA: 0s - loss: 0.0261 - accuracy: 0.9920\n",
            "Epoch 00107: val_loss improved from 0.08966 to 0.08765, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0263 - accuracy: 0.9921 - val_loss: 0.0877 - val_accuracy: 0.9872\n",
            "Epoch 108/3056\n",
            "813/821 [============================>.] - ETA: 0s - loss: 0.0256 - accuracy: 0.9920\n",
            "Epoch 00108: val_loss did not improve from 0.08765\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0254 - accuracy: 0.9921 - val_loss: 0.0956 - val_accuracy: 0.9886\n",
            "Epoch 109/3056\n",
            "788/821 [===========================>..] - ETA: 0s - loss: 0.0270 - accuracy: 0.9918\n",
            "Epoch 00109: val_loss did not improve from 0.08765\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0269 - accuracy: 0.9912 - val_loss: 0.0884 - val_accuracy: 0.9872\n",
            "Epoch 110/3056\n",
            "815/821 [============================>.] - ETA: 0s - loss: 0.0272 - accuracy: 0.9914\n",
            "Epoch 00110: val_loss did not improve from 0.08765\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0270 - accuracy: 0.9915 - val_loss: 0.0919 - val_accuracy: 0.9886\n",
            "Epoch 111/3056\n",
            "812/821 [============================>.] - ETA: 0s - loss: 0.0281 - accuracy: 0.9926\n",
            "Epoch 00111: val_loss did not improve from 0.08765\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0282 - accuracy: 0.9924 - val_loss: 0.0922 - val_accuracy: 0.9879\n",
            "Epoch 112/3056\n",
            "789/821 [===========================>..] - ETA: 0s - loss: 0.0260 - accuracy: 0.9940\n",
            "Epoch 00112: val_loss did not improve from 0.08765\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0265 - accuracy: 0.9936 - val_loss: 0.1054 - val_accuracy: 0.9851\n",
            "Epoch 113/3056\n",
            "807/821 [============================>.] - ETA: 0s - loss: 0.0272 - accuracy: 0.9926\n",
            "Epoch 00113: val_loss did not improve from 0.08765\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0271 - accuracy: 0.9924 - val_loss: 0.0900 - val_accuracy: 0.9872\n",
            "Epoch 114/3056\n",
            "806/821 [============================>.] - ETA: 0s - loss: 0.0271 - accuracy: 0.9916\n",
            "Epoch 00114: val_loss did not improve from 0.08765\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0267 - accuracy: 0.9918 - val_loss: 0.0900 - val_accuracy: 0.9886\n",
            "Epoch 115/3056\n",
            "816/821 [============================>.] - ETA: 0s - loss: 0.0256 - accuracy: 0.9923\n",
            "Epoch 00115: val_loss did not improve from 0.08765\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0263 - accuracy: 0.9921 - val_loss: 0.0949 - val_accuracy: 0.9865\n",
            "Epoch 116/3056\n",
            "814/821 [============================>.] - ETA: 0s - loss: 0.0266 - accuracy: 0.9923\n",
            "Epoch 00116: val_loss did not improve from 0.08765\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0264 - accuracy: 0.9924 - val_loss: 0.0920 - val_accuracy: 0.9872\n",
            "Epoch 117/3056\n",
            "819/821 [============================>.] - ETA: 0s - loss: 0.0239 - accuracy: 0.9939\n",
            "Epoch 00117: val_loss improved from 0.08765 to 0.08511, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0239 - accuracy: 0.9939 - val_loss: 0.0851 - val_accuracy: 0.9886\n",
            "Epoch 118/3056\n",
            "821/821 [==============================] - ETA: 0s - loss: 0.0244 - accuracy: 0.9924\n",
            "Epoch 00118: val_loss did not improve from 0.08511\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0244 - accuracy: 0.9924 - val_loss: 0.0896 - val_accuracy: 0.9858\n",
            "Epoch 119/3056\n",
            "818/821 [============================>.] - ETA: 0s - loss: 0.0253 - accuracy: 0.9930\n",
            "Epoch 00119: val_loss did not improve from 0.08511\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0253 - accuracy: 0.9930 - val_loss: 0.0919 - val_accuracy: 0.9872\n",
            "Epoch 120/3056\n",
            "808/821 [============================>.] - ETA: 0s - loss: 0.0269 - accuracy: 0.9926\n",
            "Epoch 00120: val_loss did not improve from 0.08511\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0265 - accuracy: 0.9927 - val_loss: 0.0859 - val_accuracy: 0.9879\n",
            "Epoch 121/3056\n",
            "803/821 [============================>.] - ETA: 0s - loss: 0.0271 - accuracy: 0.9928\n",
            "Epoch 00121: val_loss did not improve from 0.08511\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0267 - accuracy: 0.9930 - val_loss: 0.0862 - val_accuracy: 0.9858\n",
            "Epoch 122/3056\n",
            "814/821 [============================>.] - ETA: 0s - loss: 0.0246 - accuracy: 0.9942\n",
            "Epoch 00122: val_loss improved from 0.08511 to 0.08328, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0244 - accuracy: 0.9942 - val_loss: 0.0833 - val_accuracy: 0.9886\n",
            "Epoch 123/3056\n",
            "796/821 [============================>.] - ETA: 0s - loss: 0.0249 - accuracy: 0.9931\n",
            "Epoch 00123: val_loss improved from 0.08328 to 0.07700, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0251 - accuracy: 0.9930 - val_loss: 0.0770 - val_accuracy: 0.9886\n",
            "Epoch 124/3056\n",
            "798/821 [============================>.] - ETA: 0s - loss: 0.0225 - accuracy: 0.9925\n",
            "Epoch 00124: val_loss improved from 0.07700 to 0.07665, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0219 - accuracy: 0.9927 - val_loss: 0.0766 - val_accuracy: 0.9886\n",
            "Epoch 125/3056\n",
            "785/821 [===========================>..] - ETA: 0s - loss: 0.0221 - accuracy: 0.9939\n",
            "Epoch 00125: val_loss did not improve from 0.07665\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0217 - accuracy: 0.9939 - val_loss: 0.0792 - val_accuracy: 0.9865\n",
            "Epoch 126/3056\n",
            "791/821 [===========================>..] - ETA: 0s - loss: 0.0259 - accuracy: 0.9934\n",
            "Epoch 00126: val_loss improved from 0.07665 to 0.07414, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0251 - accuracy: 0.9936 - val_loss: 0.0741 - val_accuracy: 0.9865\n",
            "Epoch 127/3056\n",
            "820/821 [============================>.] - ETA: 0s - loss: 0.0236 - accuracy: 0.9930\n",
            "Epoch 00127: val_loss did not improve from 0.07414\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0236 - accuracy: 0.9930 - val_loss: 0.0758 - val_accuracy: 0.9879\n",
            "Epoch 128/3056\n",
            "809/821 [============================>.] - ETA: 0s - loss: 0.0244 - accuracy: 0.9929\n",
            "Epoch 00128: val_loss improved from 0.07414 to 0.07404, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0240 - accuracy: 0.9930 - val_loss: 0.0740 - val_accuracy: 0.9893\n",
            "Epoch 129/3056\n",
            "806/821 [============================>.] - ETA: 0s - loss: 0.0238 - accuracy: 0.9932\n",
            "Epoch 00129: val_loss improved from 0.07404 to 0.07277, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0239 - accuracy: 0.9930 - val_loss: 0.0728 - val_accuracy: 0.9879\n",
            "Epoch 130/3056\n",
            "810/821 [============================>.] - ETA: 0s - loss: 0.0238 - accuracy: 0.9929\n",
            "Epoch 00130: val_loss did not improve from 0.07277\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0235 - accuracy: 0.9930 - val_loss: 0.0730 - val_accuracy: 0.9872\n",
            "Epoch 131/3056\n",
            "793/821 [===========================>..] - ETA: 0s - loss: 0.0229 - accuracy: 0.9940\n",
            "Epoch 00131: val_loss did not improve from 0.07277\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0229 - accuracy: 0.9939 - val_loss: 0.0771 - val_accuracy: 0.9865\n",
            "Epoch 132/3056\n",
            "802/821 [============================>.] - ETA: 0s - loss: 0.0209 - accuracy: 0.9931\n",
            "Epoch 00132: val_loss improved from 0.07277 to 0.07116, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0220 - accuracy: 0.9927 - val_loss: 0.0712 - val_accuracy: 0.9872\n",
            "Epoch 133/3056\n",
            "789/821 [===========================>..] - ETA: 0s - loss: 0.0212 - accuracy: 0.9937\n",
            "Epoch 00133: val_loss did not improve from 0.07116\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0205 - accuracy: 0.9939 - val_loss: 0.0731 - val_accuracy: 0.9844\n",
            "Epoch 134/3056\n",
            "812/821 [============================>.] - ETA: 0s - loss: 0.0231 - accuracy: 0.9938\n",
            "Epoch 00134: val_loss improved from 0.07116 to 0.06927, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0229 - accuracy: 0.9939 - val_loss: 0.0693 - val_accuracy: 0.9872\n",
            "Epoch 135/3056\n",
            "811/821 [============================>.] - ETA: 0s - loss: 0.0197 - accuracy: 0.9938\n",
            "Epoch 00135: val_loss improved from 0.06927 to 0.06875, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0214 - accuracy: 0.9933 - val_loss: 0.0688 - val_accuracy: 0.9886\n",
            "Epoch 136/3056\n",
            "814/821 [============================>.] - ETA: 0s - loss: 0.0206 - accuracy: 0.9923\n",
            "Epoch 00136: val_loss improved from 0.06875 to 0.06260, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0205 - accuracy: 0.9924 - val_loss: 0.0626 - val_accuracy: 0.9879\n",
            "Epoch 137/3056\n",
            "794/821 [============================>.] - ETA: 0s - loss: 0.0236 - accuracy: 0.9931\n",
            "Epoch 00137: val_loss did not improve from 0.06260\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0229 - accuracy: 0.9933 - val_loss: 0.0663 - val_accuracy: 0.9872\n",
            "Epoch 138/3056\n",
            "801/821 [============================>.] - ETA: 0s - loss: 0.0212 - accuracy: 0.9931\n",
            "Epoch 00138: val_loss did not improve from 0.06260\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0207 - accuracy: 0.9933 - val_loss: 0.0676 - val_accuracy: 0.9886\n",
            "Epoch 139/3056\n",
            "811/821 [============================>.] - ETA: 0s - loss: 0.0222 - accuracy: 0.9932\n",
            "Epoch 00139: val_loss did not improve from 0.06260\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0224 - accuracy: 0.9930 - val_loss: 0.0712 - val_accuracy: 0.9865\n",
            "Epoch 140/3056\n",
            "821/821 [==============================] - ETA: 0s - loss: 0.0195 - accuracy: 0.9939\n",
            "Epoch 00140: val_loss did not improve from 0.06260\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0195 - accuracy: 0.9939 - val_loss: 0.0719 - val_accuracy: 0.9865\n",
            "Epoch 141/3056\n",
            "815/821 [============================>.] - ETA: 0s - loss: 0.0239 - accuracy: 0.9929\n",
            "Epoch 00141: val_loss did not improve from 0.06260\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0237 - accuracy: 0.9930 - val_loss: 0.0658 - val_accuracy: 0.9900\n",
            "Epoch 142/3056\n",
            "805/821 [============================>.] - ETA: 0s - loss: 0.0215 - accuracy: 0.9935\n",
            "Epoch 00142: val_loss did not improve from 0.06260\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0211 - accuracy: 0.9936 - val_loss: 0.0708 - val_accuracy: 0.9865\n",
            "Epoch 143/3056\n",
            "803/821 [============================>.] - ETA: 0s - loss: 0.0226 - accuracy: 0.9932\n",
            "Epoch 00143: val_loss did not improve from 0.06260\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0224 - accuracy: 0.9930 - val_loss: 0.0635 - val_accuracy: 0.9865\n",
            "Epoch 144/3056\n",
            "816/821 [============================>.] - ETA: 0s - loss: 0.0209 - accuracy: 0.9933\n",
            "Epoch 00144: val_loss did not improve from 0.06260\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0208 - accuracy: 0.9933 - val_loss: 0.0706 - val_accuracy: 0.9865\n",
            "Epoch 145/3056\n",
            "815/821 [============================>.] - ETA: 0s - loss: 0.0200 - accuracy: 0.9942\n",
            "Epoch 00145: val_loss did not improve from 0.06260\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0199 - accuracy: 0.9942 - val_loss: 0.0663 - val_accuracy: 0.9865\n",
            "Epoch 146/3056\n",
            "804/821 [============================>.] - ETA: 0s - loss: 0.0206 - accuracy: 0.9941\n",
            "Epoch 00146: val_loss improved from 0.06260 to 0.05755, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0202 - accuracy: 0.9942 - val_loss: 0.0576 - val_accuracy: 0.9886\n",
            "Epoch 147/3056\n",
            "806/821 [============================>.] - ETA: 0s - loss: 0.0222 - accuracy: 0.9935\n",
            "Epoch 00147: val_loss did not improve from 0.05755\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0218 - accuracy: 0.9936 - val_loss: 0.0639 - val_accuracy: 0.9872\n",
            "Epoch 148/3056\n",
            "817/821 [============================>.] - ETA: 0s - loss: 0.0184 - accuracy: 0.9948\n",
            "Epoch 00148: val_loss did not improve from 0.05755\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0189 - accuracy: 0.9945 - val_loss: 0.0611 - val_accuracy: 0.9872\n",
            "Epoch 149/3056\n",
            "808/821 [============================>.] - ETA: 0s - loss: 0.0194 - accuracy: 0.9947\n",
            "Epoch 00149: val_loss did not improve from 0.05755\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0192 - accuracy: 0.9948 - val_loss: 0.0582 - val_accuracy: 0.9851\n",
            "Epoch 150/3056\n",
            "792/821 [===========================>..] - ETA: 0s - loss: 0.0210 - accuracy: 0.9946\n",
            "Epoch 00150: val_loss did not improve from 0.05755\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0203 - accuracy: 0.9948 - val_loss: 0.0654 - val_accuracy: 0.9851\n",
            "Epoch 151/3056\n",
            "792/821 [===========================>..] - ETA: 0s - loss: 0.0198 - accuracy: 0.9943\n",
            "Epoch 00151: val_loss improved from 0.05755 to 0.05525, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0210 - accuracy: 0.9939 - val_loss: 0.0553 - val_accuracy: 0.9879\n",
            "Epoch 152/3056\n",
            "800/821 [============================>.] - ETA: 0s - loss: 0.0193 - accuracy: 0.9944\n",
            "Epoch 00152: val_loss did not improve from 0.05525\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0191 - accuracy: 0.9945 - val_loss: 0.0622 - val_accuracy: 0.9865\n",
            "Epoch 153/3056\n",
            "793/821 [===========================>..] - ETA: 0s - loss: 0.0163 - accuracy: 0.9943\n",
            "Epoch 00153: val_loss did not improve from 0.05525\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0203 - accuracy: 0.9939 - val_loss: 0.0565 - val_accuracy: 0.9908\n",
            "Epoch 154/3056\n",
            "821/821 [==============================] - ETA: 0s - loss: 0.0193 - accuracy: 0.9933\n",
            "Epoch 00154: val_loss did not improve from 0.05525\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0193 - accuracy: 0.9933 - val_loss: 0.0595 - val_accuracy: 0.9879\n",
            "Epoch 155/3056\n",
            "793/821 [===========================>..] - ETA: 0s - loss: 0.0205 - accuracy: 0.9937\n",
            "Epoch 00155: val_loss did not improve from 0.05525\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0198 - accuracy: 0.9939 - val_loss: 0.0559 - val_accuracy: 0.9865\n",
            "Epoch 156/3056\n",
            "782/821 [===========================>..] - ETA: 0s - loss: 0.0181 - accuracy: 0.9946\n",
            "Epoch 00156: val_loss did not improve from 0.05525\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0208 - accuracy: 0.9939 - val_loss: 0.0615 - val_accuracy: 0.9858\n",
            "Epoch 157/3056\n",
            "816/821 [============================>.] - ETA: 0s - loss: 0.0209 - accuracy: 0.9942\n",
            "Epoch 00157: val_loss improved from 0.05525 to 0.05493, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0208 - accuracy: 0.9942 - val_loss: 0.0549 - val_accuracy: 0.9879\n",
            "Epoch 158/3056\n",
            "779/821 [===========================>..] - ETA: 0s - loss: 0.0207 - accuracy: 0.9939\n",
            "Epoch 00158: val_loss did not improve from 0.05493\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0200 - accuracy: 0.9942 - val_loss: 0.0618 - val_accuracy: 0.9872\n",
            "Epoch 159/3056\n",
            "782/821 [===========================>..] - ETA: 0s - loss: 0.0188 - accuracy: 0.9936\n",
            "Epoch 00159: val_loss improved from 0.05493 to 0.05210, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0211 - accuracy: 0.9933 - val_loss: 0.0521 - val_accuracy: 0.9879\n",
            "Epoch 160/3056\n",
            "785/821 [===========================>..] - ETA: 0s - loss: 0.0194 - accuracy: 0.9936\n",
            "Epoch 00160: val_loss improved from 0.05210 to 0.05167, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0212 - accuracy: 0.9936 - val_loss: 0.0517 - val_accuracy: 0.9900\n",
            "Epoch 161/3056\n",
            "814/821 [============================>.] - ETA: 0s - loss: 0.0180 - accuracy: 0.9951\n",
            "Epoch 00161: val_loss did not improve from 0.05167\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0179 - accuracy: 0.9951 - val_loss: 0.0571 - val_accuracy: 0.9858\n",
            "Epoch 162/3056\n",
            "808/821 [============================>.] - ETA: 0s - loss: 0.0193 - accuracy: 0.9941\n",
            "Epoch 00162: val_loss did not improve from 0.05167\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0191 - accuracy: 0.9942 - val_loss: 0.0537 - val_accuracy: 0.9879\n",
            "Epoch 163/3056\n",
            "780/821 [===========================>..] - ETA: 0s - loss: 0.0197 - accuracy: 0.9949\n",
            "Epoch 00163: val_loss improved from 0.05167 to 0.04948, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0208 - accuracy: 0.9948 - val_loss: 0.0495 - val_accuracy: 0.9865\n",
            "Epoch 164/3056\n",
            "801/821 [============================>.] - ETA: 0s - loss: 0.0215 - accuracy: 0.9938\n",
            "Epoch 00164: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0210 - accuracy: 0.9939 - val_loss: 0.0517 - val_accuracy: 0.9908\n",
            "Epoch 165/3056\n",
            "812/821 [============================>.] - ETA: 0s - loss: 0.0190 - accuracy: 0.9945\n",
            "Epoch 00165: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0188 - accuracy: 0.9945 - val_loss: 0.0610 - val_accuracy: 0.9829\n",
            "Epoch 166/3056\n",
            "811/821 [============================>.] - ETA: 0s - loss: 0.0192 - accuracy: 0.9941\n",
            "Epoch 00166: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0190 - accuracy: 0.9942 - val_loss: 0.0570 - val_accuracy: 0.9865\n",
            "Epoch 167/3056\n",
            "784/821 [===========================>..] - ETA: 0s - loss: 0.0186 - accuracy: 0.9946\n",
            "Epoch 00167: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0204 - accuracy: 0.9945 - val_loss: 0.0570 - val_accuracy: 0.9851\n",
            "Epoch 168/3056\n",
            "810/821 [============================>.] - ETA: 0s - loss: 0.0204 - accuracy: 0.9944\n",
            "Epoch 00168: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0202 - accuracy: 0.9945 - val_loss: 0.0625 - val_accuracy: 0.9851\n",
            "Epoch 169/3056\n",
            "820/821 [============================>.] - ETA: 0s - loss: 0.0192 - accuracy: 0.9951\n",
            "Epoch 00169: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0192 - accuracy: 0.9951 - val_loss: 0.0613 - val_accuracy: 0.9851\n",
            "Epoch 170/3056\n",
            "821/821 [==============================] - ETA: 0s - loss: 0.0186 - accuracy: 0.9945\n",
            "Epoch 00170: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0186 - accuracy: 0.9945 - val_loss: 0.0522 - val_accuracy: 0.9886\n",
            "Epoch 171/3056\n",
            "807/821 [============================>.] - ETA: 0s - loss: 0.0184 - accuracy: 0.9954\n",
            "Epoch 00171: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0181 - accuracy: 0.9954 - val_loss: 0.0586 - val_accuracy: 0.9879\n",
            "Epoch 172/3056\n",
            "810/821 [============================>.] - ETA: 0s - loss: 0.0176 - accuracy: 0.9944\n",
            "Epoch 00172: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0174 - accuracy: 0.9945 - val_loss: 0.0538 - val_accuracy: 0.9851\n",
            "Epoch 173/3056\n",
            "813/821 [============================>.] - ETA: 0s - loss: 0.0202 - accuracy: 0.9945\n",
            "Epoch 00173: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0200 - accuracy: 0.9945 - val_loss: 0.0530 - val_accuracy: 0.9865\n",
            "Epoch 174/3056\n",
            "819/821 [============================>.] - ETA: 0s - loss: 0.0182 - accuracy: 0.9942\n",
            "Epoch 00174: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0182 - accuracy: 0.9942 - val_loss: 0.0575 - val_accuracy: 0.9886\n",
            "Epoch 175/3056\n",
            "812/821 [============================>.] - ETA: 0s - loss: 0.0198 - accuracy: 0.9948\n",
            "Epoch 00175: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0196 - accuracy: 0.9948 - val_loss: 0.0555 - val_accuracy: 0.9886\n",
            "Epoch 176/3056\n",
            "806/821 [============================>.] - ETA: 0s - loss: 0.0198 - accuracy: 0.9941\n",
            "Epoch 00176: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0214 - accuracy: 0.9939 - val_loss: 0.0534 - val_accuracy: 0.9893\n",
            "Epoch 177/3056\n",
            "813/821 [============================>.] - ETA: 0s - loss: 0.0197 - accuracy: 0.9954\n",
            "Epoch 00177: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0195 - accuracy: 0.9954 - val_loss: 0.0547 - val_accuracy: 0.9886\n",
            "Epoch 178/3056\n",
            "815/821 [============================>.] - ETA: 0s - loss: 0.0189 - accuracy: 0.9942\n",
            "Epoch 00178: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0189 - accuracy: 0.9942 - val_loss: 0.0546 - val_accuracy: 0.9893\n",
            "Epoch 179/3056\n",
            "817/821 [============================>.] - ETA: 0s - loss: 0.0189 - accuracy: 0.9957\n",
            "Epoch 00179: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0188 - accuracy: 0.9957 - val_loss: 0.0676 - val_accuracy: 0.9844\n",
            "Epoch 180/3056\n",
            "805/821 [============================>.] - ETA: 0s - loss: 0.0189 - accuracy: 0.9947\n",
            "Epoch 00180: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0186 - accuracy: 0.9948 - val_loss: 0.0658 - val_accuracy: 0.9865\n",
            "Epoch 181/3056\n",
            "810/821 [============================>.] - ETA: 0s - loss: 0.0200 - accuracy: 0.9957\n",
            "Epoch 00181: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0198 - accuracy: 0.9957 - val_loss: 0.0568 - val_accuracy: 0.9886\n",
            "Epoch 182/3056\n",
            "818/821 [============================>.] - ETA: 0s - loss: 0.0171 - accuracy: 0.9954\n",
            "Epoch 00182: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0171 - accuracy: 0.9954 - val_loss: 0.0572 - val_accuracy: 0.9908\n",
            "Epoch 183/3056\n",
            "778/821 [===========================>..] - ETA: 0s - loss: 0.0176 - accuracy: 0.9955\n",
            "Epoch 00183: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0174 - accuracy: 0.9954 - val_loss: 0.0594 - val_accuracy: 0.9879\n",
            "Epoch 184/3056\n",
            "803/821 [============================>.] - ETA: 0s - loss: 0.0188 - accuracy: 0.9953\n",
            "Epoch 00184: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0188 - accuracy: 0.9951 - val_loss: 0.0524 - val_accuracy: 0.9886\n",
            "Epoch 185/3056\n",
            "813/821 [============================>.] - ETA: 0s - loss: 0.0193 - accuracy: 0.9938\n",
            "Epoch 00185: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0191 - accuracy: 0.9939 - val_loss: 0.0538 - val_accuracy: 0.9886\n",
            "Epoch 186/3056\n",
            "806/821 [============================>.] - ETA: 0s - loss: 0.0182 - accuracy: 0.9960\n",
            "Epoch 00186: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0182 - accuracy: 0.9957 - val_loss: 0.0537 - val_accuracy: 0.9900\n",
            "Epoch 187/3056\n",
            "810/821 [============================>.] - ETA: 0s - loss: 0.0192 - accuracy: 0.9957\n",
            "Epoch 00187: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0190 - accuracy: 0.9957 - val_loss: 0.0557 - val_accuracy: 0.9893\n",
            "Epoch 188/3056\n",
            "811/821 [============================>.] - ETA: 0s - loss: 0.0192 - accuracy: 0.9951\n",
            "Epoch 00188: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0190 - accuracy: 0.9951 - val_loss: 0.0577 - val_accuracy: 0.9900\n",
            "Epoch 189/3056\n",
            "814/821 [============================>.] - ETA: 0s - loss: 0.0195 - accuracy: 0.9951\n",
            "Epoch 00189: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0193 - accuracy: 0.9951 - val_loss: 0.0524 - val_accuracy: 0.9886\n",
            "Epoch 190/3056\n",
            "795/821 [============================>.] - ETA: 0s - loss: 0.0179 - accuracy: 0.9956\n",
            "Epoch 00190: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0180 - accuracy: 0.9954 - val_loss: 0.0559 - val_accuracy: 0.9893\n",
            "Epoch 191/3056\n",
            "797/821 [============================>.] - ETA: 0s - loss: 0.0180 - accuracy: 0.9956\n",
            "Epoch 00191: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0211 - accuracy: 0.9951 - val_loss: 0.0544 - val_accuracy: 0.9886\n",
            "Epoch 192/3056\n",
            "819/821 [============================>.] - ETA: 0s - loss: 0.0191 - accuracy: 0.9957\n",
            "Epoch 00192: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0190 - accuracy: 0.9957 - val_loss: 0.0529 - val_accuracy: 0.9886\n",
            "Epoch 193/3056\n",
            "807/821 [============================>.] - ETA: 0s - loss: 0.0197 - accuracy: 0.9954\n",
            "Epoch 00193: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0194 - accuracy: 0.9954 - val_loss: 0.0603 - val_accuracy: 0.9872\n",
            "Epoch 194/3056\n",
            "815/821 [============================>.] - ETA: 0s - loss: 0.0181 - accuracy: 0.9948\n",
            "Epoch 00194: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0180 - accuracy: 0.9948 - val_loss: 0.0577 - val_accuracy: 0.9865\n",
            "Epoch 195/3056\n",
            "814/821 [============================>.] - ETA: 0s - loss: 0.0197 - accuracy: 0.9945\n",
            "Epoch 00195: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0195 - accuracy: 0.9945 - val_loss: 0.0609 - val_accuracy: 0.9872\n",
            "Epoch 196/3056\n",
            "800/821 [============================>.] - ETA: 0s - loss: 0.0212 - accuracy: 0.9947\n",
            "Epoch 00196: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0207 - accuracy: 0.9948 - val_loss: 0.0596 - val_accuracy: 0.9886\n",
            "Epoch 197/3056\n",
            "798/821 [============================>.] - ETA: 0s - loss: 0.0211 - accuracy: 0.9956\n",
            "Epoch 00197: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0205 - accuracy: 0.9957 - val_loss: 0.0711 - val_accuracy: 0.9858\n",
            "Epoch 198/3056\n",
            "803/821 [============================>.] - ETA: 0s - loss: 0.0195 - accuracy: 0.9953\n",
            "Epoch 00198: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0203 - accuracy: 0.9951 - val_loss: 0.0692 - val_accuracy: 0.9851\n",
            "Epoch 199/3056\n",
            "807/821 [============================>.] - ETA: 0s - loss: 0.0200 - accuracy: 0.9957\n",
            "Epoch 00199: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0199 - accuracy: 0.9957 - val_loss: 0.0590 - val_accuracy: 0.9893\n",
            "Epoch 200/3056\n",
            "815/821 [============================>.] - ETA: 0s - loss: 0.0219 - accuracy: 0.9948\n",
            "Epoch 00200: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0217 - accuracy: 0.9948 - val_loss: 0.0641 - val_accuracy: 0.9851\n",
            "Epoch 201/3056\n",
            "814/821 [============================>.] - ETA: 0s - loss: 0.0186 - accuracy: 0.9954\n",
            "Epoch 00201: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0184 - accuracy: 0.9954 - val_loss: 0.0620 - val_accuracy: 0.9886\n",
            "Epoch 202/3056\n",
            "807/821 [============================>.] - ETA: 0s - loss: 0.0193 - accuracy: 0.9960\n",
            "Epoch 00202: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0190 - accuracy: 0.9960 - val_loss: 0.0628 - val_accuracy: 0.9865\n",
            "Epoch 203/3056\n",
            "820/821 [============================>.] - ETA: 0s - loss: 0.0173 - accuracy: 0.9954\n",
            "Epoch 00203: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0173 - accuracy: 0.9954 - val_loss: 0.0559 - val_accuracy: 0.9893\n",
            "Epoch 204/3056\n",
            "795/821 [============================>.] - ETA: 0s - loss: 0.0171 - accuracy: 0.9953\n",
            "Epoch 00204: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0166 - accuracy: 0.9954 - val_loss: 0.0638 - val_accuracy: 0.9858\n",
            "Epoch 205/3056\n",
            "812/821 [============================>.] - ETA: 0s - loss: 0.0137 - accuracy: 0.9963\n",
            "Epoch 00205: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0184 - accuracy: 0.9960 - val_loss: 0.0767 - val_accuracy: 0.9822\n",
            "Epoch 206/3056\n",
            "787/821 [===========================>..] - ETA: 0s - loss: 0.0195 - accuracy: 0.9952\n",
            "Epoch 00206: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0187 - accuracy: 0.9954 - val_loss: 0.0696 - val_accuracy: 0.9851\n",
            "Epoch 207/3056\n",
            "821/821 [==============================] - ETA: 0s - loss: 0.0192 - accuracy: 0.9942\n",
            "Epoch 00207: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0192 - accuracy: 0.9942 - val_loss: 0.0606 - val_accuracy: 0.9872\n",
            "Epoch 208/3056\n",
            "817/821 [============================>.] - ETA: 0s - loss: 0.0157 - accuracy: 0.9951\n",
            "Epoch 00208: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0161 - accuracy: 0.9948 - val_loss: 0.0803 - val_accuracy: 0.9836\n",
            "Epoch 209/3056\n",
            "813/821 [============================>.] - ETA: 0s - loss: 0.0179 - accuracy: 0.9954\n",
            "Epoch 00209: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0177 - accuracy: 0.9954 - val_loss: 0.0719 - val_accuracy: 0.9836\n",
            "Epoch 210/3056\n",
            "809/821 [============================>.] - ETA: 0s - loss: 0.0165 - accuracy: 0.9954\n",
            "Epoch 00210: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0168 - accuracy: 0.9951 - val_loss: 0.0630 - val_accuracy: 0.9879\n",
            "Epoch 211/3056\n",
            "816/821 [============================>.] - ETA: 0s - loss: 0.0185 - accuracy: 0.9951\n",
            "Epoch 00211: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0184 - accuracy: 0.9951 - val_loss: 0.0664 - val_accuracy: 0.9872\n",
            "Epoch 212/3056\n",
            "816/821 [============================>.] - ETA: 0s - loss: 0.0201 - accuracy: 0.9963\n",
            "Epoch 00212: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0199 - accuracy: 0.9963 - val_loss: 0.0725 - val_accuracy: 0.9851\n",
            "Epoch 213/3056\n",
            "821/821 [==============================] - ETA: 0s - loss: 0.0180 - accuracy: 0.9954\n",
            "Epoch 00213: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0180 - accuracy: 0.9954 - val_loss: 0.0641 - val_accuracy: 0.9872\n",
            "Epoch 214/3056\n",
            "816/821 [============================>.] - ETA: 0s - loss: 0.0184 - accuracy: 0.9960\n",
            "Epoch 00214: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0183 - accuracy: 0.9960 - val_loss: 0.0601 - val_accuracy: 0.9900\n",
            "Epoch 215/3056\n",
            "781/821 [===========================>..] - ETA: 0s - loss: 0.0169 - accuracy: 0.9952\n",
            "Epoch 00215: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0166 - accuracy: 0.9951 - val_loss: 0.0608 - val_accuracy: 0.9879\n",
            "Epoch 216/3056\n",
            "810/821 [============================>.] - ETA: 0s - loss: 0.0188 - accuracy: 0.9960\n",
            "Epoch 00216: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0186 - accuracy: 0.9960 - val_loss: 0.0770 - val_accuracy: 0.9844\n",
            "Epoch 217/3056\n",
            "800/821 [============================>.] - ETA: 0s - loss: 0.0197 - accuracy: 0.9947\n",
            "Epoch 00217: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0192 - accuracy: 0.9948 - val_loss: 0.0669 - val_accuracy: 0.9851\n",
            "Epoch 218/3056\n",
            "815/821 [============================>.] - ETA: 0s - loss: 0.0190 - accuracy: 0.9957\n",
            "Epoch 00218: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0188 - accuracy: 0.9957 - val_loss: 0.0663 - val_accuracy: 0.9872\n",
            "Epoch 219/3056\n",
            "800/821 [============================>.] - ETA: 0s - loss: 0.0183 - accuracy: 0.9959\n",
            "Epoch 00219: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0179 - accuracy: 0.9960 - val_loss: 0.0652 - val_accuracy: 0.9865\n",
            "Epoch 220/3056\n",
            "784/821 [===========================>..] - ETA: 0s - loss: 0.0165 - accuracy: 0.9965\n",
            "Epoch 00220: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0183 - accuracy: 0.9960 - val_loss: 0.0725 - val_accuracy: 0.9858\n",
            "Epoch 221/3056\n",
            "803/821 [============================>.] - ETA: 0s - loss: 0.0163 - accuracy: 0.9960\n",
            "Epoch 00221: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0174 - accuracy: 0.9957 - val_loss: 0.0696 - val_accuracy: 0.9865\n",
            "Epoch 222/3056\n",
            "813/821 [============================>.] - ETA: 0s - loss: 0.0180 - accuracy: 0.9948\n",
            "Epoch 00222: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0181 - accuracy: 0.9948 - val_loss: 0.0663 - val_accuracy: 0.9851\n",
            "Epoch 223/3056\n",
            "815/821 [============================>.] - ETA: 0s - loss: 0.0152 - accuracy: 0.9957\n",
            "Epoch 00223: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0151 - accuracy: 0.9957 - val_loss: 0.0887 - val_accuracy: 0.9822\n",
            "Epoch 224/3056\n",
            "813/821 [============================>.] - ETA: 0s - loss: 0.0208 - accuracy: 0.9954\n",
            "Epoch 00224: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0206 - accuracy: 0.9954 - val_loss: 0.0683 - val_accuracy: 0.9872\n",
            "Epoch 225/3056\n",
            "814/821 [============================>.] - ETA: 0s - loss: 0.0169 - accuracy: 0.9954\n",
            "Epoch 00225: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0174 - accuracy: 0.9951 - val_loss: 0.0674 - val_accuracy: 0.9879\n",
            "Epoch 226/3056\n",
            "806/821 [============================>.] - ETA: 0s - loss: 0.0204 - accuracy: 0.9953\n",
            "Epoch 00226: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0201 - accuracy: 0.9954 - val_loss: 0.0667 - val_accuracy: 0.9872\n",
            "Epoch 227/3056\n",
            "810/821 [============================>.] - ETA: 0s - loss: 0.0192 - accuracy: 0.9954\n",
            "Epoch 00227: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0190 - accuracy: 0.9954 - val_loss: 0.0696 - val_accuracy: 0.9886\n",
            "Epoch 228/3056\n",
            "816/821 [============================>.] - ETA: 0s - loss: 0.0172 - accuracy: 0.9942\n",
            "Epoch 00228: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0171 - accuracy: 0.9942 - val_loss: 0.0696 - val_accuracy: 0.9865\n",
            "Epoch 229/3056\n",
            "812/821 [============================>.] - ETA: 0s - loss: 0.0186 - accuracy: 0.9963\n",
            "Epoch 00229: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0185 - accuracy: 0.9963 - val_loss: 0.0707 - val_accuracy: 0.9872\n",
            "Epoch 230/3056\n",
            "790/821 [===========================>..] - ETA: 0s - loss: 0.0170 - accuracy: 0.9962\n",
            "Epoch 00230: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0180 - accuracy: 0.9957 - val_loss: 0.0733 - val_accuracy: 0.9872\n",
            "Epoch 231/3056\n",
            "814/821 [============================>.] - ETA: 0s - loss: 0.0184 - accuracy: 0.9963\n",
            "Epoch 00231: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0183 - accuracy: 0.9963 - val_loss: 0.0729 - val_accuracy: 0.9865\n",
            "Epoch 232/3056\n",
            "806/821 [============================>.] - ETA: 0s - loss: 0.0197 - accuracy: 0.9950\n",
            "Epoch 00232: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0194 - accuracy: 0.9951 - val_loss: 0.0745 - val_accuracy: 0.9844\n",
            "Epoch 233/3056\n",
            "814/821 [============================>.] - ETA: 0s - loss: 0.0196 - accuracy: 0.9957\n",
            "Epoch 00233: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0194 - accuracy: 0.9957 - val_loss: 0.0830 - val_accuracy: 0.9865\n",
            "Epoch 234/3056\n",
            "806/821 [============================>.] - ETA: 0s - loss: 0.0187 - accuracy: 0.9953\n",
            "Epoch 00234: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0184 - accuracy: 0.9954 - val_loss: 0.0854 - val_accuracy: 0.9865\n",
            "Epoch 235/3056\n",
            "803/821 [============================>.] - ETA: 0s - loss: 0.0185 - accuracy: 0.9963\n",
            "Epoch 00235: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0198 - accuracy: 0.9960 - val_loss: 0.0720 - val_accuracy: 0.9886\n",
            "Epoch 236/3056\n",
            "814/821 [============================>.] - ETA: 0s - loss: 0.0174 - accuracy: 0.9957\n",
            "Epoch 00236: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0172 - accuracy: 0.9957 - val_loss: 0.0767 - val_accuracy: 0.9879\n",
            "Epoch 237/3056\n",
            "810/821 [============================>.] - ETA: 0s - loss: 0.0183 - accuracy: 0.9960\n",
            "Epoch 00237: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0180 - accuracy: 0.9960 - val_loss: 0.0722 - val_accuracy: 0.9886\n",
            "Epoch 238/3056\n",
            "808/821 [============================>.] - ETA: 0s - loss: 0.0178 - accuracy: 0.9954\n",
            "Epoch 00238: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0175 - accuracy: 0.9954 - val_loss: 0.0762 - val_accuracy: 0.9865\n",
            "Epoch 239/3056\n",
            "817/821 [============================>.] - ETA: 0s - loss: 0.0182 - accuracy: 0.9963\n",
            "Epoch 00239: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0204 - accuracy: 0.9960 - val_loss: 0.0817 - val_accuracy: 0.9836\n",
            "Epoch 240/3056\n",
            "816/821 [============================>.] - ETA: 0s - loss: 0.0206 - accuracy: 0.9954\n",
            "Epoch 00240: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0205 - accuracy: 0.9954 - val_loss: 0.0726 - val_accuracy: 0.9872\n",
            "Epoch 241/3056\n",
            "813/821 [============================>.] - ETA: 0s - loss: 0.0185 - accuracy: 0.9960\n",
            "Epoch 00241: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0198 - accuracy: 0.9957 - val_loss: 0.0791 - val_accuracy: 0.9865\n",
            "Epoch 242/3056\n",
            "806/821 [============================>.] - ETA: 0s - loss: 0.0204 - accuracy: 0.9953\n",
            "Epoch 00242: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0200 - accuracy: 0.9954 - val_loss: 0.0769 - val_accuracy: 0.9858\n",
            "Epoch 243/3056\n",
            "806/821 [============================>.] - ETA: 0s - loss: 0.0207 - accuracy: 0.9953\n",
            "Epoch 00243: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0204 - accuracy: 0.9954 - val_loss: 0.0752 - val_accuracy: 0.9872\n",
            "Epoch 244/3056\n",
            "803/821 [============================>.] - ETA: 0s - loss: 0.0204 - accuracy: 0.9966\n",
            "Epoch 00244: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0200 - accuracy: 0.9966 - val_loss: 0.0794 - val_accuracy: 0.9886\n",
            "Epoch 245/3056\n",
            "808/821 [============================>.] - ETA: 0s - loss: 0.0207 - accuracy: 0.9957\n",
            "Epoch 00245: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0204 - accuracy: 0.9957 - val_loss: 0.0768 - val_accuracy: 0.9865\n",
            "Epoch 246/3056\n",
            "811/821 [============================>.] - ETA: 0s - loss: 0.0193 - accuracy: 0.9960\n",
            "Epoch 00246: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0208 - accuracy: 0.9957 - val_loss: 0.0888 - val_accuracy: 0.9844\n",
            "Epoch 247/3056\n",
            "813/821 [============================>.] - ETA: 0s - loss: 0.0194 - accuracy: 0.9963\n",
            "Epoch 00247: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0192 - accuracy: 0.9963 - val_loss: 0.0794 - val_accuracy: 0.9865\n",
            "Epoch 248/3056\n",
            "791/821 [===========================>..] - ETA: 0s - loss: 0.0183 - accuracy: 0.9962\n",
            "Epoch 00248: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0177 - accuracy: 0.9963 - val_loss: 0.0775 - val_accuracy: 0.9851\n",
            "Epoch 249/3056\n",
            "809/821 [============================>.] - ETA: 0s - loss: 0.0201 - accuracy: 0.9963\n",
            "Epoch 00249: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0199 - accuracy: 0.9963 - val_loss: 0.0828 - val_accuracy: 0.9844\n",
            "Epoch 250/3056\n",
            "812/821 [============================>.] - ETA: 0s - loss: 0.0199 - accuracy: 0.9960\n",
            "Epoch 00250: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0197 - accuracy: 0.9960 - val_loss: 0.0735 - val_accuracy: 0.9872\n",
            "Epoch 251/3056\n",
            "818/821 [============================>.] - ETA: 0s - loss: 0.0196 - accuracy: 0.9957\n",
            "Epoch 00251: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0195 - accuracy: 0.9957 - val_loss: 0.0794 - val_accuracy: 0.9872\n",
            "Epoch 252/3056\n",
            "819/821 [============================>.] - ETA: 0s - loss: 0.0216 - accuracy: 0.9960\n",
            "Epoch 00252: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0216 - accuracy: 0.9960 - val_loss: 0.0750 - val_accuracy: 0.9851\n",
            "Epoch 253/3056\n",
            "790/821 [===========================>..] - ETA: 0s - loss: 0.0198 - accuracy: 0.9962\n",
            "Epoch 00253: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0211 - accuracy: 0.9960 - val_loss: 0.0829 - val_accuracy: 0.9851\n",
            "Epoch 254/3056\n",
            "800/821 [============================>.] - ETA: 0s - loss: 0.0208 - accuracy: 0.9953\n",
            "Epoch 00254: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0203 - accuracy: 0.9954 - val_loss: 0.0845 - val_accuracy: 0.9865\n",
            "Epoch 255/3056\n",
            "820/821 [============================>.] - ETA: 0s - loss: 0.0215 - accuracy: 0.9957\n",
            "Epoch 00255: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0215 - accuracy: 0.9957 - val_loss: 0.0898 - val_accuracy: 0.9851\n",
            "Epoch 256/3056\n",
            "779/821 [===========================>..] - ETA: 0s - loss: 0.0196 - accuracy: 0.9965\n",
            "Epoch 00256: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0193 - accuracy: 0.9963 - val_loss: 0.0899 - val_accuracy: 0.9858\n",
            "Epoch 257/3056\n",
            "813/821 [============================>.] - ETA: 0s - loss: 0.0204 - accuracy: 0.9951\n",
            "Epoch 00257: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0202 - accuracy: 0.9951 - val_loss: 0.0862 - val_accuracy: 0.9858\n",
            "Epoch 258/3056\n",
            "814/821 [============================>.] - ETA: 0s - loss: 0.0188 - accuracy: 0.9963\n",
            "Epoch 00258: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0186 - accuracy: 0.9963 - val_loss: 0.0859 - val_accuracy: 0.9858\n",
            "Epoch 259/3056\n",
            "792/821 [===========================>..] - ETA: 0s - loss: 0.0201 - accuracy: 0.9959\n",
            "Epoch 00259: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0195 - accuracy: 0.9960 - val_loss: 0.0844 - val_accuracy: 0.9844\n",
            "Epoch 260/3056\n",
            "816/821 [============================>.] - ETA: 0s - loss: 0.0204 - accuracy: 0.9960\n",
            "Epoch 00260: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0203 - accuracy: 0.9960 - val_loss: 0.0789 - val_accuracy: 0.9858\n",
            "Epoch 261/3056\n",
            "783/821 [===========================>..] - ETA: 0s - loss: 0.0224 - accuracy: 0.9968\n",
            "Epoch 00261: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0214 - accuracy: 0.9970 - val_loss: 0.0877 - val_accuracy: 0.9858\n",
            "Epoch 262/3056\n",
            "806/821 [============================>.] - ETA: 0s - loss: 0.0211 - accuracy: 0.9957\n",
            "Epoch 00262: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0208 - accuracy: 0.9957 - val_loss: 0.0865 - val_accuracy: 0.9865\n",
            "Epoch 263/3056\n",
            "778/821 [===========================>..] - ETA: 0s - loss: 0.0191 - accuracy: 0.9958\n",
            "Epoch 00263: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0220 - accuracy: 0.9957 - val_loss: 0.0893 - val_accuracy: 0.9851\n",
            "Epoch 264/3056\n",
            "821/821 [==============================] - ETA: 0s - loss: 0.0204 - accuracy: 0.9957\n",
            "Epoch 00264: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0204 - accuracy: 0.9957 - val_loss: 0.0915 - val_accuracy: 0.9829\n",
            "Epoch 265/3056\n",
            "809/821 [============================>.] - ETA: 0s - loss: 0.0208 - accuracy: 0.9963\n",
            "Epoch 00265: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0222 - accuracy: 0.9960 - val_loss: 0.0973 - val_accuracy: 0.9844\n",
            "Epoch 266/3056\n",
            "816/821 [============================>.] - ETA: 0s - loss: 0.0184 - accuracy: 0.9963\n",
            "Epoch 00266: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0183 - accuracy: 0.9963 - val_loss: 0.1086 - val_accuracy: 0.9836\n",
            "Epoch 267/3056\n",
            "785/821 [===========================>..] - ETA: 0s - loss: 0.0156 - accuracy: 0.9955\n",
            "Epoch 00267: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0196 - accuracy: 0.9954 - val_loss: 0.0924 - val_accuracy: 0.9836\n",
            "Epoch 268/3056\n",
            "787/821 [===========================>..] - ETA: 0s - loss: 0.0206 - accuracy: 0.9962\n",
            "Epoch 00268: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0200 - accuracy: 0.9963 - val_loss: 0.0953 - val_accuracy: 0.9815\n",
            "Epoch 269/3056\n",
            "813/821 [============================>.] - ETA: 0s - loss: 0.0204 - accuracy: 0.9966\n",
            "Epoch 00269: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0216 - accuracy: 0.9963 - val_loss: 0.0903 - val_accuracy: 0.9836\n",
            "Epoch 270/3056\n",
            "817/821 [============================>.] - ETA: 0s - loss: 0.0222 - accuracy: 0.9954\n",
            "Epoch 00270: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0222 - accuracy: 0.9954 - val_loss: 0.0903 - val_accuracy: 0.9844\n",
            "Epoch 271/3056\n",
            "811/821 [============================>.] - ETA: 0s - loss: 0.0202 - accuracy: 0.9963\n",
            "Epoch 00271: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0201 - accuracy: 0.9963 - val_loss: 0.0992 - val_accuracy: 0.9836\n",
            "Epoch 272/3056\n",
            "810/821 [============================>.] - ETA: 0s - loss: 0.0201 - accuracy: 0.9960\n",
            "Epoch 00272: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0198 - accuracy: 0.9960 - val_loss: 0.0944 - val_accuracy: 0.9822\n",
            "Epoch 273/3056\n",
            "813/821 [============================>.] - ETA: 0s - loss: 0.0217 - accuracy: 0.9960\n",
            "Epoch 00273: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0215 - accuracy: 0.9960 - val_loss: 0.0974 - val_accuracy: 0.9844\n",
            "Epoch 274/3056\n",
            "789/821 [===========================>..] - ETA: 0s - loss: 0.0196 - accuracy: 0.9965\n",
            "Epoch 00274: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0203 - accuracy: 0.9963 - val_loss: 0.0990 - val_accuracy: 0.9836\n",
            "Epoch 275/3056\n",
            "821/821 [==============================] - ETA: 0s - loss: 0.0212 - accuracy: 0.9960\n",
            "Epoch 00275: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0212 - accuracy: 0.9960 - val_loss: 0.1001 - val_accuracy: 0.9822\n",
            "Epoch 276/3056\n",
            "783/821 [===========================>..] - ETA: 0s - loss: 0.0231 - accuracy: 0.9962\n",
            "Epoch 00276: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0221 - accuracy: 0.9963 - val_loss: 0.1000 - val_accuracy: 0.9822\n",
            "Epoch 277/3056\n",
            "810/821 [============================>.] - ETA: 0s - loss: 0.0221 - accuracy: 0.9960\n",
            "Epoch 00277: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0218 - accuracy: 0.9960 - val_loss: 0.0937 - val_accuracy: 0.9829\n",
            "Epoch 278/3056\n",
            "821/821 [==============================] - ETA: 0s - loss: 0.0221 - accuracy: 0.9963\n",
            "Epoch 00278: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0221 - accuracy: 0.9963 - val_loss: 0.1068 - val_accuracy: 0.9801\n",
            "Epoch 279/3056\n",
            "779/821 [===========================>..] - ETA: 0s - loss: 0.0215 - accuracy: 0.9955\n",
            "Epoch 00279: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0205 - accuracy: 0.9957 - val_loss: 0.1051 - val_accuracy: 0.9787\n",
            "Epoch 280/3056\n",
            "815/821 [============================>.] - ETA: 0s - loss: 0.0223 - accuracy: 0.9963\n",
            "Epoch 00280: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0222 - accuracy: 0.9963 - val_loss: 0.0982 - val_accuracy: 0.9829\n",
            "Epoch 281/3056\n",
            "817/821 [============================>.] - ETA: 0s - loss: 0.0210 - accuracy: 0.9960\n",
            "Epoch 00281: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0209 - accuracy: 0.9960 - val_loss: 0.1042 - val_accuracy: 0.9822\n",
            "Epoch 282/3056\n",
            "807/821 [============================>.] - ETA: 0s - loss: 0.0228 - accuracy: 0.9957\n",
            "Epoch 00282: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0225 - accuracy: 0.9957 - val_loss: 0.1075 - val_accuracy: 0.9822\n",
            "Epoch 283/3056\n",
            "778/821 [===========================>..] - ETA: 0s - loss: 0.0189 - accuracy: 0.9968\n",
            "Epoch 00283: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0221 - accuracy: 0.9963 - val_loss: 0.1065 - val_accuracy: 0.9829\n",
            "Epoch 284/3056\n",
            "818/821 [============================>.] - ETA: 0s - loss: 0.0226 - accuracy: 0.9954\n",
            "Epoch 00284: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0226 - accuracy: 0.9954 - val_loss: 0.1066 - val_accuracy: 0.9822\n",
            "Epoch 285/3056\n",
            "817/821 [============================>.] - ETA: 0s - loss: 0.0209 - accuracy: 0.9963\n",
            "Epoch 00285: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0208 - accuracy: 0.9963 - val_loss: 0.1152 - val_accuracy: 0.9815\n",
            "Epoch 286/3056\n",
            "777/821 [===========================>..] - ETA: 0s - loss: 0.0243 - accuracy: 0.9958\n",
            "Epoch 00286: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0231 - accuracy: 0.9960 - val_loss: 0.1078 - val_accuracy: 0.9822\n",
            "Epoch 287/3056\n",
            "812/821 [============================>.] - ETA: 0s - loss: 0.0220 - accuracy: 0.9954\n",
            "Epoch 00287: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0218 - accuracy: 0.9954 - val_loss: 0.1079 - val_accuracy: 0.9829\n",
            "Epoch 288/3056\n",
            "811/821 [============================>.] - ETA: 0s - loss: 0.0233 - accuracy: 0.9969\n",
            "Epoch 00288: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0230 - accuracy: 0.9970 - val_loss: 0.1088 - val_accuracy: 0.9829\n",
            "Epoch 289/3056\n",
            "817/821 [============================>.] - ETA: 0s - loss: 0.0201 - accuracy: 0.9966\n",
            "Epoch 00289: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0201 - accuracy: 0.9966 - val_loss: 0.1073 - val_accuracy: 0.9822\n",
            "Epoch 290/3056\n",
            "788/821 [===========================>..] - ETA: 0s - loss: 0.0166 - accuracy: 0.9962\n",
            "Epoch 00290: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0227 - accuracy: 0.9954 - val_loss: 0.1148 - val_accuracy: 0.9822\n",
            "Epoch 291/3056\n",
            "820/821 [============================>.] - ETA: 0s - loss: 0.0215 - accuracy: 0.9960\n",
            "Epoch 00291: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0215 - accuracy: 0.9960 - val_loss: 0.1085 - val_accuracy: 0.9822\n",
            "Epoch 292/3056\n",
            "779/821 [===========================>..] - ETA: 0s - loss: 0.0206 - accuracy: 0.9961\n",
            "Epoch 00292: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0196 - accuracy: 0.9963 - val_loss: 0.1223 - val_accuracy: 0.9808\n",
            "Epoch 293/3056\n",
            "821/821 [==============================] - ETA: 0s - loss: 0.0233 - accuracy: 0.9963\n",
            "Epoch 00293: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0233 - accuracy: 0.9963 - val_loss: 0.1102 - val_accuracy: 0.9808\n",
            "Epoch 294/3056\n",
            "821/821 [==============================] - ETA: 0s - loss: 0.0221 - accuracy: 0.9960\n",
            "Epoch 00294: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0221 - accuracy: 0.9960 - val_loss: 0.1164 - val_accuracy: 0.9815\n",
            "Epoch 295/3056\n",
            "794/821 [============================>.] - ETA: 0s - loss: 0.0225 - accuracy: 0.9962\n",
            "Epoch 00295: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 2s 2ms/step - loss: 0.0218 - accuracy: 0.9963 - val_loss: 0.1133 - val_accuracy: 0.9801\n",
            "Epoch 296/3056\n",
            "785/821 [===========================>..] - ETA: 0s - loss: 0.0217 - accuracy: 0.9965\n",
            "Epoch 00296: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0211 - accuracy: 0.9966 - val_loss: 0.1135 - val_accuracy: 0.9808\n",
            "Epoch 297/3056\n",
            "810/821 [============================>.] - ETA: 0s - loss: 0.0210 - accuracy: 0.9960\n",
            "Epoch 00297: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0208 - accuracy: 0.9960 - val_loss: 0.1189 - val_accuracy: 0.9808\n",
            "Epoch 298/3056\n",
            "780/821 [===========================>..] - ETA: 0s - loss: 0.0235 - accuracy: 0.9958\n",
            "Epoch 00298: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0224 - accuracy: 0.9960 - val_loss: 0.1153 - val_accuracy: 0.9801\n",
            "Epoch 299/3056\n",
            "812/821 [============================>.] - ETA: 0s - loss: 0.0233 - accuracy: 0.9963\n",
            "Epoch 00299: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0231 - accuracy: 0.9963 - val_loss: 0.1205 - val_accuracy: 0.9794\n",
            "Epoch 300/3056\n",
            "786/821 [===========================>..] - ETA: 0s - loss: 0.0223 - accuracy: 0.9962\n",
            "Epoch 00300: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0214 - accuracy: 0.9963 - val_loss: 0.1172 - val_accuracy: 0.9815\n",
            "Epoch 301/3056\n",
            "818/821 [============================>.] - ETA: 0s - loss: 0.0208 - accuracy: 0.9963\n",
            "Epoch 00301: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0207 - accuracy: 0.9963 - val_loss: 0.1191 - val_accuracy: 0.9801\n",
            "Epoch 302/3056\n",
            "803/821 [============================>.] - ETA: 0s - loss: 0.0216 - accuracy: 0.9966\n",
            "Epoch 00302: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0211 - accuracy: 0.9966 - val_loss: 0.1159 - val_accuracy: 0.9822\n",
            "Epoch 303/3056\n",
            "814/821 [============================>.] - ETA: 0s - loss: 0.0203 - accuracy: 0.9963\n",
            "Epoch 00303: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0202 - accuracy: 0.9963 - val_loss: 0.1114 - val_accuracy: 0.9822\n",
            "Epoch 304/3056\n",
            "811/821 [============================>.] - ETA: 0s - loss: 0.0216 - accuracy: 0.9969\n",
            "Epoch 00304: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0214 - accuracy: 0.9970 - val_loss: 0.1133 - val_accuracy: 0.9822\n",
            "Epoch 305/3056\n",
            "807/821 [============================>.] - ETA: 0s - loss: 0.0212 - accuracy: 0.9960\n",
            "Epoch 00305: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0216 - accuracy: 0.9957 - val_loss: 0.1278 - val_accuracy: 0.9787\n",
            "Epoch 306/3056\n",
            "809/821 [============================>.] - ETA: 0s - loss: 0.0211 - accuracy: 0.9966\n",
            "Epoch 00306: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0208 - accuracy: 0.9966 - val_loss: 0.1168 - val_accuracy: 0.9829\n",
            "Epoch 307/3056\n",
            "807/821 [============================>.] - ETA: 0s - loss: 0.0202 - accuracy: 0.9966\n",
            "Epoch 00307: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0219 - accuracy: 0.9963 - val_loss: 0.1178 - val_accuracy: 0.9815\n",
            "Epoch 308/3056\n",
            "809/821 [============================>.] - ETA: 0s - loss: 0.0191 - accuracy: 0.9963\n",
            "Epoch 00308: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0188 - accuracy: 0.9963 - val_loss: 0.1234 - val_accuracy: 0.9815\n",
            "Epoch 309/3056\n",
            "812/821 [============================>.] - ETA: 0s - loss: 0.0202 - accuracy: 0.9963\n",
            "Epoch 00309: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0201 - accuracy: 0.9963 - val_loss: 0.1193 - val_accuracy: 0.9787\n",
            "Epoch 310/3056\n",
            "810/821 [============================>.] - ETA: 0s - loss: 0.0170 - accuracy: 0.9963\n",
            "Epoch 00310: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0200 - accuracy: 0.9960 - val_loss: 0.1369 - val_accuracy: 0.9787\n",
            "Epoch 311/3056\n",
            "810/821 [============================>.] - ETA: 0s - loss: 0.0218 - accuracy: 0.9963\n",
            "Epoch 00311: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0215 - accuracy: 0.9963 - val_loss: 0.1245 - val_accuracy: 0.9801\n",
            "Epoch 312/3056\n",
            "788/821 [===========================>..] - ETA: 0s - loss: 0.0195 - accuracy: 0.9959\n",
            "Epoch 00312: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0187 - accuracy: 0.9960 - val_loss: 0.1237 - val_accuracy: 0.9794\n",
            "Epoch 313/3056\n",
            "777/821 [===========================>..] - ETA: 0s - loss: 0.0230 - accuracy: 0.9955\n",
            "Epoch 00313: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0219 - accuracy: 0.9957 - val_loss: 0.1211 - val_accuracy: 0.9801\n",
            "Epoch 314/3056\n",
            "811/821 [============================>.] - ETA: 0s - loss: 0.0207 - accuracy: 0.9957\n",
            "Epoch 00314: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0205 - accuracy: 0.9957 - val_loss: 0.1276 - val_accuracy: 0.9787\n",
            "Epoch 315/3056\n",
            "783/821 [===========================>..] - ETA: 0s - loss: 0.0192 - accuracy: 0.9955\n",
            "Epoch 00315: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0184 - accuracy: 0.9957 - val_loss: 0.1237 - val_accuracy: 0.9815\n",
            "Epoch 316/3056\n",
            "820/821 [============================>.] - ETA: 0s - loss: 0.0225 - accuracy: 0.9960\n",
            "Epoch 00316: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0225 - accuracy: 0.9960 - val_loss: 0.1252 - val_accuracy: 0.9794\n",
            "Epoch 317/3056\n",
            "821/821 [==============================] - ETA: 0s - loss: 0.0209 - accuracy: 0.9960\n",
            "Epoch 00317: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0209 - accuracy: 0.9960 - val_loss: 0.1227 - val_accuracy: 0.9801\n",
            "Epoch 318/3056\n",
            "793/821 [===========================>..] - ETA: 0s - loss: 0.0206 - accuracy: 0.9959\n",
            "Epoch 00318: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0199 - accuracy: 0.9960 - val_loss: 0.1354 - val_accuracy: 0.9794\n",
            "Epoch 319/3056\n",
            "784/821 [===========================>..] - ETA: 0s - loss: 0.0195 - accuracy: 0.9965\n",
            "Epoch 00319: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0201 - accuracy: 0.9963 - val_loss: 0.1262 - val_accuracy: 0.9808\n",
            "Epoch 320/3056\n",
            "813/821 [============================>.] - ETA: 0s - loss: 0.0202 - accuracy: 0.9957\n",
            "Epoch 00320: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0200 - accuracy: 0.9957 - val_loss: 0.1337 - val_accuracy: 0.9787\n",
            "Epoch 321/3056\n",
            "815/821 [============================>.] - ETA: 0s - loss: 0.0201 - accuracy: 0.9957\n",
            "Epoch 00321: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 2s 2ms/step - loss: 0.0200 - accuracy: 0.9957 - val_loss: 0.1265 - val_accuracy: 0.9794\n",
            "Epoch 322/3056\n",
            "786/821 [===========================>..] - ETA: 0s - loss: 0.0207 - accuracy: 0.9959\n",
            "Epoch 00322: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0199 - accuracy: 0.9960 - val_loss: 0.1176 - val_accuracy: 0.9780\n",
            "Epoch 323/3056\n",
            "782/821 [===========================>..] - ETA: 0s - loss: 0.0218 - accuracy: 0.9968\n",
            "Epoch 00323: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0208 - accuracy: 0.9970 - val_loss: 0.1228 - val_accuracy: 0.9787\n",
            "Epoch 324/3056\n",
            "787/821 [===========================>..] - ETA: 0s - loss: 0.0228 - accuracy: 0.9952\n",
            "Epoch 00324: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0219 - accuracy: 0.9954 - val_loss: 0.1240 - val_accuracy: 0.9787\n",
            "Epoch 325/3056\n",
            "791/821 [===========================>..] - ETA: 0s - loss: 0.0208 - accuracy: 0.9965\n",
            "Epoch 00325: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0221 - accuracy: 0.9963 - val_loss: 0.1280 - val_accuracy: 0.9772\n",
            "Epoch 326/3056\n",
            "820/821 [============================>.] - ETA: 0s - loss: 0.0196 - accuracy: 0.9970\n",
            "Epoch 00326: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0196 - accuracy: 0.9970 - val_loss: 0.1158 - val_accuracy: 0.9801\n",
            "Epoch 327/3056\n",
            "807/821 [============================>.] - ETA: 0s - loss: 0.0199 - accuracy: 0.9969\n",
            "Epoch 00327: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0196 - accuracy: 0.9970 - val_loss: 0.1335 - val_accuracy: 0.9787\n",
            "Epoch 328/3056\n",
            "817/821 [============================>.] - ETA: 0s - loss: 0.0224 - accuracy: 0.9966\n",
            "Epoch 00328: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0223 - accuracy: 0.9966 - val_loss: 0.1284 - val_accuracy: 0.9772\n",
            "Epoch 329/3056\n",
            "783/821 [===========================>..] - ETA: 0s - loss: 0.0202 - accuracy: 0.9962\n",
            "Epoch 00329: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0194 - accuracy: 0.9963 - val_loss: 0.1260 - val_accuracy: 0.9787\n",
            "Epoch 330/3056\n",
            "813/821 [============================>.] - ETA: 0s - loss: 0.0214 - accuracy: 0.9966\n",
            "Epoch 00330: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0212 - accuracy: 0.9966 - val_loss: 0.1254 - val_accuracy: 0.9780\n",
            "Epoch 331/3056\n",
            "815/821 [============================>.] - ETA: 0s - loss: 0.0203 - accuracy: 0.9963\n",
            "Epoch 00331: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0202 - accuracy: 0.9963 - val_loss: 0.1274 - val_accuracy: 0.9794\n",
            "Epoch 332/3056\n",
            "787/821 [===========================>..] - ETA: 0s - loss: 0.0215 - accuracy: 0.9959\n",
            "Epoch 00332: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0206 - accuracy: 0.9960 - val_loss: 0.1299 - val_accuracy: 0.9801\n",
            "Epoch 333/3056\n",
            "800/821 [============================>.] - ETA: 0s - loss: 0.0201 - accuracy: 0.9962\n",
            "Epoch 00333: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0196 - accuracy: 0.9963 - val_loss: 0.1286 - val_accuracy: 0.9794\n",
            "Epoch 334/3056\n",
            "808/821 [============================>.] - ETA: 0s - loss: 0.0209 - accuracy: 0.9963\n",
            "Epoch 00334: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0206 - accuracy: 0.9963 - val_loss: 0.1287 - val_accuracy: 0.9801\n",
            "Epoch 335/3056\n",
            "816/821 [============================>.] - ETA: 0s - loss: 0.0224 - accuracy: 0.9957\n",
            "Epoch 00335: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0223 - accuracy: 0.9957 - val_loss: 0.1241 - val_accuracy: 0.9787\n",
            "Epoch 336/3056\n",
            "788/821 [===========================>..] - ETA: 0s - loss: 0.0239 - accuracy: 0.9956\n",
            "Epoch 00336: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0230 - accuracy: 0.9957 - val_loss: 0.1282 - val_accuracy: 0.9787\n",
            "Epoch 337/3056\n",
            "781/821 [===========================>..] - ETA: 0s - loss: 0.0208 - accuracy: 0.9958\n",
            "Epoch 00337: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0198 - accuracy: 0.9960 - val_loss: 0.1408 - val_accuracy: 0.9772\n",
            "Epoch 338/3056\n",
            "779/821 [===========================>..] - ETA: 0s - loss: 0.0211 - accuracy: 0.9961\n",
            "Epoch 00338: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0216 - accuracy: 0.9960 - val_loss: 0.1308 - val_accuracy: 0.9772\n",
            "Epoch 339/3056\n",
            "795/821 [============================>.] - ETA: 0s - loss: 0.0188 - accuracy: 0.9965\n",
            "Epoch 00339: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0209 - accuracy: 0.9960 - val_loss: 0.1366 - val_accuracy: 0.9801\n",
            "Epoch 340/3056\n",
            "793/821 [===========================>..] - ETA: 0s - loss: 0.0191 - accuracy: 0.9959\n",
            "Epoch 00340: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0205 - accuracy: 0.9957 - val_loss: 0.1374 - val_accuracy: 0.9787\n",
            "Epoch 341/3056\n",
            "792/821 [===========================>..] - ETA: 0s - loss: 0.0187 - accuracy: 0.9959\n",
            "Epoch 00341: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0181 - accuracy: 0.9960 - val_loss: 0.1295 - val_accuracy: 0.9808\n",
            "Epoch 342/3056\n",
            "785/821 [===========================>..] - ETA: 0s - loss: 0.0200 - accuracy: 0.9952\n",
            "Epoch 00342: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0192 - accuracy: 0.9954 - val_loss: 0.1275 - val_accuracy: 0.9794\n",
            "Epoch 343/3056\n",
            "803/821 [============================>.] - ETA: 0s - loss: 0.0203 - accuracy: 0.9960\n",
            "Epoch 00343: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0198 - accuracy: 0.9960 - val_loss: 0.1240 - val_accuracy: 0.9787\n",
            "Epoch 344/3056\n",
            "812/821 [============================>.] - ETA: 0s - loss: 0.0182 - accuracy: 0.9963\n",
            "Epoch 00344: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0180 - accuracy: 0.9963 - val_loss: 0.1348 - val_accuracy: 0.9801\n",
            "Epoch 345/3056\n",
            "795/821 [============================>.] - ETA: 0s - loss: 0.0225 - accuracy: 0.9953\n",
            "Epoch 00345: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0218 - accuracy: 0.9954 - val_loss: 0.1317 - val_accuracy: 0.9794\n",
            "Epoch 346/3056\n",
            "781/821 [===========================>..] - ETA: 0s - loss: 0.0200 - accuracy: 0.9958\n",
            "Epoch 00346: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0191 - accuracy: 0.9960 - val_loss: 0.1340 - val_accuracy: 0.9787\n",
            "Epoch 347/3056\n",
            "790/821 [===========================>..] - ETA: 0s - loss: 0.0190 - accuracy: 0.9962\n",
            "Epoch 00347: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0184 - accuracy: 0.9963 - val_loss: 0.1284 - val_accuracy: 0.9787\n",
            "Epoch 348/3056\n",
            "781/821 [===========================>..] - ETA: 0s - loss: 0.0150 - accuracy: 0.9965\n",
            "Epoch 00348: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0179 - accuracy: 0.9963 - val_loss: 0.1389 - val_accuracy: 0.9787\n",
            "Epoch 349/3056\n",
            "797/821 [============================>.] - ETA: 0s - loss: 0.0176 - accuracy: 0.9962\n",
            "Epoch 00349: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0171 - accuracy: 0.9963 - val_loss: 0.1474 - val_accuracy: 0.9787\n",
            "Epoch 350/3056\n",
            "791/821 [===========================>..] - ETA: 0s - loss: 0.0189 - accuracy: 0.9968\n",
            "Epoch 00350: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0189 - accuracy: 0.9966 - val_loss: 0.1449 - val_accuracy: 0.9794\n",
            "Epoch 351/3056\n",
            "808/821 [============================>.] - ETA: 0s - loss: 0.0170 - accuracy: 0.9963\n",
            "Epoch 00351: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0168 - accuracy: 0.9963 - val_loss: 0.1419 - val_accuracy: 0.9801\n",
            "Epoch 352/3056\n",
            "795/821 [============================>.] - ETA: 0s - loss: 0.0178 - accuracy: 0.9959\n",
            "Epoch 00352: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0189 - accuracy: 0.9957 - val_loss: 0.1328 - val_accuracy: 0.9787\n",
            "Epoch 353/3056\n",
            "785/821 [===========================>..] - ETA: 0s - loss: 0.0165 - accuracy: 0.9962\n",
            "Epoch 00353: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0172 - accuracy: 0.9960 - val_loss: 0.1305 - val_accuracy: 0.9794\n",
            "Epoch 354/3056\n",
            "793/821 [===========================>..] - ETA: 0s - loss: 0.0197 - accuracy: 0.9962\n",
            "Epoch 00354: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0191 - accuracy: 0.9963 - val_loss: 0.1339 - val_accuracy: 0.9801\n",
            "Epoch 355/3056\n",
            "787/821 [===========================>..] - ETA: 0s - loss: 0.0200 - accuracy: 0.9959\n",
            "Epoch 00355: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0192 - accuracy: 0.9960 - val_loss: 0.1242 - val_accuracy: 0.9780\n",
            "Epoch 356/3056\n",
            "795/821 [============================>.] - ETA: 0s - loss: 0.0160 - accuracy: 0.9956\n",
            "Epoch 00356: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0155 - accuracy: 0.9957 - val_loss: 0.1287 - val_accuracy: 0.9772\n",
            "Epoch 357/3056\n",
            "782/821 [===========================>..] - ETA: 0s - loss: 0.0181 - accuracy: 0.9962\n",
            "Epoch 00357: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0205 - accuracy: 0.9960 - val_loss: 0.1328 - val_accuracy: 0.9794\n",
            "Epoch 358/3056\n",
            "789/821 [===========================>..] - ETA: 0s - loss: 0.0179 - accuracy: 0.9965\n",
            "Epoch 00358: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0173 - accuracy: 0.9966 - val_loss: 0.1339 - val_accuracy: 0.9780\n",
            "Epoch 359/3056\n",
            "821/821 [==============================] - ETA: 0s - loss: 0.0191 - accuracy: 0.9963\n",
            "Epoch 00359: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0191 - accuracy: 0.9963 - val_loss: 0.1318 - val_accuracy: 0.9794\n",
            "Epoch 360/3056\n",
            "808/821 [============================>.] - ETA: 0s - loss: 0.0191 - accuracy: 0.9957\n",
            "Epoch 00360: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0188 - accuracy: 0.9957 - val_loss: 0.1305 - val_accuracy: 0.9787\n",
            "Epoch 361/3056\n",
            "781/821 [===========================>..] - ETA: 0s - loss: 0.0185 - accuracy: 0.9962\n",
            "Epoch 00361: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0177 - accuracy: 0.9963 - val_loss: 0.1258 - val_accuracy: 0.9787\n",
            "Epoch 362/3056\n",
            "784/821 [===========================>..] - ETA: 0s - loss: 0.0181 - accuracy: 0.9955\n",
            "Epoch 00362: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0173 - accuracy: 0.9957 - val_loss: 0.1383 - val_accuracy: 0.9794\n",
            "Epoch 363/3056\n",
            "812/821 [============================>.] - ETA: 0s - loss: 0.0173 - accuracy: 0.9969\n",
            "Epoch 00363: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0171 - accuracy: 0.9970 - val_loss: 0.1304 - val_accuracy: 0.9801\n",
            "Epoch 364/3056\n",
            "816/821 [============================>.] - ETA: 0s - loss: 0.0181 - accuracy: 0.9963\n",
            "Epoch 00364: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0180 - accuracy: 0.9963 - val_loss: 0.1297 - val_accuracy: 0.9780\n",
            "Epoch 365/3056\n",
            "807/821 [============================>.] - ETA: 0s - loss: 0.0187 - accuracy: 0.9963\n",
            "Epoch 00365: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0184 - accuracy: 0.9963 - val_loss: 0.1299 - val_accuracy: 0.9794\n",
            "Epoch 366/3056\n",
            "810/821 [============================>.] - ETA: 0s - loss: 0.0156 - accuracy: 0.9969\n",
            "Epoch 00366: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0169 - accuracy: 0.9966 - val_loss: 0.1292 - val_accuracy: 0.9787\n",
            "Epoch 367/3056\n",
            "784/821 [===========================>..] - ETA: 0s - loss: 0.0186 - accuracy: 0.9968\n",
            "Epoch 00367: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0178 - accuracy: 0.9970 - val_loss: 0.1346 - val_accuracy: 0.9794\n",
            "Epoch 368/3056\n",
            "820/821 [============================>.] - ETA: 0s - loss: 0.0162 - accuracy: 0.9957\n",
            "Epoch 00368: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0162 - accuracy: 0.9957 - val_loss: 0.1277 - val_accuracy: 0.9765\n",
            "Epoch 369/3056\n",
            "787/821 [===========================>..] - ETA: 0s - loss: 0.0184 - accuracy: 0.9962\n",
            "Epoch 00369: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0177 - accuracy: 0.9963 - val_loss: 0.1294 - val_accuracy: 0.9787\n",
            "Epoch 370/3056\n",
            "821/821 [==============================] - ETA: 0s - loss: 0.0153 - accuracy: 0.9963\n",
            "Epoch 00370: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0153 - accuracy: 0.9963 - val_loss: 0.1218 - val_accuracy: 0.9787\n",
            "Epoch 371/3056\n",
            "788/821 [===========================>..] - ETA: 0s - loss: 0.0188 - accuracy: 0.9962\n",
            "Epoch 00371: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0181 - accuracy: 0.9963 - val_loss: 0.1272 - val_accuracy: 0.9794\n",
            "Epoch 372/3056\n",
            "809/821 [============================>.] - ETA: 0s - loss: 0.0174 - accuracy: 0.9957\n",
            "Epoch 00372: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0171 - accuracy: 0.9957 - val_loss: 0.1376 - val_accuracy: 0.9780\n",
            "Epoch 373/3056\n",
            "820/821 [============================>.] - ETA: 0s - loss: 0.0198 - accuracy: 0.9966\n",
            "Epoch 00373: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0198 - accuracy: 0.9966 - val_loss: 0.1332 - val_accuracy: 0.9780\n",
            "Epoch 374/3056\n",
            "814/821 [============================>.] - ETA: 0s - loss: 0.0190 - accuracy: 0.9966\n",
            "Epoch 00374: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0189 - accuracy: 0.9966 - val_loss: 0.1324 - val_accuracy: 0.9801\n",
            "Epoch 375/3056\n",
            "777/821 [===========================>..] - ETA: 0s - loss: 0.0174 - accuracy: 0.9965\n",
            "Epoch 00375: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0165 - accuracy: 0.9966 - val_loss: 0.1342 - val_accuracy: 0.9808\n",
            "Epoch 376/3056\n",
            "783/821 [===========================>..] - ETA: 0s - loss: 0.0128 - accuracy: 0.9968\n",
            "Epoch 00376: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0144 - accuracy: 0.9963 - val_loss: 0.1345 - val_accuracy: 0.9794\n",
            "Epoch 377/3056\n",
            "782/821 [===========================>..] - ETA: 0s - loss: 0.0120 - accuracy: 0.9968\n",
            "Epoch 00377: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0123 - accuracy: 0.9966 - val_loss: 0.1257 - val_accuracy: 0.9758\n",
            "Epoch 378/3056\n",
            "789/821 [===========================>..] - ETA: 0s - loss: 0.0145 - accuracy: 0.9968\n",
            "Epoch 00378: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0141 - accuracy: 0.9970 - val_loss: 0.1288 - val_accuracy: 0.9801\n",
            "Epoch 379/3056\n",
            "809/821 [============================>.] - ETA: 0s - loss: 0.0148 - accuracy: 0.9966\n",
            "Epoch 00379: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0146 - accuracy: 0.9966 - val_loss: 0.1290 - val_accuracy: 0.9787\n",
            "Epoch 380/3056\n",
            "811/821 [============================>.] - ETA: 0s - loss: 0.0143 - accuracy: 0.9963\n",
            "Epoch 00380: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0142 - accuracy: 0.9963 - val_loss: 0.1415 - val_accuracy: 0.9801\n",
            "Epoch 381/3056\n",
            "789/821 [===========================>..] - ETA: 0s - loss: 0.0157 - accuracy: 0.9962\n",
            "Epoch 00381: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0151 - accuracy: 0.9963 - val_loss: 0.1355 - val_accuracy: 0.9780\n",
            "Epoch 382/3056\n",
            "821/821 [==============================] - ETA: 0s - loss: 0.0149 - accuracy: 0.9970\n",
            "Epoch 00382: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0149 - accuracy: 0.9970 - val_loss: 0.1359 - val_accuracy: 0.9794\n",
            "Epoch 383/3056\n",
            "783/821 [===========================>..] - ETA: 0s - loss: 0.0155 - accuracy: 0.9962\n",
            "Epoch 00383: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0159 - accuracy: 0.9960 - val_loss: 0.1256 - val_accuracy: 0.9787\n",
            "Epoch 384/3056\n",
            "811/821 [============================>.] - ETA: 0s - loss: 0.0136 - accuracy: 0.9969\n",
            "Epoch 00384: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0134 - accuracy: 0.9970 - val_loss: 0.1330 - val_accuracy: 0.9765\n",
            "Epoch 385/3056\n",
            "816/821 [============================>.] - ETA: 0s - loss: 0.0156 - accuracy: 0.9969\n",
            "Epoch 00385: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0156 - accuracy: 0.9970 - val_loss: 0.1391 - val_accuracy: 0.9808\n",
            "Epoch 386/3056\n",
            "786/821 [===========================>..] - ETA: 0s - loss: 0.0149 - accuracy: 0.9959\n",
            "Epoch 00386: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0144 - accuracy: 0.9960 - val_loss: 0.1356 - val_accuracy: 0.9808\n",
            "Epoch 387/3056\n",
            "820/821 [============================>.] - ETA: 0s - loss: 0.0163 - accuracy: 0.9957\n",
            "Epoch 00387: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0163 - accuracy: 0.9957 - val_loss: 0.1405 - val_accuracy: 0.9772\n",
            "Epoch 388/3056\n",
            "807/821 [============================>.] - ETA: 0s - loss: 0.0154 - accuracy: 0.9960\n",
            "Epoch 00388: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0152 - accuracy: 0.9960 - val_loss: 0.1377 - val_accuracy: 0.9787\n",
            "Epoch 389/3056\n",
            "799/821 [============================>.] - ETA: 0s - loss: 0.0151 - accuracy: 0.9966\n",
            "Epoch 00389: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0147 - accuracy: 0.9966 - val_loss: 0.1318 - val_accuracy: 0.9765\n",
            "Epoch 390/3056\n",
            "791/821 [===========================>..] - ETA: 0s - loss: 0.0146 - accuracy: 0.9972\n",
            "Epoch 00390: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0152 - accuracy: 0.9970 - val_loss: 0.1424 - val_accuracy: 0.9780\n",
            "Epoch 391/3056\n",
            "812/821 [============================>.] - ETA: 0s - loss: 0.0176 - accuracy: 0.9969\n",
            "Epoch 00391: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0175 - accuracy: 0.9970 - val_loss: 0.1393 - val_accuracy: 0.9765\n",
            "Epoch 392/3056\n",
            "809/821 [============================>.] - ETA: 0s - loss: 0.0147 - accuracy: 0.9963\n",
            "Epoch 00392: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0147 - accuracy: 0.9963 - val_loss: 0.1356 - val_accuracy: 0.9772\n",
            "Epoch 393/3056\n",
            "796/821 [============================>.] - ETA: 0s - loss: 0.0173 - accuracy: 0.9965\n",
            "Epoch 00393: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0168 - accuracy: 0.9966 - val_loss: 0.1321 - val_accuracy: 0.9794\n",
            "Epoch 394/3056\n",
            "802/821 [============================>.] - ETA: 0s - loss: 0.0164 - accuracy: 0.9969\n",
            "Epoch 00394: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0168 - accuracy: 0.9966 - val_loss: 0.1341 - val_accuracy: 0.9808\n",
            "Epoch 395/3056\n",
            "818/821 [============================>.] - ETA: 0s - loss: 0.0155 - accuracy: 0.9966\n",
            "Epoch 00395: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0155 - accuracy: 0.9966 - val_loss: 0.1390 - val_accuracy: 0.9780\n",
            "Epoch 396/3056\n",
            "779/821 [===========================>..] - ETA: 0s - loss: 0.0157 - accuracy: 0.9961\n",
            "Epoch 00396: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0150 - accuracy: 0.9963 - val_loss: 0.1407 - val_accuracy: 0.9801\n",
            "Epoch 397/3056\n",
            "815/821 [============================>.] - ETA: 0s - loss: 0.0156 - accuracy: 0.9966\n",
            "Epoch 00397: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0155 - accuracy: 0.9966 - val_loss: 0.1428 - val_accuracy: 0.9780\n",
            "Epoch 398/3056\n",
            "808/821 [============================>.] - ETA: 0s - loss: 0.0171 - accuracy: 0.9966\n",
            "Epoch 00398: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0169 - accuracy: 0.9966 - val_loss: 0.1448 - val_accuracy: 0.9772\n",
            "Epoch 399/3056\n",
            "782/821 [===========================>..] - ETA: 0s - loss: 0.0160 - accuracy: 0.9968\n",
            "Epoch 00399: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0169 - accuracy: 0.9966 - val_loss: 0.1423 - val_accuracy: 0.9801\n",
            "Epoch 400/3056\n",
            "777/821 [===========================>..] - ETA: 0s - loss: 0.0147 - accuracy: 0.9971\n",
            "Epoch 00400: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0157 - accuracy: 0.9966 - val_loss: 0.1334 - val_accuracy: 0.9787\n",
            "Epoch 401/3056\n",
            "802/821 [============================>.] - ETA: 0s - loss: 0.0161 - accuracy: 0.9969\n",
            "Epoch 00401: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0158 - accuracy: 0.9970 - val_loss: 0.1318 - val_accuracy: 0.9787\n",
            "Epoch 402/3056\n",
            "816/821 [============================>.] - ETA: 0s - loss: 0.0183 - accuracy: 0.9966\n",
            "Epoch 00402: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0182 - accuracy: 0.9966 - val_loss: 0.1370 - val_accuracy: 0.9794\n",
            "Epoch 403/3056\n",
            "821/821 [==============================] - ETA: 0s - loss: 0.0171 - accuracy: 0.9966\n",
            "Epoch 00403: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0171 - accuracy: 0.9966 - val_loss: 0.1329 - val_accuracy: 0.9780\n",
            "Epoch 404/3056\n",
            "803/821 [============================>.] - ETA: 0s - loss: 0.0170 - accuracy: 0.9969\n",
            "Epoch 00404: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0166 - accuracy: 0.9970 - val_loss: 0.1368 - val_accuracy: 0.9794\n",
            "Epoch 405/3056\n",
            "792/821 [===========================>..] - ETA: 0s - loss: 0.0167 - accuracy: 0.9962\n",
            "Epoch 00405: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0161 - accuracy: 0.9963 - val_loss: 0.1382 - val_accuracy: 0.9794\n",
            "Epoch 406/3056\n",
            "816/821 [============================>.] - ETA: 0s - loss: 0.0150 - accuracy: 0.9969\n",
            "Epoch 00406: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0149 - accuracy: 0.9970 - val_loss: 0.1309 - val_accuracy: 0.9794\n",
            "Epoch 407/3056\n",
            "803/821 [============================>.] - ETA: 0s - loss: 0.0156 - accuracy: 0.9966\n",
            "Epoch 00407: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0154 - accuracy: 0.9966 - val_loss: 0.1384 - val_accuracy: 0.9765\n",
            "Epoch 408/3056\n",
            "801/821 [============================>.] - ETA: 0s - loss: 0.0157 - accuracy: 0.9969\n",
            "Epoch 00408: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0154 - accuracy: 0.9970 - val_loss: 0.1390 - val_accuracy: 0.9787\n",
            "Epoch 409/3056\n",
            "820/821 [============================>.] - ETA: 0s - loss: 0.0153 - accuracy: 0.9970\n",
            "Epoch 00409: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0153 - accuracy: 0.9970 - val_loss: 0.1425 - val_accuracy: 0.9794\n",
            "Epoch 410/3056\n",
            "791/821 [===========================>..] - ETA: 0s - loss: 0.0141 - accuracy: 0.9972\n",
            "Epoch 00410: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0157 - accuracy: 0.9963 - val_loss: 0.1351 - val_accuracy: 0.9772\n",
            "Epoch 411/3056\n",
            "814/821 [============================>.] - ETA: 0s - loss: 0.0160 - accuracy: 0.9969\n",
            "Epoch 00411: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0159 - accuracy: 0.9970 - val_loss: 0.1396 - val_accuracy: 0.9780\n",
            "Epoch 412/3056\n",
            "815/821 [============================>.] - ETA: 0s - loss: 0.0162 - accuracy: 0.9963\n",
            "Epoch 00412: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0162 - accuracy: 0.9963 - val_loss: 0.1411 - val_accuracy: 0.9772\n",
            "Epoch 413/3056\n",
            "817/821 [============================>.] - ETA: 0s - loss: 0.0134 - accuracy: 0.9969\n",
            "Epoch 00413: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0134 - accuracy: 0.9970 - val_loss: 0.1351 - val_accuracy: 0.9758\n",
            "Epoch 414/3056\n",
            "812/821 [============================>.] - ETA: 0s - loss: 0.0159 - accuracy: 0.9963\n",
            "Epoch 00414: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0158 - accuracy: 0.9963 - val_loss: 0.1392 - val_accuracy: 0.9772\n",
            "Epoch 415/3056\n",
            "783/821 [===========================>..] - ETA: 0s - loss: 0.0133 - accuracy: 0.9968\n",
            "Epoch 00415: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0157 - accuracy: 0.9963 - val_loss: 0.1389 - val_accuracy: 0.9787\n",
            "Epoch 416/3056\n",
            "779/821 [===========================>..] - ETA: 0s - loss: 0.0167 - accuracy: 0.9971\n",
            "Epoch 00416: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0167 - accuracy: 0.9970 - val_loss: 0.1414 - val_accuracy: 0.9794\n",
            "Epoch 417/3056\n",
            "818/821 [============================>.] - ETA: 0s - loss: 0.0160 - accuracy: 0.9963\n",
            "Epoch 00417: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0160 - accuracy: 0.9963 - val_loss: 0.1398 - val_accuracy: 0.9808\n",
            "Epoch 418/3056\n",
            "815/821 [============================>.] - ETA: 0s - loss: 0.0147 - accuracy: 0.9966\n",
            "Epoch 00418: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0146 - accuracy: 0.9966 - val_loss: 0.1448 - val_accuracy: 0.9801\n",
            "Epoch 419/3056\n",
            "782/821 [===========================>..] - ETA: 0s - loss: 0.0167 - accuracy: 0.9962\n",
            "Epoch 00419: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0159 - accuracy: 0.9963 - val_loss: 0.1404 - val_accuracy: 0.9808\n",
            "Epoch 420/3056\n",
            "821/821 [==============================] - ETA: 0s - loss: 0.0173 - accuracy: 0.9966\n",
            "Epoch 00420: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0173 - accuracy: 0.9966 - val_loss: 0.1367 - val_accuracy: 0.9787\n",
            "Epoch 421/3056\n",
            "803/821 [============================>.] - ETA: 0s - loss: 0.0163 - accuracy: 0.9969\n",
            "Epoch 00421: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0160 - accuracy: 0.9970 - val_loss: 0.1319 - val_accuracy: 0.9787\n",
            "Epoch 422/3056\n",
            "812/821 [============================>.] - ETA: 0s - loss: 0.0148 - accuracy: 0.9960\n",
            "Epoch 00422: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0147 - accuracy: 0.9960 - val_loss: 0.1361 - val_accuracy: 0.9787\n",
            "Epoch 423/3056\n",
            "800/821 [============================>.] - ETA: 0s - loss: 0.0155 - accuracy: 0.9969\n",
            "Epoch 00423: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0151 - accuracy: 0.9970 - val_loss: 0.1321 - val_accuracy: 0.9787\n",
            "Epoch 424/3056\n",
            "799/821 [============================>.] - ETA: 0s - loss: 0.0153 - accuracy: 0.9966\n",
            "Epoch 00424: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0149 - accuracy: 0.9966 - val_loss: 0.1309 - val_accuracy: 0.9772\n",
            "Epoch 425/3056\n",
            "786/821 [===========================>..] - ETA: 0s - loss: 0.0135 - accuracy: 0.9968\n",
            "Epoch 00425: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0131 - accuracy: 0.9970 - val_loss: 0.1305 - val_accuracy: 0.9787\n",
            "Epoch 426/3056\n",
            "816/821 [============================>.] - ETA: 0s - loss: 0.0153 - accuracy: 0.9963\n",
            "Epoch 00426: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0153 - accuracy: 0.9963 - val_loss: 0.1229 - val_accuracy: 0.9787\n",
            "Epoch 427/3056\n",
            "781/821 [===========================>..] - ETA: 0s - loss: 0.0156 - accuracy: 0.9971\n",
            "Epoch 00427: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0149 - accuracy: 0.9973 - val_loss: 0.1286 - val_accuracy: 0.9808\n",
            "Epoch 428/3056\n",
            "810/821 [============================>.] - ETA: 0s - loss: 0.0130 - accuracy: 0.9972\n",
            "Epoch 00428: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0128 - accuracy: 0.9973 - val_loss: 0.1316 - val_accuracy: 0.9801\n",
            "Epoch 429/3056\n",
            "788/821 [===========================>..] - ETA: 0s - loss: 0.0136 - accuracy: 0.9965\n",
            "Epoch 00429: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0143 - accuracy: 0.9963 - val_loss: 0.1283 - val_accuracy: 0.9801\n",
            "Epoch 430/3056\n",
            "806/821 [============================>.] - ETA: 0s - loss: 0.0163 - accuracy: 0.9966\n",
            "Epoch 00430: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0160 - accuracy: 0.9966 - val_loss: 0.1241 - val_accuracy: 0.9801\n",
            "Epoch 431/3056\n",
            "808/821 [============================>.] - ETA: 0s - loss: 0.0141 - accuracy: 0.9969\n",
            "Epoch 00431: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0139 - accuracy: 0.9970 - val_loss: 0.1299 - val_accuracy: 0.9780\n",
            "Epoch 432/3056\n",
            "806/821 [============================>.] - ETA: 0s - loss: 0.0144 - accuracy: 0.9975\n",
            "Epoch 00432: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0141 - accuracy: 0.9976 - val_loss: 0.1221 - val_accuracy: 0.9808\n",
            "Epoch 433/3056\n",
            "812/821 [============================>.] - ETA: 0s - loss: 0.0118 - accuracy: 0.9972\n",
            "Epoch 00433: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0116 - accuracy: 0.9973 - val_loss: 0.1287 - val_accuracy: 0.9815\n",
            "Epoch 434/3056\n",
            "811/821 [============================>.] - ETA: 0s - loss: 0.0162 - accuracy: 0.9969\n",
            "Epoch 00434: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0161 - accuracy: 0.9970 - val_loss: 0.1242 - val_accuracy: 0.9801\n",
            "Epoch 435/3056\n",
            "816/821 [============================>.] - ETA: 0s - loss: 0.0130 - accuracy: 0.9966\n",
            "Epoch 00435: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0129 - accuracy: 0.9966 - val_loss: 0.1277 - val_accuracy: 0.9808\n",
            "Epoch 436/3056\n",
            "817/821 [============================>.] - ETA: 0s - loss: 0.0134 - accuracy: 0.9966\n",
            "Epoch 00436: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0150 - accuracy: 0.9963 - val_loss: 0.1265 - val_accuracy: 0.9801\n",
            "Epoch 437/3056\n",
            "820/821 [============================>.] - ETA: 0s - loss: 0.0133 - accuracy: 0.9973\n",
            "Epoch 00437: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0133 - accuracy: 0.9973 - val_loss: 0.1252 - val_accuracy: 0.9801\n",
            "Epoch 438/3056\n",
            "784/821 [===========================>..] - ETA: 0s - loss: 0.0131 - accuracy: 0.9971\n",
            "Epoch 00438: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0135 - accuracy: 0.9970 - val_loss: 0.1251 - val_accuracy: 0.9794\n",
            "Epoch 439/3056\n",
            "791/821 [===========================>..] - ETA: 0s - loss: 0.0140 - accuracy: 0.9972\n",
            "Epoch 00439: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0136 - accuracy: 0.9973 - val_loss: 0.1165 - val_accuracy: 0.9808\n",
            "Epoch 440/3056\n",
            "816/821 [============================>.] - ETA: 0s - loss: 0.0129 - accuracy: 0.9969\n",
            "Epoch 00440: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0128 - accuracy: 0.9970 - val_loss: 0.1255 - val_accuracy: 0.9808\n",
            "Epoch 441/3056\n",
            "807/821 [============================>.] - ETA: 0s - loss: 0.0115 - accuracy: 0.9969\n",
            "Epoch 00441: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 2s 2ms/step - loss: 0.0114 - accuracy: 0.9970 - val_loss: 0.1243 - val_accuracy: 0.9815\n",
            "Epoch 442/3056\n",
            "806/821 [============================>.] - ETA: 0s - loss: 0.0139 - accuracy: 0.9969\n",
            "Epoch 00442: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 2s 2ms/step - loss: 0.0136 - accuracy: 0.9970 - val_loss: 0.1259 - val_accuracy: 0.9787\n",
            "Epoch 443/3056\n",
            "816/821 [============================>.] - ETA: 0s - loss: 0.0118 - accuracy: 0.9975\n",
            "Epoch 00443: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 2s 2ms/step - loss: 0.0117 - accuracy: 0.9976 - val_loss: 0.1244 - val_accuracy: 0.9787\n",
            "Epoch 444/3056\n",
            "794/821 [============================>.] - ETA: 0s - loss: 0.0130 - accuracy: 0.9972\n",
            "Epoch 00444: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 2s 2ms/step - loss: 0.0127 - accuracy: 0.9973 - val_loss: 0.1332 - val_accuracy: 0.9815\n",
            "Epoch 445/3056\n",
            "815/821 [============================>.] - ETA: 0s - loss: 0.0129 - accuracy: 0.9969\n",
            "Epoch 00445: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 2s 3ms/step - loss: 0.0128 - accuracy: 0.9970 - val_loss: 0.1270 - val_accuracy: 0.9815\n",
            "Epoch 446/3056\n",
            "808/821 [============================>.] - ETA: 0s - loss: 0.0123 - accuracy: 0.9978\n",
            "Epoch 00446: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 2s 2ms/step - loss: 0.0121 - accuracy: 0.9979 - val_loss: 0.1303 - val_accuracy: 0.9815\n",
            "Epoch 447/3056\n",
            "807/821 [============================>.] - ETA: 0s - loss: 0.0111 - accuracy: 0.9978\n",
            "Epoch 00447: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0122 - accuracy: 0.9976 - val_loss: 0.1279 - val_accuracy: 0.9794\n",
            "Epoch 448/3056\n",
            "819/821 [============================>.] - ETA: 0s - loss: 0.0137 - accuracy: 0.9969\n",
            "Epoch 00448: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0137 - accuracy: 0.9970 - val_loss: 0.1252 - val_accuracy: 0.9787\n",
            "Epoch 449/3056\n",
            "821/821 [==============================] - ETA: 0s - loss: 0.0139 - accuracy: 0.9970\n",
            "Epoch 00449: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0139 - accuracy: 0.9970 - val_loss: 0.1327 - val_accuracy: 0.9794\n",
            "Epoch 450/3056\n",
            "786/821 [===========================>..] - ETA: 0s - loss: 0.0074 - accuracy: 0.9978\n",
            "Epoch 00450: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0135 - accuracy: 0.9973 - val_loss: 0.1369 - val_accuracy: 0.9801\n",
            "Epoch 451/3056\n",
            "814/821 [============================>.] - ETA: 0s - loss: 0.0151 - accuracy: 0.9969\n",
            "Epoch 00451: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0149 - accuracy: 0.9970 - val_loss: 0.1289 - val_accuracy: 0.9794\n",
            "Epoch 452/3056\n",
            "784/821 [===========================>..] - ETA: 0s - loss: 0.0132 - accuracy: 0.9965\n",
            "Epoch 00452: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0126 - accuracy: 0.9966 - val_loss: 0.1247 - val_accuracy: 0.9794\n",
            "Epoch 453/3056\n",
            "819/821 [============================>.] - ETA: 0s - loss: 0.0132 - accuracy: 0.9973\n",
            "Epoch 00453: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0132 - accuracy: 0.9973 - val_loss: 0.1368 - val_accuracy: 0.9808\n",
            "Epoch 454/3056\n",
            "784/821 [===========================>..] - ETA: 0s - loss: 0.0120 - accuracy: 0.9981\n",
            "Epoch 00454: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0134 - accuracy: 0.9973 - val_loss: 0.1354 - val_accuracy: 0.9780\n",
            "Epoch 455/3056\n",
            "817/821 [============================>.] - ETA: 0s - loss: 0.0104 - accuracy: 0.9976\n",
            "Epoch 00455: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0104 - accuracy: 0.9976 - val_loss: 0.1273 - val_accuracy: 0.9808\n",
            "Epoch 456/3056\n",
            "803/821 [============================>.] - ETA: 0s - loss: 0.0111 - accuracy: 0.9975\n",
            "Epoch 00456: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0114 - accuracy: 0.9973 - val_loss: 0.1373 - val_accuracy: 0.9822\n",
            "Epoch 457/3056\n",
            "779/821 [===========================>..] - ETA: 0s - loss: 0.0130 - accuracy: 0.9968\n",
            "Epoch 00457: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0124 - accuracy: 0.9970 - val_loss: 0.1283 - val_accuracy: 0.9787\n",
            "Epoch 458/3056\n",
            "783/821 [===========================>..] - ETA: 0s - loss: 0.0144 - accuracy: 0.9974\n",
            "Epoch 00458: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0138 - accuracy: 0.9976 - val_loss: 0.1251 - val_accuracy: 0.9794\n",
            "Epoch 459/3056\n",
            "779/821 [===========================>..] - ETA: 0s - loss: 0.0131 - accuracy: 0.9978\n",
            "Epoch 00459: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0124 - accuracy: 0.9979 - val_loss: 0.1255 - val_accuracy: 0.9780\n",
            "Epoch 460/3056\n",
            "798/821 [============================>.] - ETA: 0s - loss: 0.0121 - accuracy: 0.9969\n",
            "Epoch 00460: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0118 - accuracy: 0.9970 - val_loss: 0.1330 - val_accuracy: 0.9815\n",
            "Epoch 461/3056\n",
            "789/821 [===========================>..] - ETA: 0s - loss: 0.0152 - accuracy: 0.9965\n",
            "Epoch 00461: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0146 - accuracy: 0.9966 - val_loss: 0.1287 - val_accuracy: 0.9801\n",
            "Epoch 462/3056\n",
            "816/821 [============================>.] - ETA: 0s - loss: 0.0125 - accuracy: 0.9972\n",
            "Epoch 00462: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0125 - accuracy: 0.9973 - val_loss: 0.1261 - val_accuracy: 0.9794\n",
            "Epoch 463/3056\n",
            "813/821 [============================>.] - ETA: 0s - loss: 0.0126 - accuracy: 0.9969\n",
            "Epoch 00463: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0125 - accuracy: 0.9970 - val_loss: 0.1207 - val_accuracy: 0.9801\n",
            "Epoch 464/3056\n",
            "816/821 [============================>.] - ETA: 0s - loss: 0.0124 - accuracy: 0.9975\n",
            "Epoch 00464: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0124 - accuracy: 0.9976 - val_loss: 0.1291 - val_accuracy: 0.9787\n",
            "Epoch 465/3056\n",
            "814/821 [============================>.] - ETA: 0s - loss: 0.0136 - accuracy: 0.9972\n",
            "Epoch 00465: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0135 - accuracy: 0.9973 - val_loss: 0.1184 - val_accuracy: 0.9822\n",
            "Epoch 466/3056\n",
            "815/821 [============================>.] - ETA: 0s - loss: 0.0122 - accuracy: 0.9979\n",
            "Epoch 00466: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0124 - accuracy: 0.9976 - val_loss: 0.1295 - val_accuracy: 0.9801\n",
            "Epoch 467/3056\n",
            "820/821 [============================>.] - ETA: 0s - loss: 0.0131 - accuracy: 0.9970\n",
            "Epoch 00467: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0131 - accuracy: 0.9970 - val_loss: 0.1334 - val_accuracy: 0.9794\n",
            "Epoch 468/3056\n",
            "802/821 [============================>.] - ETA: 0s - loss: 0.0136 - accuracy: 0.9975\n",
            "Epoch 00468: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0133 - accuracy: 0.9976 - val_loss: 0.1283 - val_accuracy: 0.9822\n",
            "Epoch 469/3056\n",
            "811/821 [============================>.] - ETA: 0s - loss: 0.0125 - accuracy: 0.9978\n",
            "Epoch 00469: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0124 - accuracy: 0.9979 - val_loss: 0.1285 - val_accuracy: 0.9787\n",
            "Epoch 470/3056\n",
            "785/821 [===========================>..] - ETA: 0s - loss: 0.0129 - accuracy: 0.9971\n",
            "Epoch 00470: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0127 - accuracy: 0.9973 - val_loss: 0.1163 - val_accuracy: 0.9801\n",
            "Epoch 471/3056\n",
            "812/821 [============================>.] - ETA: 0s - loss: 0.0127 - accuracy: 0.9975\n",
            "Epoch 00471: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0126 - accuracy: 0.9976 - val_loss: 0.1159 - val_accuracy: 0.9801\n",
            "Epoch 472/3056\n",
            "807/821 [============================>.] - ETA: 0s - loss: 0.0124 - accuracy: 0.9969\n",
            "Epoch 00472: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0123 - accuracy: 0.9970 - val_loss: 0.1191 - val_accuracy: 0.9822\n",
            "Epoch 473/3056\n",
            "819/821 [============================>.] - ETA: 0s - loss: 0.0110 - accuracy: 0.9982\n",
            "Epoch 00473: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0110 - accuracy: 0.9982 - val_loss: 0.1195 - val_accuracy: 0.9808\n",
            "Epoch 474/3056\n",
            "782/821 [===========================>..] - ETA: 0s - loss: 0.0057 - accuracy: 0.9987\n",
            "Epoch 00474: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0120 - accuracy: 0.9973 - val_loss: 0.1351 - val_accuracy: 0.9787\n",
            "Epoch 475/3056\n",
            "803/821 [============================>.] - ETA: 0s - loss: 0.0125 - accuracy: 0.9969\n",
            "Epoch 00475: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0125 - accuracy: 0.9970 - val_loss: 0.1242 - val_accuracy: 0.9801\n",
            "Epoch 476/3056\n",
            "818/821 [============================>.] - ETA: 0s - loss: 0.0129 - accuracy: 0.9972\n",
            "Epoch 00476: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0129 - accuracy: 0.9973 - val_loss: 0.1371 - val_accuracy: 0.9794\n",
            "Epoch 477/3056\n",
            "815/821 [============================>.] - ETA: 0s - loss: 0.0096 - accuracy: 0.9972\n",
            "Epoch 00477: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0115 - accuracy: 0.9970 - val_loss: 0.1339 - val_accuracy: 0.9808\n",
            "Epoch 478/3056\n",
            "814/821 [============================>.] - ETA: 0s - loss: 0.0106 - accuracy: 0.9982\n",
            "Epoch 00478: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0105 - accuracy: 0.9982 - val_loss: 0.1308 - val_accuracy: 0.9808\n",
            "Epoch 479/3056\n",
            "808/821 [============================>.] - ETA: 0s - loss: 0.0107 - accuracy: 0.9978\n",
            "Epoch 00479: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0105 - accuracy: 0.9979 - val_loss: 0.1230 - val_accuracy: 0.9808\n",
            "Epoch 480/3056\n",
            "784/821 [===========================>..] - ETA: 0s - loss: 0.0136 - accuracy: 0.9974\n",
            "Epoch 00480: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0130 - accuracy: 0.9976 - val_loss: 0.1172 - val_accuracy: 0.9801\n",
            "Epoch 481/3056\n",
            "810/821 [============================>.] - ETA: 0s - loss: 0.0120 - accuracy: 0.9975\n",
            "Epoch 00481: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0118 - accuracy: 0.9976 - val_loss: 0.1173 - val_accuracy: 0.9822\n",
            "Epoch 482/3056\n",
            "792/821 [===========================>..] - ETA: 0s - loss: 0.0118 - accuracy: 0.9975\n",
            "Epoch 00482: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0114 - accuracy: 0.9976 - val_loss: 0.1226 - val_accuracy: 0.9808\n",
            "Epoch 483/3056\n",
            "782/821 [===========================>..] - ETA: 0s - loss: 0.0096 - accuracy: 0.9981\n",
            "Epoch 00483: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0093 - accuracy: 0.9982 - val_loss: 0.1240 - val_accuracy: 0.9822\n",
            "Epoch 484/3056\n",
            "797/821 [============================>.] - ETA: 0s - loss: 0.0122 - accuracy: 0.9975\n",
            "Epoch 00484: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0119 - accuracy: 0.9976 - val_loss: 0.1176 - val_accuracy: 0.9808\n",
            "Epoch 485/3056\n",
            "796/821 [============================>.] - ETA: 0s - loss: 0.0124 - accuracy: 0.9978\n",
            "Epoch 00485: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0124 - accuracy: 0.9976 - val_loss: 0.1263 - val_accuracy: 0.9822\n",
            "Epoch 486/3056\n",
            "787/821 [===========================>..] - ETA: 0s - loss: 0.0073 - accuracy: 0.9978\n",
            "Epoch 00486: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0123 - accuracy: 0.9976 - val_loss: 0.1242 - val_accuracy: 0.9815\n",
            "Epoch 487/3056\n",
            "797/821 [============================>.] - ETA: 0s - loss: 0.0104 - accuracy: 0.9978\n",
            "Epoch 00487: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0101 - accuracy: 0.9979 - val_loss: 0.1284 - val_accuracy: 0.9815\n",
            "Epoch 488/3056\n",
            "799/821 [============================>.] - ETA: 0s - loss: 0.0125 - accuracy: 0.9981\n",
            "Epoch 00488: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0122 - accuracy: 0.9982 - val_loss: 0.1205 - val_accuracy: 0.9815\n",
            "Epoch 489/3056\n",
            "797/821 [============================>.] - ETA: 0s - loss: 0.0114 - accuracy: 0.9978\n",
            "Epoch 00489: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0111 - accuracy: 0.9979 - val_loss: 0.1246 - val_accuracy: 0.9808\n",
            "Epoch 490/3056\n",
            "817/821 [============================>.] - ETA: 0s - loss: 0.0114 - accuracy: 0.9976\n",
            "Epoch 00490: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0114 - accuracy: 0.9976 - val_loss: 0.1285 - val_accuracy: 0.9808\n",
            "Epoch 491/3056\n",
            "821/821 [==============================] - ETA: 0s - loss: 0.0113 - accuracy: 0.9979\n",
            "Epoch 00491: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0113 - accuracy: 0.9979 - val_loss: 0.1142 - val_accuracy: 0.9808\n",
            "Epoch 492/3056\n",
            "787/821 [===========================>..] - ETA: 0s - loss: 0.0144 - accuracy: 0.9975\n",
            "Epoch 00492: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0138 - accuracy: 0.9976 - val_loss: 0.1177 - val_accuracy: 0.9815\n",
            "Epoch 493/3056\n",
            "779/821 [===========================>..] - ETA: 0s - loss: 0.0126 - accuracy: 0.9978\n",
            "Epoch 00493: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0120 - accuracy: 0.9979 - val_loss: 0.1208 - val_accuracy: 0.9851\n",
            "Epoch 494/3056\n",
            "777/821 [===========================>..] - ETA: 0s - loss: 0.0126 - accuracy: 0.9974\n",
            "Epoch 00494: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0119 - accuracy: 0.9976 - val_loss: 0.1194 - val_accuracy: 0.9808\n",
            "Epoch 495/3056\n",
            "810/821 [============================>.] - ETA: 0s - loss: 0.0128 - accuracy: 0.9978\n",
            "Epoch 00495: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0144 - accuracy: 0.9976 - val_loss: 0.1180 - val_accuracy: 0.9822\n",
            "Epoch 496/3056\n",
            "815/821 [============================>.] - ETA: 0s - loss: 0.0133 - accuracy: 0.9979\n",
            "Epoch 00496: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0132 - accuracy: 0.9979 - val_loss: 0.1145 - val_accuracy: 0.9822\n",
            "Epoch 497/3056\n",
            "813/821 [============================>.] - ETA: 0s - loss: 0.0135 - accuracy: 0.9978\n",
            "Epoch 00497: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0134 - accuracy: 0.9979 - val_loss: 0.1165 - val_accuracy: 0.9844\n",
            "Epoch 498/3056\n",
            "796/821 [============================>.] - ETA: 0s - loss: 0.0136 - accuracy: 0.9972\n",
            "Epoch 00498: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0132 - accuracy: 0.9973 - val_loss: 0.1154 - val_accuracy: 0.9815\n",
            "Epoch 499/3056\n",
            "819/821 [============================>.] - ETA: 0s - loss: 0.0103 - accuracy: 0.9979\n",
            "Epoch 00499: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0103 - accuracy: 0.9979 - val_loss: 0.1176 - val_accuracy: 0.9829\n",
            "Epoch 500/3056\n",
            "816/821 [============================>.] - ETA: 0s - loss: 0.0139 - accuracy: 0.9975\n",
            "Epoch 00500: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0138 - accuracy: 0.9976 - val_loss: 0.1186 - val_accuracy: 0.9815\n",
            "Epoch 501/3056\n",
            "778/821 [===========================>..] - ETA: 0s - loss: 0.0060 - accuracy: 0.9978\n",
            "Epoch 00501: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0132 - accuracy: 0.9973 - val_loss: 0.1189 - val_accuracy: 0.9829\n",
            "Epoch 502/3056\n",
            "813/821 [============================>.] - ETA: 0s - loss: 0.0125 - accuracy: 0.9978\n",
            "Epoch 00502: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0124 - accuracy: 0.9979 - val_loss: 0.1206 - val_accuracy: 0.9829\n",
            "Epoch 503/3056\n",
            "816/821 [============================>.] - ETA: 0s - loss: 0.0113 - accuracy: 0.9982\n",
            "Epoch 00503: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0112 - accuracy: 0.9982 - val_loss: 0.1160 - val_accuracy: 0.9851\n",
            "Epoch 504/3056\n",
            "790/821 [===========================>..] - ETA: 0s - loss: 0.0123 - accuracy: 0.9981\n",
            "Epoch 00504: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0118 - accuracy: 0.9982 - val_loss: 0.1230 - val_accuracy: 0.9808\n",
            "Epoch 505/3056\n",
            "787/821 [===========================>..] - ETA: 0s - loss: 0.0134 - accuracy: 0.9968\n",
            "Epoch 00505: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0129 - accuracy: 0.9970 - val_loss: 0.1252 - val_accuracy: 0.9829\n",
            "Epoch 506/3056\n",
            "780/821 [===========================>..] - ETA: 0s - loss: 0.0109 - accuracy: 0.9981\n",
            "Epoch 00506: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0104 - accuracy: 0.9982 - val_loss: 0.1203 - val_accuracy: 0.9822\n",
            "Epoch 507/3056\n",
            "812/821 [============================>.] - ETA: 0s - loss: 0.0128 - accuracy: 0.9978\n",
            "Epoch 00507: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0126 - accuracy: 0.9979 - val_loss: 0.1252 - val_accuracy: 0.9808\n",
            "Epoch 508/3056\n",
            "812/821 [============================>.] - ETA: 0s - loss: 0.0112 - accuracy: 0.9975\n",
            "Epoch 00508: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0111 - accuracy: 0.9976 - val_loss: 0.1252 - val_accuracy: 0.9822\n",
            "Epoch 509/3056\n",
            "804/821 [============================>.] - ETA: 0s - loss: 0.0115 - accuracy: 0.9972\n",
            "Epoch 00509: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0113 - accuracy: 0.9973 - val_loss: 0.1268 - val_accuracy: 0.9822\n",
            "Epoch 510/3056\n",
            "807/821 [============================>.] - ETA: 0s - loss: 0.0097 - accuracy: 0.9981\n",
            "Epoch 00510: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0096 - accuracy: 0.9982 - val_loss: 0.1246 - val_accuracy: 0.9836\n",
            "Epoch 511/3056\n",
            "793/821 [===========================>..] - ETA: 0s - loss: 0.0127 - accuracy: 0.9984\n",
            "Epoch 00511: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0124 - accuracy: 0.9985 - val_loss: 0.1289 - val_accuracy: 0.9815\n",
            "Epoch 512/3056\n",
            "790/821 [===========================>..] - ETA: 0s - loss: 0.0139 - accuracy: 0.9975\n",
            "Epoch 00512: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0134 - accuracy: 0.9976 - val_loss: 0.1215 - val_accuracy: 0.9822\n",
            "Epoch 513/3056\n",
            "816/821 [============================>.] - ETA: 0s - loss: 0.0105 - accuracy: 0.9982\n",
            "Epoch 00513: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0104 - accuracy: 0.9982 - val_loss: 0.1320 - val_accuracy: 0.9829\n",
            "Epoch 514/3056\n",
            "814/821 [============================>.] - ETA: 0s - loss: 0.0120 - accuracy: 0.9975\n",
            "Epoch 00514: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0119 - accuracy: 0.9976 - val_loss: 0.1182 - val_accuracy: 0.9844\n",
            "Epoch 515/3056\n",
            "812/821 [============================>.] - ETA: 0s - loss: 0.0117 - accuracy: 0.9978\n",
            "Epoch 00515: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0116 - accuracy: 0.9979 - val_loss: 0.1282 - val_accuracy: 0.9808\n",
            "Epoch 516/3056\n",
            "819/821 [============================>.] - ETA: 0s - loss: 0.0124 - accuracy: 0.9979\n",
            "Epoch 00516: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0124 - accuracy: 0.9979 - val_loss: 0.1356 - val_accuracy: 0.9808\n",
            "Epoch 517/3056\n",
            "814/821 [============================>.] - ETA: 0s - loss: 0.0114 - accuracy: 0.9979\n",
            "Epoch 00517: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0118 - accuracy: 0.9976 - val_loss: 0.1146 - val_accuracy: 0.9822\n",
            "Epoch 518/3056\n",
            "813/821 [============================>.] - ETA: 0s - loss: 0.0121 - accuracy: 0.9978\n",
            "Epoch 00518: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0120 - accuracy: 0.9979 - val_loss: 0.1168 - val_accuracy: 0.9815\n",
            "Epoch 519/3056\n",
            "777/821 [===========================>..] - ETA: 0s - loss: 0.0128 - accuracy: 0.9981\n",
            "Epoch 00519: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0121 - accuracy: 0.9982 - val_loss: 0.1092 - val_accuracy: 0.9844\n",
            "Epoch 520/3056\n",
            "818/821 [============================>.] - ETA: 0s - loss: 0.0106 - accuracy: 0.9982\n",
            "Epoch 00520: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0106 - accuracy: 0.9982 - val_loss: 0.1127 - val_accuracy: 0.9829\n",
            "Epoch 521/3056\n",
            "814/821 [============================>.] - ETA: 0s - loss: 0.0130 - accuracy: 0.9972\n",
            "Epoch 00521: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0129 - accuracy: 0.9973 - val_loss: 0.1120 - val_accuracy: 0.9822\n",
            "Epoch 522/3056\n",
            "800/821 [============================>.] - ETA: 0s - loss: 0.0113 - accuracy: 0.9978\n",
            "Epoch 00522: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0111 - accuracy: 0.9979 - val_loss: 0.1264 - val_accuracy: 0.9815\n",
            "Epoch 523/3056\n",
            "817/821 [============================>.] - ETA: 0s - loss: 0.0135 - accuracy: 0.9972\n",
            "Epoch 00523: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0135 - accuracy: 0.9973 - val_loss: 0.1191 - val_accuracy: 0.9822\n",
            "Epoch 524/3056\n",
            "808/821 [============================>.] - ETA: 0s - loss: 0.0127 - accuracy: 0.9985\n",
            "Epoch 00524: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0125 - accuracy: 0.9985 - val_loss: 0.1152 - val_accuracy: 0.9822\n",
            "Epoch 525/3056\n",
            "817/821 [============================>.] - ETA: 0s - loss: 0.0116 - accuracy: 0.9972\n",
            "Epoch 00525: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0116 - accuracy: 0.9973 - val_loss: 0.1294 - val_accuracy: 0.9815\n",
            "Epoch 526/3056\n",
            "785/821 [===========================>..] - ETA: 0s - loss: 0.0136 - accuracy: 0.9981\n",
            "Epoch 00526: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0133 - accuracy: 0.9979 - val_loss: 0.1115 - val_accuracy: 0.9815\n",
            "Epoch 527/3056\n",
            "782/821 [===========================>..] - ETA: 0s - loss: 0.0149 - accuracy: 0.9974\n",
            "Epoch 00527: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0142 - accuracy: 0.9976 - val_loss: 0.1122 - val_accuracy: 0.9829\n",
            "Epoch 528/3056\n",
            "818/821 [============================>.] - ETA: 0s - loss: 0.0139 - accuracy: 0.9976\n",
            "Epoch 00528: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0139 - accuracy: 0.9976 - val_loss: 0.1169 - val_accuracy: 0.9822\n",
            "Epoch 529/3056\n",
            "808/821 [============================>.] - ETA: 0s - loss: 0.0121 - accuracy: 0.9975\n",
            "Epoch 00529: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0119 - accuracy: 0.9976 - val_loss: 0.1201 - val_accuracy: 0.9808\n",
            "Epoch 530/3056\n",
            "818/821 [============================>.] - ETA: 0s - loss: 0.0129 - accuracy: 0.9976\n",
            "Epoch 00530: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0129 - accuracy: 0.9976 - val_loss: 0.1124 - val_accuracy: 0.9822\n",
            "Epoch 531/3056\n",
            "809/821 [============================>.] - ETA: 0s - loss: 0.0124 - accuracy: 0.9975\n",
            "Epoch 00531: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0123 - accuracy: 0.9976 - val_loss: 0.1262 - val_accuracy: 0.9836\n",
            "Epoch 532/3056\n",
            "784/821 [===========================>..] - ETA: 0s - loss: 0.0128 - accuracy: 0.9971\n",
            "Epoch 00532: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0129 - accuracy: 0.9970 - val_loss: 0.1291 - val_accuracy: 0.9829\n",
            "Epoch 533/3056\n",
            "805/821 [============================>.] - ETA: 0s - loss: 0.0112 - accuracy: 0.9981\n",
            "Epoch 00533: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0111 - accuracy: 0.9982 - val_loss: 0.1259 - val_accuracy: 0.9815\n",
            "Epoch 534/3056\n",
            "815/821 [============================>.] - ETA: 0s - loss: 0.0130 - accuracy: 0.9972\n",
            "Epoch 00534: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0130 - accuracy: 0.9973 - val_loss: 0.1245 - val_accuracy: 0.9829\n",
            "Epoch 535/3056\n",
            "802/821 [============================>.] - ETA: 0s - loss: 0.0122 - accuracy: 0.9978\n",
            "Epoch 00535: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0120 - accuracy: 0.9979 - val_loss: 0.1201 - val_accuracy: 0.9815\n",
            "Epoch 536/3056\n",
            "817/821 [============================>.] - ETA: 0s - loss: 0.0109 - accuracy: 0.9982\n",
            "Epoch 00536: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0109 - accuracy: 0.9982 - val_loss: 0.1313 - val_accuracy: 0.9844\n",
            "Epoch 537/3056\n",
            "812/821 [============================>.] - ETA: 0s - loss: 0.0106 - accuracy: 0.9982\n",
            "Epoch 00537: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0105 - accuracy: 0.9982 - val_loss: 0.1119 - val_accuracy: 0.9822\n",
            "Epoch 538/3056\n",
            "807/821 [============================>.] - ETA: 0s - loss: 0.0126 - accuracy: 0.9972\n",
            "Epoch 00538: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0124 - accuracy: 0.9973 - val_loss: 0.1220 - val_accuracy: 0.9836\n",
            "Epoch 539/3056\n",
            "819/821 [============================>.] - ETA: 0s - loss: 0.0132 - accuracy: 0.9982\n",
            "Epoch 00539: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0148 - accuracy: 0.9979 - val_loss: 0.1236 - val_accuracy: 0.9822\n",
            "Epoch 540/3056\n",
            "777/821 [===========================>..] - ETA: 0s - loss: 0.0127 - accuracy: 0.9977\n",
            "Epoch 00540: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0121 - accuracy: 0.9979 - val_loss: 0.1207 - val_accuracy: 0.9822\n",
            "Epoch 541/3056\n",
            "820/821 [============================>.] - ETA: 0s - loss: 0.0143 - accuracy: 0.9979\n",
            "Epoch 00541: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0143 - accuracy: 0.9979 - val_loss: 0.1171 - val_accuracy: 0.9815\n",
            "Epoch 542/3056\n",
            "780/821 [===========================>..] - ETA: 0s - loss: 0.0148 - accuracy: 0.9981\n",
            "Epoch 00542: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0140 - accuracy: 0.9982 - val_loss: 0.1216 - val_accuracy: 0.9822\n",
            "Epoch 543/3056\n",
            "806/821 [============================>.] - ETA: 0s - loss: 0.0141 - accuracy: 0.9978\n",
            "Epoch 00543: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0139 - accuracy: 0.9979 - val_loss: 0.1167 - val_accuracy: 0.9829\n",
            "Epoch 544/3056\n",
            "808/821 [============================>.] - ETA: 0s - loss: 0.0133 - accuracy: 0.9975\n",
            "Epoch 00544: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0131 - accuracy: 0.9976 - val_loss: 0.1067 - val_accuracy: 0.9815\n",
            "Epoch 545/3056\n",
            "812/821 [============================>.] - ETA: 0s - loss: 0.0126 - accuracy: 0.9978\n",
            "Epoch 00545: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0125 - accuracy: 0.9979 - val_loss: 0.1145 - val_accuracy: 0.9801\n",
            "Epoch 546/3056\n",
            "815/821 [============================>.] - ETA: 0s - loss: 0.0137 - accuracy: 0.9979\n",
            "Epoch 00546: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0136 - accuracy: 0.9979 - val_loss: 0.1178 - val_accuracy: 0.9836\n",
            "Epoch 547/3056\n",
            "812/821 [============================>.] - ETA: 0s - loss: 0.0144 - accuracy: 0.9975\n",
            "Epoch 00547: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0142 - accuracy: 0.9976 - val_loss: 0.1199 - val_accuracy: 0.9815\n",
            "Epoch 548/3056\n",
            "803/821 [============================>.] - ETA: 0s - loss: 0.0148 - accuracy: 0.9978\n",
            "Epoch 00548: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0145 - accuracy: 0.9979 - val_loss: 0.1438 - val_accuracy: 0.9844\n",
            "Epoch 549/3056\n",
            "778/821 [===========================>..] - ETA: 0s - loss: 0.0148 - accuracy: 0.9978\n",
            "Epoch 00549: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0140 - accuracy: 0.9979 - val_loss: 0.1156 - val_accuracy: 0.9822\n",
            "Epoch 550/3056\n",
            "811/821 [============================>.] - ETA: 0s - loss: 0.0120 - accuracy: 0.9978\n",
            "Epoch 00550: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0118 - accuracy: 0.9979 - val_loss: 0.1386 - val_accuracy: 0.9844\n",
            "Epoch 551/3056\n",
            "818/821 [============================>.] - ETA: 0s - loss: 0.0148 - accuracy: 0.9982\n",
            "Epoch 00551: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0147 - accuracy: 0.9982 - val_loss: 0.1405 - val_accuracy: 0.9836\n",
            "Epoch 552/3056\n",
            "779/821 [===========================>..] - ETA: 0s - loss: 0.0165 - accuracy: 0.9974\n",
            "Epoch 00552: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0157 - accuracy: 0.9976 - val_loss: 0.1225 - val_accuracy: 0.9836\n",
            "Epoch 553/3056\n",
            "798/821 [============================>.] - ETA: 0s - loss: 0.0061 - accuracy: 0.9987\n",
            "Epoch 00553: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0153 - accuracy: 0.9982 - val_loss: 0.1166 - val_accuracy: 0.9829\n",
            "Epoch 554/3056\n",
            "814/821 [============================>.] - ETA: 0s - loss: 0.0133 - accuracy: 0.9975\n",
            "Epoch 00554: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0132 - accuracy: 0.9976 - val_loss: 0.1247 - val_accuracy: 0.9822\n",
            "Epoch 555/3056\n",
            "818/821 [============================>.] - ETA: 0s - loss: 0.0134 - accuracy: 0.9979\n",
            "Epoch 00555: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0133 - accuracy: 0.9979 - val_loss: 0.1206 - val_accuracy: 0.9815\n",
            "Epoch 556/3056\n",
            "784/821 [===========================>..] - ETA: 0s - loss: 0.0133 - accuracy: 0.9978\n",
            "Epoch 00556: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0144 - accuracy: 0.9976 - val_loss: 0.1258 - val_accuracy: 0.9822\n",
            "Epoch 557/3056\n",
            "777/821 [===========================>..] - ETA: 0s - loss: 0.0156 - accuracy: 0.9977\n",
            "Epoch 00557: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0148 - accuracy: 0.9979 - val_loss: 0.1272 - val_accuracy: 0.9836\n",
            "Epoch 558/3056\n",
            "804/821 [============================>.] - ETA: 0s - loss: 0.0134 - accuracy: 0.9978\n",
            "Epoch 00558: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0132 - accuracy: 0.9979 - val_loss: 0.1305 - val_accuracy: 0.9836\n",
            "Epoch 559/3056\n",
            "802/821 [============================>.] - ETA: 0s - loss: 0.0144 - accuracy: 0.9984\n",
            "Epoch 00559: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0141 - accuracy: 0.9985 - val_loss: 0.1304 - val_accuracy: 0.9836\n",
            "Epoch 560/3056\n",
            "802/821 [============================>.] - ETA: 0s - loss: 0.0116 - accuracy: 0.9981\n",
            "Epoch 00560: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0114 - accuracy: 0.9982 - val_loss: 0.1499 - val_accuracy: 0.9829\n",
            "Epoch 561/3056\n",
            "813/821 [============================>.] - ETA: 0s - loss: 0.0140 - accuracy: 0.9975\n",
            "Epoch 00561: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0138 - accuracy: 0.9976 - val_loss: 0.1278 - val_accuracy: 0.9815\n",
            "Epoch 562/3056\n",
            "811/821 [============================>.] - ETA: 0s - loss: 0.0149 - accuracy: 0.9978\n",
            "Epoch 00562: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0149 - accuracy: 0.9979 - val_loss: 0.1264 - val_accuracy: 0.9829\n",
            "Epoch 563/3056\n",
            "787/821 [===========================>..] - ETA: 0s - loss: 0.0124 - accuracy: 0.9984\n",
            "Epoch 00563: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0119 - accuracy: 0.9985 - val_loss: 0.1260 - val_accuracy: 0.9815\n",
            "Epoch 564/3056\n",
            "815/821 [============================>.] - ETA: 0s - loss: 0.0127 - accuracy: 0.9982\n",
            "Epoch 00564: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0126 - accuracy: 0.9982 - val_loss: 0.1308 - val_accuracy: 0.9815\n",
            "Epoch 565/3056\n",
            "811/821 [============================>.] - ETA: 0s - loss: 0.0130 - accuracy: 0.9982\n",
            "Epoch 00565: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0129 - accuracy: 0.9982 - val_loss: 0.1460 - val_accuracy: 0.9822\n",
            "Epoch 566/3056\n",
            "781/821 [===========================>..] - ETA: 0s - loss: 0.0076 - accuracy: 0.9981\n",
            "Epoch 00566: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0138 - accuracy: 0.9976 - val_loss: 0.1540 - val_accuracy: 0.9822\n",
            "Epoch 567/3056\n",
            "821/821 [==============================] - ETA: 0s - loss: 0.0137 - accuracy: 0.9979\n",
            "Epoch 00567: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0137 - accuracy: 0.9979 - val_loss: 0.1467 - val_accuracy: 0.9808\n",
            "Epoch 568/3056\n",
            "821/821 [==============================] - ETA: 0s - loss: 0.0130 - accuracy: 0.9979\n",
            "Epoch 00568: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0130 - accuracy: 0.9979 - val_loss: 0.1384 - val_accuracy: 0.9822\n",
            "Epoch 569/3056\n",
            "812/821 [============================>.] - ETA: 0s - loss: 0.0148 - accuracy: 0.9978\n",
            "Epoch 00569: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0146 - accuracy: 0.9979 - val_loss: 0.1473 - val_accuracy: 0.9808\n",
            "Epoch 570/3056\n",
            "812/821 [============================>.] - ETA: 0s - loss: 0.0150 - accuracy: 0.9969\n",
            "Epoch 00570: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0148 - accuracy: 0.9970 - val_loss: 0.1601 - val_accuracy: 0.9815\n",
            "Epoch 571/3056\n",
            "815/821 [============================>.] - ETA: 0s - loss: 0.0155 - accuracy: 0.9972\n",
            "Epoch 00571: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0154 - accuracy: 0.9973 - val_loss: 0.1595 - val_accuracy: 0.9822\n",
            "Epoch 572/3056\n",
            "811/821 [============================>.] - ETA: 0s - loss: 0.0123 - accuracy: 0.9982\n",
            "Epoch 00572: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0121 - accuracy: 0.9982 - val_loss: 0.1504 - val_accuracy: 0.9808\n",
            "Epoch 573/3056\n",
            "806/821 [============================>.] - ETA: 0s - loss: 0.0140 - accuracy: 0.9972\n",
            "Epoch 00573: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0138 - accuracy: 0.9973 - val_loss: 0.1554 - val_accuracy: 0.9815\n",
            "Epoch 574/3056\n",
            "785/821 [===========================>..] - ETA: 0s - loss: 0.0099 - accuracy: 0.9987\n",
            "Epoch 00574: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0115 - accuracy: 0.9985 - val_loss: 0.1841 - val_accuracy: 0.9815\n",
            "Epoch 575/3056\n",
            "778/821 [===========================>..] - ETA: 0s - loss: 0.0157 - accuracy: 0.9978\n",
            "Epoch 00575: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0149 - accuracy: 0.9979 - val_loss: 0.1461 - val_accuracy: 0.9822\n",
            "Epoch 576/3056\n",
            "818/821 [============================>.] - ETA: 0s - loss: 0.0125 - accuracy: 0.9979\n",
            "Epoch 00576: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0124 - accuracy: 0.9979 - val_loss: 0.1475 - val_accuracy: 0.9822\n",
            "Epoch 577/3056\n",
            "808/821 [============================>.] - ETA: 0s - loss: 0.0127 - accuracy: 0.9978\n",
            "Epoch 00577: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0125 - accuracy: 0.9979 - val_loss: 0.1432 - val_accuracy: 0.9829\n",
            "Epoch 578/3056\n",
            "812/821 [============================>.] - ETA: 0s - loss: 0.0137 - accuracy: 0.9978\n",
            "Epoch 00578: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0136 - accuracy: 0.9979 - val_loss: 0.1436 - val_accuracy: 0.9815\n",
            "Epoch 579/3056\n",
            "814/821 [============================>.] - ETA: 0s - loss: 0.0128 - accuracy: 0.9979\n",
            "Epoch 00579: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0127 - accuracy: 0.9979 - val_loss: 0.1508 - val_accuracy: 0.9822\n",
            "Epoch 580/3056\n",
            "804/821 [============================>.] - ETA: 0s - loss: 0.0127 - accuracy: 0.9978\n",
            "Epoch 00580: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0124 - accuracy: 0.9979 - val_loss: 0.1495 - val_accuracy: 0.9815\n",
            "Epoch 581/3056\n",
            "815/821 [============================>.] - ETA: 0s - loss: 0.0116 - accuracy: 0.9982\n",
            "Epoch 00581: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0115 - accuracy: 0.9982 - val_loss: 0.1597 - val_accuracy: 0.9822\n",
            "Epoch 582/3056\n",
            "786/821 [===========================>..] - ETA: 0s - loss: 0.0145 - accuracy: 0.9975\n",
            "Epoch 00582: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0139 - accuracy: 0.9976 - val_loss: 0.1528 - val_accuracy: 0.9815\n",
            "Epoch 583/3056\n",
            "818/821 [============================>.] - ETA: 0s - loss: 0.0123 - accuracy: 0.9982\n",
            "Epoch 00583: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0123 - accuracy: 0.9982 - val_loss: 0.1634 - val_accuracy: 0.9822\n",
            "Epoch 584/3056\n",
            "814/821 [============================>.] - ETA: 0s - loss: 0.0106 - accuracy: 0.9972\n",
            "Epoch 00584: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0105 - accuracy: 0.9973 - val_loss: 0.1549 - val_accuracy: 0.9822\n",
            "Epoch 585/3056\n",
            "804/821 [============================>.] - ETA: 0s - loss: 0.0117 - accuracy: 0.9978\n",
            "Epoch 00585: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0115 - accuracy: 0.9979 - val_loss: 0.1532 - val_accuracy: 0.9815\n",
            "Epoch 586/3056\n",
            "811/821 [============================>.] - ETA: 0s - loss: 0.0116 - accuracy: 0.9982\n",
            "Epoch 00586: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0114 - accuracy: 0.9982 - val_loss: 0.1787 - val_accuracy: 0.9808\n",
            "Epoch 587/3056\n",
            "805/821 [============================>.] - ETA: 0s - loss: 0.0130 - accuracy: 0.9981\n",
            "Epoch 00587: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0128 - accuracy: 0.9982 - val_loss: 0.1599 - val_accuracy: 0.9829\n",
            "Epoch 588/3056\n",
            "805/821 [============================>.] - ETA: 0s - loss: 0.0117 - accuracy: 0.9981\n",
            "Epoch 00588: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0127 - accuracy: 0.9979 - val_loss: 0.1711 - val_accuracy: 0.9829\n",
            "Epoch 589/3056\n",
            "819/821 [============================>.] - ETA: 0s - loss: 0.0117 - accuracy: 0.9979\n",
            "Epoch 00589: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0117 - accuracy: 0.9979 - val_loss: 0.1820 - val_accuracy: 0.9822\n",
            "Epoch 590/3056\n",
            "812/821 [============================>.] - ETA: 0s - loss: 0.0111 - accuracy: 0.9978\n",
            "Epoch 00590: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0116 - accuracy: 0.9976 - val_loss: 0.1656 - val_accuracy: 0.9822\n",
            "Epoch 591/3056\n",
            "798/821 [============================>.] - ETA: 0s - loss: 0.0105 - accuracy: 0.9978\n",
            "Epoch 00591: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0102 - accuracy: 0.9979 - val_loss: 0.1798 - val_accuracy: 0.9815\n",
            "Epoch 592/3056\n",
            "801/821 [============================>.] - ETA: 0s - loss: 0.0097 - accuracy: 0.9972\n",
            "Epoch 00592: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0094 - accuracy: 0.9973 - val_loss: 0.1755 - val_accuracy: 0.9808\n",
            "Epoch 593/3056\n",
            "808/821 [============================>.] - ETA: 0s - loss: 0.0115 - accuracy: 0.9978\n",
            "Epoch 00593: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0113 - accuracy: 0.9979 - val_loss: 0.1811 - val_accuracy: 0.9815\n",
            "Epoch 594/3056\n",
            "810/821 [============================>.] - ETA: 0s - loss: 0.0125 - accuracy: 0.9981\n",
            "Epoch 00594: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0124 - accuracy: 0.9982 - val_loss: 0.1674 - val_accuracy: 0.9815\n",
            "Epoch 595/3056\n",
            "814/821 [============================>.] - ETA: 0s - loss: 0.0134 - accuracy: 0.9982\n",
            "Epoch 00595: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0133 - accuracy: 0.9982 - val_loss: 0.1822 - val_accuracy: 0.9808\n",
            "Epoch 596/3056\n",
            "807/821 [============================>.] - ETA: 0s - loss: 0.0118 - accuracy: 0.9981\n",
            "Epoch 00596: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0116 - accuracy: 0.9982 - val_loss: 0.1893 - val_accuracy: 0.9808\n",
            "Epoch 597/3056\n",
            "816/821 [============================>.] - ETA: 0s - loss: 0.0098 - accuracy: 0.9985\n",
            "Epoch 00597: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0098 - accuracy: 0.9985 - val_loss: 0.1902 - val_accuracy: 0.9808\n",
            "Epoch 598/3056\n",
            "814/821 [============================>.] - ETA: 0s - loss: 0.0116 - accuracy: 0.9979\n",
            "Epoch 00598: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0115 - accuracy: 0.9979 - val_loss: 0.2047 - val_accuracy: 0.9801\n",
            "Epoch 599/3056\n",
            "813/821 [============================>.] - ETA: 0s - loss: 0.0105 - accuracy: 0.9982\n",
            "Epoch 00599: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0104 - accuracy: 0.9982 - val_loss: 0.1817 - val_accuracy: 0.9822\n",
            "Epoch 600/3056\n",
            "816/821 [============================>.] - ETA: 0s - loss: 0.0105 - accuracy: 0.9988\n",
            "Epoch 00600: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0105 - accuracy: 0.9988 - val_loss: 0.1947 - val_accuracy: 0.9801\n",
            "Epoch 601/3056\n",
            "782/821 [===========================>..] - ETA: 0s - loss: 0.0124 - accuracy: 0.9981\n",
            "Epoch 00601: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0118 - accuracy: 0.9982 - val_loss: 0.1770 - val_accuracy: 0.9822\n",
            "Epoch 602/3056\n",
            "808/821 [============================>.] - ETA: 0s - loss: 0.0108 - accuracy: 0.9981\n",
            "Epoch 00602: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 2s 2ms/step - loss: 0.0107 - accuracy: 0.9982 - val_loss: 0.1821 - val_accuracy: 0.9815\n",
            "Epoch 603/3056\n",
            "816/821 [============================>.] - ETA: 0s - loss: 0.0104 - accuracy: 0.9982\n",
            "Epoch 00603: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0104 - accuracy: 0.9982 - val_loss: 0.1738 - val_accuracy: 0.9808\n",
            "Epoch 604/3056\n",
            "820/821 [============================>.] - ETA: 0s - loss: 0.0113 - accuracy: 0.9982\n",
            "Epoch 00604: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0113 - accuracy: 0.9982 - val_loss: 0.2220 - val_accuracy: 0.9801\n",
            "Epoch 605/3056\n",
            "821/821 [==============================] - ETA: 0s - loss: 0.0109 - accuracy: 0.9982\n",
            "Epoch 00605: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0109 - accuracy: 0.9982 - val_loss: 0.1758 - val_accuracy: 0.9822\n",
            "Epoch 606/3056\n",
            "807/821 [============================>.] - ETA: 0s - loss: 0.0090 - accuracy: 0.9985\n",
            "Epoch 00606: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0115 - accuracy: 0.9982 - val_loss: 0.1934 - val_accuracy: 0.9815\n",
            "Epoch 607/3056\n",
            "813/821 [============================>.] - ETA: 0s - loss: 0.0113 - accuracy: 0.9978\n",
            "Epoch 00607: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0113 - accuracy: 0.9979 - val_loss: 0.2057 - val_accuracy: 0.9815\n",
            "Epoch 608/3056\n",
            "777/821 [===========================>..] - ETA: 0s - loss: 0.0108 - accuracy: 0.9977\n",
            "Epoch 00608: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0105 - accuracy: 0.9976 - val_loss: 0.2039 - val_accuracy: 0.9801\n",
            "Epoch 609/3056\n",
            "813/821 [============================>.] - ETA: 0s - loss: 0.0111 - accuracy: 0.9978\n",
            "Epoch 00609: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0110 - accuracy: 0.9979 - val_loss: 0.2137 - val_accuracy: 0.9822\n",
            "Epoch 610/3056\n",
            "815/821 [============================>.] - ETA: 0s - loss: 0.0113 - accuracy: 0.9982\n",
            "Epoch 00610: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0112 - accuracy: 0.9982 - val_loss: 0.1808 - val_accuracy: 0.9808\n",
            "Epoch 611/3056\n",
            "813/821 [============================>.] - ETA: 0s - loss: 0.0108 - accuracy: 0.9985\n",
            "Epoch 00611: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0107 - accuracy: 0.9985 - val_loss: 0.1967 - val_accuracy: 0.9808\n",
            "Epoch 612/3056\n",
            "808/821 [============================>.] - ETA: 0s - loss: 0.0113 - accuracy: 0.9978\n",
            "Epoch 00612: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0111 - accuracy: 0.9979 - val_loss: 0.1988 - val_accuracy: 0.9808\n",
            "Epoch 613/3056\n",
            "815/821 [============================>.] - ETA: 0s - loss: 0.0092 - accuracy: 0.9982\n",
            "Epoch 00613: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0091 - accuracy: 0.9982 - val_loss: 0.1778 - val_accuracy: 0.9822\n",
            "Epoch 614/3056\n",
            "821/821 [==============================] - ETA: 0s - loss: 0.0117 - accuracy: 0.9979\n",
            "Epoch 00614: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0117 - accuracy: 0.9979 - val_loss: 0.1740 - val_accuracy: 0.9822\n",
            "Epoch 615/3056\n",
            "814/821 [============================>.] - ETA: 0s - loss: 0.0105 - accuracy: 0.9988\n",
            "Epoch 00615: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0104 - accuracy: 0.9988 - val_loss: 0.1654 - val_accuracy: 0.9815\n",
            "Epoch 616/3056\n",
            "818/821 [============================>.] - ETA: 0s - loss: 0.0108 - accuracy: 0.9979\n",
            "Epoch 00616: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0108 - accuracy: 0.9979 - val_loss: 0.1786 - val_accuracy: 0.9815\n",
            "Epoch 617/3056\n",
            "818/821 [============================>.] - ETA: 0s - loss: 0.0098 - accuracy: 0.9982\n",
            "Epoch 00617: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0098 - accuracy: 0.9982 - val_loss: 0.1879 - val_accuracy: 0.9815\n",
            "Epoch 618/3056\n",
            "817/821 [============================>.] - ETA: 0s - loss: 0.0114 - accuracy: 0.9985\n",
            "Epoch 00618: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0114 - accuracy: 0.9985 - val_loss: 0.1683 - val_accuracy: 0.9822\n",
            "Epoch 619/3056\n",
            "814/821 [============================>.] - ETA: 0s - loss: 0.0111 - accuracy: 0.9979\n",
            "Epoch 00619: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0110 - accuracy: 0.9979 - val_loss: 0.1878 - val_accuracy: 0.9822\n",
            "Epoch 620/3056\n",
            "789/821 [===========================>..] - ETA: 0s - loss: 0.0090 - accuracy: 0.9987\n",
            "Epoch 00620: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0103 - accuracy: 0.9985 - val_loss: 0.1870 - val_accuracy: 0.9829\n",
            "Epoch 621/3056\n",
            "804/821 [============================>.] - ETA: 0s - loss: 0.0051 - accuracy: 0.9988\n",
            "Epoch 00621: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0102 - accuracy: 0.9985 - val_loss: 0.1893 - val_accuracy: 0.9815\n",
            "Epoch 622/3056\n",
            "796/821 [============================>.] - ETA: 0s - loss: 0.0101 - accuracy: 0.9987\n",
            "Epoch 00622: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0101 - accuracy: 0.9985 - val_loss: 0.1937 - val_accuracy: 0.9815\n",
            "Epoch 623/3056\n",
            "792/821 [===========================>..] - ETA: 0s - loss: 0.0108 - accuracy: 0.9981\n",
            "Epoch 00623: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0104 - accuracy: 0.9982 - val_loss: 0.1810 - val_accuracy: 0.9829\n",
            "Epoch 624/3056\n",
            "788/821 [===========================>..] - ETA: 0s - loss: 0.0111 - accuracy: 0.9984\n",
            "Epoch 00624: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0107 - accuracy: 0.9985 - val_loss: 0.1935 - val_accuracy: 0.9808\n",
            "Epoch 625/3056\n",
            "812/821 [============================>.] - ETA: 0s - loss: 0.0106 - accuracy: 0.9991\n",
            "Epoch 00625: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0105 - accuracy: 0.9991 - val_loss: 0.1690 - val_accuracy: 0.9836\n",
            "Epoch 626/3056\n",
            "815/821 [============================>.] - ETA: 0s - loss: 0.0112 - accuracy: 0.9988\n",
            "Epoch 00626: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0111 - accuracy: 0.9988 - val_loss: 0.1787 - val_accuracy: 0.9815\n",
            "Epoch 627/3056\n",
            "810/821 [============================>.] - ETA: 0s - loss: 0.0117 - accuracy: 0.9981\n",
            "Epoch 00627: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0117 - accuracy: 0.9982 - val_loss: 0.1911 - val_accuracy: 0.9822\n",
            "Epoch 628/3056\n",
            "780/821 [===========================>..] - ETA: 0s - loss: 0.0109 - accuracy: 0.9987\n",
            "Epoch 00628: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0104 - accuracy: 0.9988 - val_loss: 0.1782 - val_accuracy: 0.9836\n",
            "Epoch 629/3056\n",
            "783/821 [===========================>..] - ETA: 0s - loss: 0.0118 - accuracy: 0.9978\n",
            "Epoch 00629: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0113 - accuracy: 0.9979 - val_loss: 0.1738 - val_accuracy: 0.9829\n",
            "Epoch 630/3056\n",
            "820/821 [============================>.] - ETA: 0s - loss: 0.0111 - accuracy: 0.9982\n",
            "Epoch 00630: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0111 - accuracy: 0.9982 - val_loss: 0.1655 - val_accuracy: 0.9815\n",
            "Epoch 631/3056\n",
            "816/821 [============================>.] - ETA: 0s - loss: 0.0110 - accuracy: 0.9985\n",
            "Epoch 00631: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0109 - accuracy: 0.9985 - val_loss: 0.1729 - val_accuracy: 0.9836\n",
            "Epoch 632/3056\n",
            "812/821 [============================>.] - ETA: 0s - loss: 0.0104 - accuracy: 0.9985\n",
            "Epoch 00632: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0103 - accuracy: 0.9985 - val_loss: 0.1838 - val_accuracy: 0.9836\n",
            "Epoch 633/3056\n",
            "794/821 [============================>.] - ETA: 0s - loss: 0.0108 - accuracy: 0.9987\n",
            "Epoch 00633: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0105 - accuracy: 0.9988 - val_loss: 0.1779 - val_accuracy: 0.9836\n",
            "Epoch 634/3056\n",
            "818/821 [============================>.] - ETA: 0s - loss: 0.0117 - accuracy: 0.9979\n",
            "Epoch 00634: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0117 - accuracy: 0.9979 - val_loss: 0.1766 - val_accuracy: 0.9836\n",
            "Epoch 635/3056\n",
            "814/821 [============================>.] - ETA: 0s - loss: 0.0112 - accuracy: 0.9979\n",
            "Epoch 00635: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0111 - accuracy: 0.9979 - val_loss: 0.1800 - val_accuracy: 0.9844\n",
            "Epoch 636/3056\n",
            "814/821 [============================>.] - ETA: 0s - loss: 0.0118 - accuracy: 0.9985\n",
            "Epoch 00636: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0117 - accuracy: 0.9985 - val_loss: 0.1869 - val_accuracy: 0.9851\n",
            "Epoch 637/3056\n",
            "788/821 [===========================>..] - ETA: 0s - loss: 0.0104 - accuracy: 0.9984\n",
            "Epoch 00637: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0100 - accuracy: 0.9985 - val_loss: 0.1920 - val_accuracy: 0.9822\n",
            "Epoch 638/3056\n",
            "811/821 [============================>.] - ETA: 0s - loss: 0.0109 - accuracy: 0.9982\n",
            "Epoch 00638: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0109 - accuracy: 0.9982 - val_loss: 0.1953 - val_accuracy: 0.9822\n",
            "Epoch 639/3056\n",
            "784/821 [===========================>..] - ETA: 0s - loss: 0.0115 - accuracy: 0.9984\n",
            "Epoch 00639: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0112 - accuracy: 0.9982 - val_loss: 0.1925 - val_accuracy: 0.9829\n",
            "Epoch 640/3056\n",
            "808/821 [============================>.] - ETA: 0s - loss: 0.0128 - accuracy: 0.9978\n",
            "Epoch 00640: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0127 - accuracy: 0.9979 - val_loss: 0.1759 - val_accuracy: 0.9844\n",
            "Epoch 641/3056\n",
            "812/821 [============================>.] - ETA: 0s - loss: 0.0118 - accuracy: 0.9985\n",
            "Epoch 00641: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0117 - accuracy: 0.9985 - val_loss: 0.1983 - val_accuracy: 0.9836\n",
            "Epoch 642/3056\n",
            "817/821 [============================>.] - ETA: 0s - loss: 0.0106 - accuracy: 0.9979\n",
            "Epoch 00642: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0106 - accuracy: 0.9979 - val_loss: 0.1793 - val_accuracy: 0.9829\n",
            "Epoch 643/3056\n",
            "813/821 [============================>.] - ETA: 0s - loss: 0.0103 - accuracy: 0.9988\n",
            "Epoch 00643: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0102 - accuracy: 0.9988 - val_loss: 0.1728 - val_accuracy: 0.9822\n",
            "Epoch 644/3056\n",
            "812/821 [============================>.] - ETA: 0s - loss: 0.0101 - accuracy: 0.9982\n",
            "Epoch 00644: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0100 - accuracy: 0.9982 - val_loss: 0.1832 - val_accuracy: 0.9829\n",
            "Epoch 645/3056\n",
            "812/821 [============================>.] - ETA: 0s - loss: 0.0103 - accuracy: 0.9982\n",
            "Epoch 00645: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0102 - accuracy: 0.9982 - val_loss: 0.1920 - val_accuracy: 0.9829\n",
            "Epoch 646/3056\n",
            "807/821 [============================>.] - ETA: 0s - loss: 0.0099 - accuracy: 0.9981\n",
            "Epoch 00646: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0097 - accuracy: 0.9982 - val_loss: 0.1791 - val_accuracy: 0.9822\n",
            "Epoch 647/3056\n",
            "802/821 [============================>.] - ETA: 0s - loss: 0.0091 - accuracy: 0.9978\n",
            "Epoch 00647: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0089 - accuracy: 0.9979 - val_loss: 0.1957 - val_accuracy: 0.9815\n",
            "Epoch 648/3056\n",
            "803/821 [============================>.] - ETA: 0s - loss: 0.0077 - accuracy: 0.9984\n",
            "Epoch 00648: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0097 - accuracy: 0.9982 - val_loss: 0.1902 - val_accuracy: 0.9808\n",
            "Epoch 649/3056\n",
            "813/821 [============================>.] - ETA: 0s - loss: 0.0096 - accuracy: 0.9982\n",
            "Epoch 00649: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0095 - accuracy: 0.9982 - val_loss: 0.1751 - val_accuracy: 0.9815\n",
            "Epoch 650/3056\n",
            "805/821 [============================>.] - ETA: 0s - loss: 0.0096 - accuracy: 0.9984\n",
            "Epoch 00650: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0095 - accuracy: 0.9985 - val_loss: 0.1779 - val_accuracy: 0.9815\n",
            "Epoch 651/3056\n",
            "814/821 [============================>.] - ETA: 0s - loss: 0.0103 - accuracy: 0.9982\n",
            "Epoch 00651: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0103 - accuracy: 0.9982 - val_loss: 0.1764 - val_accuracy: 0.9822\n",
            "Epoch 652/3056\n",
            "810/821 [============================>.] - ETA: 0s - loss: 0.0089 - accuracy: 0.9978\n",
            "Epoch 00652: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0088 - accuracy: 0.9979 - val_loss: 0.1694 - val_accuracy: 0.9808\n",
            "Epoch 653/3056\n",
            "804/821 [============================>.] - ETA: 0s - loss: 0.0108 - accuracy: 0.9984\n",
            "Epoch 00653: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0106 - accuracy: 0.9985 - val_loss: 0.1651 - val_accuracy: 0.9808\n",
            "Epoch 654/3056\n",
            "812/821 [============================>.] - ETA: 0s - loss: 0.0103 - accuracy: 0.9982\n",
            "Epoch 00654: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0102 - accuracy: 0.9982 - val_loss: 0.1789 - val_accuracy: 0.9822\n",
            "Epoch 655/3056\n",
            "821/821 [==============================] - ETA: 0s - loss: 0.0091 - accuracy: 0.9988\n",
            "Epoch 00655: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0091 - accuracy: 0.9988 - val_loss: 0.1784 - val_accuracy: 0.9808\n",
            "Epoch 656/3056\n",
            "811/821 [============================>.] - ETA: 0s - loss: 0.0037 - accuracy: 0.9985\n",
            "Epoch 00656: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0095 - accuracy: 0.9982 - val_loss: 0.1801 - val_accuracy: 0.9808\n",
            "Epoch 657/3056\n",
            "807/821 [============================>.] - ETA: 0s - loss: 0.0098 - accuracy: 0.9985\n",
            "Epoch 00657: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0098 - accuracy: 0.9985 - val_loss: 0.1763 - val_accuracy: 0.9822\n",
            "Epoch 658/3056\n",
            "806/821 [============================>.] - ETA: 0s - loss: 0.0092 - accuracy: 0.9984\n",
            "Epoch 00658: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0090 - accuracy: 0.9985 - val_loss: 0.1749 - val_accuracy: 0.9801\n",
            "Epoch 659/3056\n",
            "804/821 [============================>.] - ETA: 0s - loss: 0.0097 - accuracy: 0.9984\n",
            "Epoch 00659: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0095 - accuracy: 0.9985 - val_loss: 0.1817 - val_accuracy: 0.9822\n",
            "Epoch 660/3056\n",
            "783/821 [===========================>..] - ETA: 0s - loss: 0.0100 - accuracy: 0.9984\n",
            "Epoch 00660: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0096 - accuracy: 0.9985 - val_loss: 0.1692 - val_accuracy: 0.9822\n",
            "Epoch 661/3056\n",
            "819/821 [============================>.] - ETA: 0s - loss: 0.0089 - accuracy: 0.9982\n",
            "Epoch 00661: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0088 - accuracy: 0.9982 - val_loss: 0.1658 - val_accuracy: 0.9808\n",
            "Epoch 662/3056\n",
            "803/821 [============================>.] - ETA: 0s - loss: 0.0093 - accuracy: 0.9988\n",
            "Epoch 00662: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0091 - accuracy: 0.9988 - val_loss: 0.1748 - val_accuracy: 0.9822\n",
            "Epoch 663/3056\n",
            "780/821 [===========================>..] - ETA: 0s - loss: 0.0100 - accuracy: 0.9984\n",
            "Epoch 00663: val_loss did not improve from 0.04948\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.0095 - accuracy: 0.9985 - val_loss: 0.1782 - val_accuracy: 0.9808\n",
            "Epoch 00663: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-Iw9StsM5j7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSDJqQID-zlH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "2d374ae1-99d1-4d82-c951-6f128d7ad77e"
      },
      "source": [
        "model.load_weights('Reduced_Nasa_asteroid.hdf5')\n",
        "prediction = model.predict(XVALID)\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "accuracy = accuracy_score(YVALID, prediction.round())\n",
        "precision = precision_score(YVALID, prediction.round())\n",
        "recall = recall_score(YVALID, prediction.round())\n",
        "f1score = f1_score(YVALID, prediction.round())\n",
        "print(\"Multilayer NN Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
        "print(\"Precision: %.2f%%\" % (precision * 100.0))\n",
        "print(\"Recall: %.2f%%\" % (recall * 100.0))\n",
        "print(\"F1-score: %.2f\" % (f1score*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Multilayer NN Accuracy: 98.65%\n",
            "Precision: 96.86%\n",
            "Recall: 94.74%\n",
            "F1-score: 95.79\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MscAotbT0aWc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "f38384f6-2946-4ab3-d48e-4662c18e40a4"
      },
      "source": [
        "model.load_weights('Reduced_Nasa_asteroid.hdf5')\n",
        "prediction=model.predict(XTRAIN)\n",
        "accuracy = accuracy_score(YTRAIN, prediction.round())\n",
        "precision = precision_score(YTRAIN, prediction.round())\n",
        "recall = recall_score(YTRAIN, prediction.round())\n",
        "f1score = f1_score(YTRAIN, prediction.round())\n",
        "print(\"Training Set Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
        "print(\"Precision: %.2f%%\" % (precision * 100.0))\n",
        "print(\"Recall: %.2f%%\" % (recall * 100.0))\n",
        "print(\"F1-score: %.2f\" % (f1score*100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Set Accuracy: 99.51%\n",
            "Precision: 98.85%\n",
            "Recall: 98.10%\n",
            "F1-score: 98.48\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sj_MxK8jneUT",
        "colab_type": "text"
      },
      "source": [
        "# C) Feature Importance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuRWYMMNb8mK",
        "colab_type": "text"
      },
      "source": [
        "##Accuracies noted by fitting models using one input feature at a time.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMItonDH9G64",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "Newdata = np.genfromtxt('Reduced_Nasa_asteroid.csv', delimiter=\",\", skip_header = True) \n",
        "np.set_printoptions(precision = 2) \n",
        "np.set_printoptions(formatter = {'float': '{: 0.1f}'.format})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diLJourMximE",
        "colab_type": "text"
      },
      "source": [
        "I used one input feature at a time and noted accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fpq9uijWtQIT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3d9d6b10-014d-47fc-8ee1-d70edfe08ff1"
      },
      "source": [
        "np.random.shuffle(Newdata)\n",
        "X = Newdata[:, 11]\n",
        "Y = Newdata[:, -1]\n",
        "index_30percent = int(0.3 * len(Newdata[:, 0]))\n",
        "XVALID = Newdata[:index_30percent, [0]]\n",
        "YVALID = Newdata[:index_30percent, 17]\n",
        "XTRAIN = Newdata[index_30percent:, [0]]\n",
        "YTRAIN = Newdata[index_30percent:, 17]\n",
        "mean = XTRAIN.mean(axis=0)\n",
        "std = XTRAIN.std(axis=0)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4687,)\n",
            "1406\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOHy7NID9K4v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1e0a00d3-345a-451f-f4dc-4c42fa0f6455"
      },
      "source": [
        "callback_a = ModelCheckpoint(filepath = 'Reduced_Nasa_asteroid.hdf5', monitor='val_loss', save_best_only = True, save_weights_only = True, verbose = 1)\n",
        "callback_b = EarlyStopping(monitor='val_loss', mode='min', patience=20, verbose=1)\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(1, input_dim = len(XTRAIN[0, :]), activation='sigmoid'))\n",
        "model.add(Dense(4, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer = 'rmsprop', metrics=['accuracy'])\n",
        "history = model.fit(XTRAIN, YTRAIN, validation_data=(XVALID, YVALID), epochs=256, batch_size=4, callbacks = [callback_a, callback_b])\n",
        "model.load_weights('Reduced_Nasa_asteroid.hdf5')\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "prediction = model.predict(XVALID)\n",
        "accuracy = accuracy_score(YVALID, prediction.round())\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/256\n",
            "819/821 [============================>.] - ETA: 0s - loss: 0.5654 - accuracy: 0.8376\n",
            "Epoch 00001: val_loss improved from inf to 0.47013, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.5648 - accuracy: 0.8379 - val_loss: 0.4701 - val_accuracy: 0.8414\n",
            "Epoch 2/256\n",
            "781/821 [===========================>..] - ETA: 0s - loss: 0.4517 - accuracy: 0.8380\n",
            "Epoch 00002: val_loss improved from 0.47013 to 0.43891, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4514 - accuracy: 0.8379 - val_loss: 0.4389 - val_accuracy: 0.8414\n",
            "Epoch 3/256\n",
            "815/821 [============================>.] - ETA: 0s - loss: 0.4437 - accuracy: 0.8377\n",
            "Epoch 00003: val_loss improved from 0.43891 to 0.43816, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4433 - accuracy: 0.8379 - val_loss: 0.4382 - val_accuracy: 0.8414\n",
            "Epoch 4/256\n",
            "811/821 [============================>.] - ETA: 0s - loss: 0.4420 - accuracy: 0.8388\n",
            "Epoch 00004: val_loss did not improve from 0.43816\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4449 - accuracy: 0.8379 - val_loss: 0.4382 - val_accuracy: 0.8414\n",
            "Epoch 5/256\n",
            "811/821 [============================>.] - ETA: 0s - loss: 0.4410 - accuracy: 0.8394\n",
            "Epoch 00005: val_loss did not improve from 0.43816\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4434 - accuracy: 0.8379 - val_loss: 0.4382 - val_accuracy: 0.8414\n",
            "Epoch 6/256\n",
            "790/821 [===========================>..] - ETA: 0s - loss: 0.4449 - accuracy: 0.8370\n",
            "Epoch 00006: val_loss did not improve from 0.43816\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.4432 - accuracy: 0.8379 - val_loss: 0.4382 - val_accuracy: 0.8414\n",
            "Epoch 7/256\n",
            "800/821 [============================>.] - ETA: 0s - loss: 0.4448 - accuracy: 0.8372\n",
            "Epoch 00007: val_loss did not improve from 0.43816\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4434 - accuracy: 0.8379 - val_loss: 0.4382 - val_accuracy: 0.8414\n",
            "Epoch 8/256\n",
            "779/821 [===========================>..] - ETA: 0s - loss: 0.4439 - accuracy: 0.8376\n",
            "Epoch 00008: val_loss did not improve from 0.43816\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4433 - accuracy: 0.8379 - val_loss: 0.4382 - val_accuracy: 0.8414\n",
            "Epoch 9/256\n",
            "788/821 [===========================>..] - ETA: 0s - loss: 0.4440 - accuracy: 0.8376\n",
            "Epoch 00009: val_loss did not improve from 0.43816\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4433 - accuracy: 0.8379 - val_loss: 0.4384 - val_accuracy: 0.8414\n",
            "Epoch 10/256\n",
            "799/821 [============================>.] - ETA: 0s - loss: 0.4431 - accuracy: 0.8382\n",
            "Epoch 00010: val_loss did not improve from 0.43816\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4435 - accuracy: 0.8379 - val_loss: 0.4382 - val_accuracy: 0.8414\n",
            "Epoch 11/256\n",
            "778/821 [===========================>..] - ETA: 0s - loss: 0.4480 - accuracy: 0.8352\n",
            "Epoch 00011: val_loss did not improve from 0.43816\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4432 - accuracy: 0.8379 - val_loss: 0.4383 - val_accuracy: 0.8414\n",
            "Epoch 12/256\n",
            "792/821 [===========================>..] - ETA: 0s - loss: 0.4459 - accuracy: 0.8365\n",
            "Epoch 00012: val_loss did not improve from 0.43816\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4433 - accuracy: 0.8379 - val_loss: 0.4385 - val_accuracy: 0.8414\n",
            "Epoch 13/256\n",
            "801/821 [============================>.] - ETA: 0s - loss: 0.4415 - accuracy: 0.8393\n",
            "Epoch 00013: val_loss did not improve from 0.43816\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4452 - accuracy: 0.8379 - val_loss: 0.4382 - val_accuracy: 0.8414\n",
            "Epoch 14/256\n",
            "807/821 [============================>.] - ETA: 0s - loss: 0.4421 - accuracy: 0.8386\n",
            "Epoch 00014: val_loss did not improve from 0.43816\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4431 - accuracy: 0.8379 - val_loss: 0.4384 - val_accuracy: 0.8414\n",
            "Epoch 15/256\n",
            "774/821 [===========================>..] - ETA: 0s - loss: 0.4432 - accuracy: 0.8382\n",
            "Epoch 00015: val_loss did not improve from 0.43816\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4435 - accuracy: 0.8379 - val_loss: 0.4383 - val_accuracy: 0.8414\n",
            "Epoch 16/256\n",
            "780/821 [===========================>..] - ETA: 0s - loss: 0.4464 - accuracy: 0.8362\n",
            "Epoch 00016: val_loss did not improve from 0.43816\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4434 - accuracy: 0.8379 - val_loss: 0.4382 - val_accuracy: 0.8414\n",
            "Epoch 17/256\n",
            "793/821 [===========================>..] - ETA: 0s - loss: 0.4440 - accuracy: 0.8376\n",
            "Epoch 00017: val_loss did not improve from 0.43816\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4434 - accuracy: 0.8379 - val_loss: 0.4382 - val_accuracy: 0.8414\n",
            "Epoch 18/256\n",
            "808/821 [============================>.] - ETA: 0s - loss: 0.4435 - accuracy: 0.8379\n",
            "Epoch 00018: val_loss did not improve from 0.43816\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4433 - accuracy: 0.8379 - val_loss: 0.4382 - val_accuracy: 0.8414\n",
            "Epoch 19/256\n",
            "788/821 [===========================>..] - ETA: 0s - loss: 0.4411 - accuracy: 0.8391\n",
            "Epoch 00019: val_loss did not improve from 0.43816\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4431 - accuracy: 0.8379 - val_loss: 0.4383 - val_accuracy: 0.8414\n",
            "Epoch 20/256\n",
            "784/821 [===========================>..] - ETA: 0s - loss: 0.4467 - accuracy: 0.8361\n",
            "Epoch 00020: val_loss did not improve from 0.43816\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4435 - accuracy: 0.8379 - val_loss: 0.4382 - val_accuracy: 0.8414\n",
            "Epoch 21/256\n",
            "785/821 [===========================>..] - ETA: 0s - loss: 0.4449 - accuracy: 0.8369\n",
            "Epoch 00021: val_loss did not improve from 0.43816\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4431 - accuracy: 0.8379 - val_loss: 0.4384 - val_accuracy: 0.8414\n",
            "Epoch 22/256\n",
            "791/821 [===========================>..] - ETA: 0s - loss: 0.4437 - accuracy: 0.8379\n",
            "Epoch 00022: val_loss did not improve from 0.43816\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4450 - accuracy: 0.8379 - val_loss: 0.4382 - val_accuracy: 0.8414\n",
            "Epoch 23/256\n",
            "791/821 [===========================>..] - ETA: 0s - loss: 0.4450 - accuracy: 0.8369\n",
            "Epoch 00023: val_loss did not improve from 0.43816\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4432 - accuracy: 0.8379 - val_loss: 0.4383 - val_accuracy: 0.8414\n",
            "Epoch 00023: early stopping\n",
            "Accuracy: 84.14%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Edxz7Dj8b3Jt",
        "colab_type": "text"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktZPj_f0FHLR",
        "colab_type": "text"
      },
      "source": [
        "#Graph showing the importance of features and accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jeD1R-o-Qiw",
        "colab_type": "text"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WQtO0MgFean",
        "colab_type": "text"
      },
      "source": [
        "# Removing least important features one by one\n",
        "Removed one by one feature till last 2 features remains and noted accuracy. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WPKZdLd13uo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3b1540cd-8c7c-4b17-ad4d-5aa5e3ad9536"
      },
      "source": [
        "\n",
        "np.random.shuffle(Newdata)\n",
        "X = Newdata[:, [9,14]]\n",
        "Y = Newdata[:, -1]\n",
        "\n",
        "index_30percent = int(0.3 * len(Newdata[:, 0]))\n",
        "print(index_30percent)\n",
        "# Split into training and validation\n",
        "XVALID = Newdata[:index_30percent, [9,14]]\n",
        "YVALID = Newdata[:index_30percent, 17]\n",
        "XTRAIN = Newdata[index_30percent:, [9,14]]\n",
        "YTRAIN = Newdata[index_30percent:, 17]\n",
        "#XVALID.shape\n",
        "\n",
        "mean = XTRAIN.mean(axis=0)\n",
        "XTRAIN -= mean\n",
        "std = XTRAIN.std(axis=0)\n",
        "XTRAIN /= std\n",
        "XVALID -= mean\n",
        "XVALID /= std\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1406\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJ98YIJzTUtS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7285f35a-123d-460b-9190-75926ea880dc"
      },
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "callback_a = ModelCheckpoint(filepath = 'Reduced_Nasa_asteroid.hdf5', monitor='val_loss', save_best_only = True, save_weights_only = True, verbose = 1)\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(1, input_dim = len(XTRAIN[0, :]), activation='sigmoid'))\n",
        "model.add(Dense(4, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer = 'rmsprop', metrics=['accuracy'])\n",
        "history = model.fit(XTRAIN, YTRAIN, validation_data=(XVALID, YVALID), epochs=256, batch_size=4, callbacks = [callback_a])\n",
        "model.load_weights('Reduced_Nasa_asteroid.hdf5')\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "prediction = model.predict(XVALID)\n",
        "accuracy = accuracy_score(YVALID, prediction.round())\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/256\n",
            "814/821 [============================>.] - ETA: 0s - loss: 0.5009 - accuracy: 0.8237\n",
            "Epoch 00001: val_loss improved from inf to 0.44740, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.4983 - accuracy: 0.8251 - val_loss: 0.4474 - val_accuracy: 0.8329\n",
            "Epoch 2/256\n",
            "785/821 [===========================>..] - ETA: 0s - loss: 0.4314 - accuracy: 0.8408\n",
            "Epoch 00002: val_loss improved from 0.44740 to 0.44630, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4297 - accuracy: 0.8415 - val_loss: 0.4463 - val_accuracy: 0.8329\n",
            "Epoch 3/256\n",
            "809/821 [============================>.] - ETA: 0s - loss: 0.4277 - accuracy: 0.8415\n",
            "Epoch 00003: val_loss improved from 0.44630 to 0.44419, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.4275 - accuracy: 0.8415 - val_loss: 0.4442 - val_accuracy: 0.8329\n",
            "Epoch 4/256\n",
            "774/821 [===========================>..] - ETA: 0s - loss: 0.4280 - accuracy: 0.8398\n",
            "Epoch 00004: val_loss improved from 0.44419 to 0.44253, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4256 - accuracy: 0.8415 - val_loss: 0.4425 - val_accuracy: 0.8329\n",
            "Epoch 5/256\n",
            "805/821 [============================>.] - ETA: 0s - loss: 0.4237 - accuracy: 0.8422\n",
            "Epoch 00005: val_loss improved from 0.44253 to 0.44121, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4245 - accuracy: 0.8415 - val_loss: 0.4412 - val_accuracy: 0.8329\n",
            "Epoch 6/256\n",
            "821/821 [==============================] - ETA: 0s - loss: 0.4234 - accuracy: 0.8415\n",
            "Epoch 00006: val_loss did not improve from 0.44121\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4234 - accuracy: 0.8415 - val_loss: 0.4413 - val_accuracy: 0.8329\n",
            "Epoch 7/256\n",
            "804/821 [============================>.] - ETA: 0s - loss: 0.4250 - accuracy: 0.8405\n",
            "Epoch 00007: val_loss improved from 0.44121 to 0.44080, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4229 - accuracy: 0.8415 - val_loss: 0.4408 - val_accuracy: 0.8329\n",
            "Epoch 8/256\n",
            "778/821 [===========================>..] - ETA: 0s - loss: 0.4231 - accuracy: 0.8406\n",
            "Epoch 00008: val_loss did not improve from 0.44080\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4223 - accuracy: 0.8415 - val_loss: 0.4416 - val_accuracy: 0.8329\n",
            "Epoch 9/256\n",
            "818/821 [============================>.] - ETA: 0s - loss: 0.4226 - accuracy: 0.8414\n",
            "Epoch 00009: val_loss improved from 0.44080 to 0.43962, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4222 - accuracy: 0.8415 - val_loss: 0.4396 - val_accuracy: 0.8329\n",
            "Epoch 10/256\n",
            "777/821 [===========================>..] - ETA: 0s - loss: 0.4218 - accuracy: 0.8414\n",
            "Epoch 00010: val_loss did not improve from 0.43962\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4216 - accuracy: 0.8415 - val_loss: 0.4407 - val_accuracy: 0.8329\n",
            "Epoch 11/256\n",
            "821/821 [==============================] - ETA: 0s - loss: 0.4216 - accuracy: 0.8415\n",
            "Epoch 00011: val_loss did not improve from 0.43962\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4216 - accuracy: 0.8415 - val_loss: 0.4409 - val_accuracy: 0.8329\n",
            "Epoch 12/256\n",
            "803/821 [============================>.] - ETA: 0s - loss: 0.4221 - accuracy: 0.8415\n",
            "Epoch 00012: val_loss did not improve from 0.43962\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4217 - accuracy: 0.8415 - val_loss: 0.4402 - val_accuracy: 0.8329\n",
            "Epoch 13/256\n",
            "803/821 [============================>.] - ETA: 0s - loss: 0.4194 - accuracy: 0.8425\n",
            "Epoch 00013: val_loss did not improve from 0.43962\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4215 - accuracy: 0.8415 - val_loss: 0.4403 - val_accuracy: 0.8329\n",
            "Epoch 14/256\n",
            "801/821 [============================>.] - ETA: 0s - loss: 0.4204 - accuracy: 0.8421\n",
            "Epoch 00014: val_loss did not improve from 0.43962\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.4212 - accuracy: 0.8415 - val_loss: 0.4397 - val_accuracy: 0.8329\n",
            "Epoch 15/256\n",
            "796/821 [============================>.] - ETA: 0s - loss: 0.4210 - accuracy: 0.8417\n",
            "Epoch 00015: val_loss did not improve from 0.43962\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4212 - accuracy: 0.8415 - val_loss: 0.4408 - val_accuracy: 0.8329\n",
            "Epoch 16/256\n",
            "789/821 [===========================>..] - ETA: 0s - loss: 0.4216 - accuracy: 0.8416\n",
            "Epoch 00016: val_loss improved from 0.43962 to 0.43952, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4222 - accuracy: 0.8415 - val_loss: 0.4395 - val_accuracy: 0.8329\n",
            "Epoch 17/256\n",
            "804/821 [============================>.] - ETA: 0s - loss: 0.4216 - accuracy: 0.8414\n",
            "Epoch 00017: val_loss improved from 0.43952 to 0.43923, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4207 - accuracy: 0.8415 - val_loss: 0.4392 - val_accuracy: 0.8329\n",
            "Epoch 18/256\n",
            "790/821 [===========================>..] - ETA: 0s - loss: 0.4204 - accuracy: 0.8418\n",
            "Epoch 00018: val_loss did not improve from 0.43923\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4209 - accuracy: 0.8415 - val_loss: 0.4400 - val_accuracy: 0.8329\n",
            "Epoch 19/256\n",
            "795/821 [============================>.] - ETA: 0s - loss: 0.4150 - accuracy: 0.8447\n",
            "Epoch 00019: val_loss did not improve from 0.43923\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4210 - accuracy: 0.8415 - val_loss: 0.4397 - val_accuracy: 0.8329\n",
            "Epoch 20/256\n",
            "821/821 [==============================] - ETA: 0s - loss: 0.4207 - accuracy: 0.8415\n",
            "Epoch 00020: val_loss did not improve from 0.43923\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.4207 - accuracy: 0.8415 - val_loss: 0.4404 - val_accuracy: 0.8329\n",
            "Epoch 21/256\n",
            "806/821 [============================>.] - ETA: 0s - loss: 0.4195 - accuracy: 0.8418\n",
            "Epoch 00021: val_loss did not improve from 0.43923\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4207 - accuracy: 0.8415 - val_loss: 0.4403 - val_accuracy: 0.8329\n",
            "Epoch 22/256\n",
            "780/821 [===========================>..] - ETA: 0s - loss: 0.4217 - accuracy: 0.8413\n",
            "Epoch 00022: val_loss did not improve from 0.43923\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4207 - accuracy: 0.8415 - val_loss: 0.4395 - val_accuracy: 0.8329\n",
            "Epoch 23/256\n",
            "784/821 [===========================>..] - ETA: 0s - loss: 0.4228 - accuracy: 0.8406\n",
            "Epoch 00023: val_loss did not improve from 0.43923\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4204 - accuracy: 0.8415 - val_loss: 0.4403 - val_accuracy: 0.8329\n",
            "Epoch 24/256\n",
            "809/821 [============================>.] - ETA: 0s - loss: 0.4219 - accuracy: 0.8409\n",
            "Epoch 00024: val_loss did not improve from 0.43923\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4207 - accuracy: 0.8415 - val_loss: 0.4414 - val_accuracy: 0.8329\n",
            "Epoch 25/256\n",
            "804/821 [============================>.] - ETA: 0s - loss: 0.4223 - accuracy: 0.8405\n",
            "Epoch 00025: val_loss did not improve from 0.43923\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4205 - accuracy: 0.8415 - val_loss: 0.4412 - val_accuracy: 0.8329\n",
            "Epoch 26/256\n",
            "800/821 [============================>.] - ETA: 0s - loss: 0.4235 - accuracy: 0.8397\n",
            "Epoch 00026: val_loss did not improve from 0.43923\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4206 - accuracy: 0.8415 - val_loss: 0.4402 - val_accuracy: 0.8329\n",
            "Epoch 27/256\n",
            "800/821 [============================>.] - ETA: 0s - loss: 0.4174 - accuracy: 0.8438\n",
            "Epoch 00027: val_loss did not improve from 0.43923\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4221 - accuracy: 0.8415 - val_loss: 0.4393 - val_accuracy: 0.8329\n",
            "Epoch 28/256\n",
            "809/821 [============================>.] - ETA: 0s - loss: 0.4226 - accuracy: 0.8399\n",
            "Epoch 00028: val_loss did not improve from 0.43923\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4201 - accuracy: 0.8415 - val_loss: 0.4418 - val_accuracy: 0.8329\n",
            "Epoch 29/256\n",
            "811/821 [============================>.] - ETA: 0s - loss: 0.4190 - accuracy: 0.8425\n",
            "Epoch 00029: val_loss did not improve from 0.43923\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4229 - accuracy: 0.8415 - val_loss: 0.4393 - val_accuracy: 0.8329\n",
            "Epoch 30/256\n",
            "800/821 [============================>.] - ETA: 0s - loss: 0.4193 - accuracy: 0.8422\n",
            "Epoch 00030: val_loss did not improve from 0.43923\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.4203 - accuracy: 0.8415 - val_loss: 0.4395 - val_accuracy: 0.8329\n",
            "Epoch 31/256\n",
            "815/821 [============================>.] - ETA: 0s - loss: 0.4195 - accuracy: 0.8417\n",
            "Epoch 00031: val_loss did not improve from 0.43923\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4202 - accuracy: 0.8415 - val_loss: 0.4394 - val_accuracy: 0.8329\n",
            "Epoch 32/256\n",
            "797/821 [============================>.] - ETA: 0s - loss: 0.4216 - accuracy: 0.8416\n",
            "Epoch 00032: val_loss did not improve from 0.43923\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4203 - accuracy: 0.8415 - val_loss: 0.4394 - val_accuracy: 0.8329\n",
            "Epoch 33/256\n",
            "803/821 [============================>.] - ETA: 0s - loss: 0.4215 - accuracy: 0.8415\n",
            "Epoch 00033: val_loss did not improve from 0.43923\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4204 - accuracy: 0.8415 - val_loss: 0.4401 - val_accuracy: 0.8329\n",
            "Epoch 34/256\n",
            "802/821 [============================>.] - ETA: 0s - loss: 0.4220 - accuracy: 0.8407\n",
            "Epoch 00034: val_loss did not improve from 0.43923\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4204 - accuracy: 0.8415 - val_loss: 0.4394 - val_accuracy: 0.8329\n",
            "Epoch 35/256\n",
            "795/821 [============================>.] - ETA: 0s - loss: 0.4195 - accuracy: 0.8421\n",
            "Epoch 00035: val_loss did not improve from 0.43923\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.4202 - accuracy: 0.8415 - val_loss: 0.4394 - val_accuracy: 0.8329\n",
            "Epoch 36/256\n",
            "808/821 [============================>.] - ETA: 0s - loss: 0.4211 - accuracy: 0.8416\n",
            "Epoch 00036: val_loss did not improve from 0.43923\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.4202 - accuracy: 0.8415 - val_loss: 0.4395 - val_accuracy: 0.8329\n",
            "Epoch 37/256\n",
            "811/821 [============================>.] - ETA: 0s - loss: 0.4225 - accuracy: 0.8403\n",
            "Epoch 00037: val_loss did not improve from 0.43923\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4201 - accuracy: 0.8415 - val_loss: 0.4403 - val_accuracy: 0.8329\n",
            "Epoch 38/256\n",
            "798/821 [============================>.] - ETA: 0s - loss: 0.4205 - accuracy: 0.8421\n",
            "Epoch 00038: val_loss improved from 0.43923 to 0.43894, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4203 - accuracy: 0.8415 - val_loss: 0.4389 - val_accuracy: 0.8329\n",
            "Epoch 39/256\n",
            "803/821 [============================>.] - ETA: 0s - loss: 0.4185 - accuracy: 0.8425\n",
            "Epoch 00039: val_loss did not improve from 0.43894\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.4199 - accuracy: 0.8415 - val_loss: 0.4412 - val_accuracy: 0.8329\n",
            "Epoch 40/256\n",
            "784/821 [===========================>..] - ETA: 0s - loss: 0.4205 - accuracy: 0.8415\n",
            "Epoch 00040: val_loss did not improve from 0.43894\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4204 - accuracy: 0.8415 - val_loss: 0.4395 - val_accuracy: 0.8329\n",
            "Epoch 41/256\n",
            "814/821 [============================>.] - ETA: 0s - loss: 0.4207 - accuracy: 0.8415\n",
            "Epoch 00041: val_loss did not improve from 0.43894\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.4202 - accuracy: 0.8415 - val_loss: 0.4400 - val_accuracy: 0.8329\n",
            "Epoch 42/256\n",
            "793/821 [===========================>..] - ETA: 0s - loss: 0.4172 - accuracy: 0.8436\n",
            "Epoch 00042: val_loss did not improve from 0.43894\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4207 - accuracy: 0.8415 - val_loss: 0.4394 - val_accuracy: 0.8329\n",
            "Epoch 43/256\n",
            "787/821 [===========================>..] - ETA: 0s - loss: 0.4236 - accuracy: 0.8393\n",
            "Epoch 00043: val_loss did not improve from 0.43894\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4219 - accuracy: 0.8415 - val_loss: 0.4399 - val_accuracy: 0.8329\n",
            "Epoch 44/256\n",
            "801/821 [============================>.] - ETA: 0s - loss: 0.4199 - accuracy: 0.8418\n",
            "Epoch 00044: val_loss did not improve from 0.43894\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4202 - accuracy: 0.8415 - val_loss: 0.4398 - val_accuracy: 0.8329\n",
            "Epoch 45/256\n",
            "790/821 [===========================>..] - ETA: 0s - loss: 0.4197 - accuracy: 0.8411\n",
            "Epoch 00045: val_loss improved from 0.43894 to 0.43889, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.4201 - accuracy: 0.8415 - val_loss: 0.4389 - val_accuracy: 0.8329\n",
            "Epoch 46/256\n",
            "800/821 [============================>.] - ETA: 0s - loss: 0.4188 - accuracy: 0.8425\n",
            "Epoch 00046: val_loss did not improve from 0.43889\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4197 - accuracy: 0.8415 - val_loss: 0.4427 - val_accuracy: 0.8329\n",
            "Epoch 47/256\n",
            "803/821 [============================>.] - ETA: 0s - loss: 0.4205 - accuracy: 0.8415\n",
            "Epoch 00047: val_loss did not improve from 0.43889\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4207 - accuracy: 0.8415 - val_loss: 0.4390 - val_accuracy: 0.8329\n",
            "Epoch 48/256\n",
            "780/821 [===========================>..] - ETA: 0s - loss: 0.4242 - accuracy: 0.8391\n",
            "Epoch 00048: val_loss did not improve from 0.43889\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4219 - accuracy: 0.8415 - val_loss: 0.4396 - val_accuracy: 0.8329\n",
            "Epoch 49/256\n",
            "807/821 [============================>.] - ETA: 0s - loss: 0.4205 - accuracy: 0.8417\n",
            "Epoch 00049: val_loss did not improve from 0.43889\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.4202 - accuracy: 0.8415 - val_loss: 0.4394 - val_accuracy: 0.8329\n",
            "Epoch 50/256\n",
            "821/821 [==============================] - ETA: 0s - loss: 0.4220 - accuracy: 0.8415\n",
            "Epoch 00050: val_loss improved from 0.43889 to 0.43840, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.4220 - accuracy: 0.8415 - val_loss: 0.4384 - val_accuracy: 0.8329\n",
            "Epoch 51/256\n",
            "808/821 [============================>.] - ETA: 0s - loss: 0.4188 - accuracy: 0.8422\n",
            "Epoch 00051: val_loss did not improve from 0.43840\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4218 - accuracy: 0.8415 - val_loss: 0.4395 - val_accuracy: 0.8329\n",
            "Epoch 52/256\n",
            "811/821 [============================>.] - ETA: 0s - loss: 0.4189 - accuracy: 0.8419\n",
            "Epoch 00052: val_loss did not improve from 0.43840\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4201 - accuracy: 0.8415 - val_loss: 0.4390 - val_accuracy: 0.8329\n",
            "Epoch 53/256\n",
            "812/821 [============================>.] - ETA: 0s - loss: 0.4208 - accuracy: 0.8411\n",
            "Epoch 00053: val_loss did not improve from 0.43840\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4201 - accuracy: 0.8415 - val_loss: 0.4402 - val_accuracy: 0.8329\n",
            "Epoch 54/256\n",
            "780/821 [===========================>..] - ETA: 0s - loss: 0.4152 - accuracy: 0.8442\n",
            "Epoch 00054: val_loss did not improve from 0.43840\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.4200 - accuracy: 0.8415 - val_loss: 0.4390 - val_accuracy: 0.8329\n",
            "Epoch 55/256\n",
            "788/821 [===========================>..] - ETA: 0s - loss: 0.4199 - accuracy: 0.8414\n",
            "Epoch 00055: val_loss did not improve from 0.43840\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4200 - accuracy: 0.8415 - val_loss: 0.4390 - val_accuracy: 0.8329\n",
            "Epoch 56/256\n",
            "779/821 [===========================>..] - ETA: 0s - loss: 0.4168 - accuracy: 0.8437\n",
            "Epoch 00056: val_loss did not improve from 0.43840\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.4212 - accuracy: 0.8415 - val_loss: 0.4394 - val_accuracy: 0.8329\n",
            "Epoch 57/256\n",
            "811/821 [============================>.] - ETA: 0s - loss: 0.4196 - accuracy: 0.8419\n",
            "Epoch 00057: val_loss did not improve from 0.43840\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.4214 - accuracy: 0.8415 - val_loss: 0.4392 - val_accuracy: 0.8329\n",
            "Epoch 58/256\n",
            "777/821 [===========================>..] - ETA: 0s - loss: 0.4213 - accuracy: 0.8411\n",
            "Epoch 00058: val_loss did not improve from 0.43840\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.4201 - accuracy: 0.8415 - val_loss: 0.4386 - val_accuracy: 0.8329\n",
            "Epoch 59/256\n",
            "792/821 [===========================>..] - ETA: 0s - loss: 0.4182 - accuracy: 0.8434\n",
            "Epoch 00059: val_loss improved from 0.43840 to 0.43825, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.4199 - accuracy: 0.8415 - val_loss: 0.4383 - val_accuracy: 0.8329\n",
            "Epoch 60/256\n",
            "798/821 [============================>.] - ETA: 0s - loss: 0.4217 - accuracy: 0.8402\n",
            "Epoch 00060: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.4200 - accuracy: 0.8415 - val_loss: 0.4400 - val_accuracy: 0.8329\n",
            "Epoch 61/256\n",
            "821/821 [==============================] - ETA: 0s - loss: 0.4203 - accuracy: 0.8415\n",
            "Epoch 00061: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.4203 - accuracy: 0.8415 - val_loss: 0.4396 - val_accuracy: 0.8329\n",
            "Epoch 62/256\n",
            "783/821 [===========================>..] - ETA: 0s - loss: 0.4213 - accuracy: 0.8416\n",
            "Epoch 00062: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.4211 - accuracy: 0.8415 - val_loss: 0.4405 - val_accuracy: 0.8329\n",
            "Epoch 63/256\n",
            "820/821 [============================>.] - ETA: 0s - loss: 0.4204 - accuracy: 0.8415\n",
            "Epoch 00063: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.4200 - accuracy: 0.8415 - val_loss: 0.4384 - val_accuracy: 0.8329\n",
            "Epoch 64/256\n",
            "798/821 [============================>.] - ETA: 0s - loss: 0.4169 - accuracy: 0.8437\n",
            "Epoch 00064: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.4199 - accuracy: 0.8415 - val_loss: 0.4394 - val_accuracy: 0.8329\n",
            "Epoch 65/256\n",
            "773/821 [===========================>..] - ETA: 0s - loss: 0.4165 - accuracy: 0.8441\n",
            "Epoch 00065: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4204 - accuracy: 0.8415 - val_loss: 0.4388 - val_accuracy: 0.8329\n",
            "Epoch 66/256\n",
            "797/821 [============================>.] - ETA: 0s - loss: 0.4208 - accuracy: 0.8413\n",
            "Epoch 00066: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4199 - accuracy: 0.8415 - val_loss: 0.4395 - val_accuracy: 0.8329\n",
            "Epoch 67/256\n",
            "784/821 [===========================>..] - ETA: 0s - loss: 0.4221 - accuracy: 0.8399\n",
            "Epoch 00067: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4201 - accuracy: 0.8415 - val_loss: 0.4385 - val_accuracy: 0.8329\n",
            "Epoch 68/256\n",
            "788/821 [===========================>..] - ETA: 0s - loss: 0.4225 - accuracy: 0.8411\n",
            "Epoch 00068: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4202 - accuracy: 0.8415 - val_loss: 0.4396 - val_accuracy: 0.8329\n",
            "Epoch 69/256\n",
            "783/821 [===========================>..] - ETA: 0s - loss: 0.4220 - accuracy: 0.8404\n",
            "Epoch 00069: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.4201 - accuracy: 0.8415 - val_loss: 0.4404 - val_accuracy: 0.8329\n",
            "Epoch 70/256\n",
            "810/821 [============================>.] - ETA: 0s - loss: 0.4207 - accuracy: 0.8414\n",
            "Epoch 00070: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.4201 - accuracy: 0.8415 - val_loss: 0.4397 - val_accuracy: 0.8329\n",
            "Epoch 71/256\n",
            "781/821 [===========================>..] - ETA: 0s - loss: 0.4195 - accuracy: 0.8419\n",
            "Epoch 00071: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.4200 - accuracy: 0.8415 - val_loss: 0.4396 - val_accuracy: 0.8329\n",
            "Epoch 72/256\n",
            "792/821 [===========================>..] - ETA: 0s - loss: 0.4208 - accuracy: 0.8409\n",
            "Epoch 00072: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.4202 - accuracy: 0.8415 - val_loss: 0.4405 - val_accuracy: 0.8329\n",
            "Epoch 73/256\n",
            "815/821 [============================>.] - ETA: 0s - loss: 0.4208 - accuracy: 0.8414\n",
            "Epoch 00073: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.4202 - accuracy: 0.8415 - val_loss: 0.4392 - val_accuracy: 0.8329\n",
            "Epoch 74/256\n",
            "817/821 [============================>.] - ETA: 0s - loss: 0.4208 - accuracy: 0.8412\n",
            "Epoch 00074: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.4213 - accuracy: 0.8415 - val_loss: 0.4395 - val_accuracy: 0.8329\n",
            "Epoch 75/256\n",
            "806/821 [============================>.] - ETA: 0s - loss: 0.4198 - accuracy: 0.8421\n",
            "Epoch 00075: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.4202 - accuracy: 0.8415 - val_loss: 0.4383 - val_accuracy: 0.8329\n",
            "Epoch 76/256\n",
            "784/821 [===========================>..] - ETA: 0s - loss: 0.4207 - accuracy: 0.8415\n",
            "Epoch 00076: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.4200 - accuracy: 0.8415 - val_loss: 0.4397 - val_accuracy: 0.8329\n",
            "Epoch 77/256\n",
            "796/821 [============================>.] - ETA: 0s - loss: 0.4207 - accuracy: 0.8411\n",
            "Epoch 00077: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.4216 - accuracy: 0.8415 - val_loss: 0.4417 - val_accuracy: 0.8329\n",
            "Epoch 78/256\n",
            "779/821 [===========================>..] - ETA: 0s - loss: 0.4205 - accuracy: 0.8415\n",
            "Epoch 00078: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4210 - accuracy: 0.8415 - val_loss: 0.4394 - val_accuracy: 0.8329\n",
            "Epoch 79/256\n",
            "805/821 [============================>.] - ETA: 0s - loss: 0.4225 - accuracy: 0.8404\n",
            "Epoch 00079: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.4201 - accuracy: 0.8415 - val_loss: 0.4403 - val_accuracy: 0.8329\n",
            "Epoch 80/256\n",
            "821/821 [==============================] - ETA: 0s - loss: 0.4202 - accuracy: 0.8415\n",
            "Epoch 00080: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4202 - accuracy: 0.8415 - val_loss: 0.4406 - val_accuracy: 0.8329\n",
            "Epoch 81/256\n",
            "785/821 [===========================>..] - ETA: 0s - loss: 0.4217 - accuracy: 0.8408\n",
            "Epoch 00081: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4202 - accuracy: 0.8415 - val_loss: 0.4398 - val_accuracy: 0.8329\n",
            "Epoch 82/256\n",
            "813/821 [============================>.] - ETA: 0s - loss: 0.4186 - accuracy: 0.8426\n",
            "Epoch 00082: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.4215 - accuracy: 0.8415 - val_loss: 0.4383 - val_accuracy: 0.8329\n",
            "Epoch 83/256\n",
            "790/821 [===========================>..] - ETA: 0s - loss: 0.4219 - accuracy: 0.8408\n",
            "Epoch 00083: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4201 - accuracy: 0.8415 - val_loss: 0.4397 - val_accuracy: 0.8329\n",
            "Epoch 84/256\n",
            "796/821 [============================>.] - ETA: 0s - loss: 0.4178 - accuracy: 0.8427\n",
            "Epoch 00084: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4224 - accuracy: 0.8415 - val_loss: 0.4384 - val_accuracy: 0.8329\n",
            "Epoch 85/256\n",
            "801/821 [============================>.] - ETA: 0s - loss: 0.4207 - accuracy: 0.8411\n",
            "Epoch 00085: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4200 - accuracy: 0.8415 - val_loss: 0.4404 - val_accuracy: 0.8329\n",
            "Epoch 86/256\n",
            "790/821 [===========================>..] - ETA: 0s - loss: 0.4230 - accuracy: 0.8402\n",
            "Epoch 00086: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4198 - accuracy: 0.8415 - val_loss: 0.4403 - val_accuracy: 0.8329\n",
            "Epoch 87/256\n",
            "808/821 [============================>.] - ETA: 0s - loss: 0.4195 - accuracy: 0.8422\n",
            "Epoch 00087: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.4202 - accuracy: 0.8415 - val_loss: 0.4392 - val_accuracy: 0.8329\n",
            "Epoch 88/256\n",
            "821/821 [==============================] - ETA: 0s - loss: 0.4204 - accuracy: 0.8415\n",
            "Epoch 00088: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.4204 - accuracy: 0.8415 - val_loss: 0.4385 - val_accuracy: 0.8329\n",
            "Epoch 89/256\n",
            "790/821 [===========================>..] - ETA: 0s - loss: 0.4209 - accuracy: 0.8415\n",
            "Epoch 00089: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4203 - accuracy: 0.8415 - val_loss: 0.4394 - val_accuracy: 0.8329\n",
            "Epoch 90/256\n",
            "811/821 [============================>.] - ETA: 0s - loss: 0.4201 - accuracy: 0.8416\n",
            "Epoch 00090: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4203 - accuracy: 0.8415 - val_loss: 0.4405 - val_accuracy: 0.8329\n",
            "Epoch 91/256\n",
            "782/821 [===========================>..] - ETA: 0s - loss: 0.4200 - accuracy: 0.8414\n",
            "Epoch 00091: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4203 - accuracy: 0.8415 - val_loss: 0.4400 - val_accuracy: 0.8329\n",
            "Epoch 92/256\n",
            "821/821 [==============================] - ETA: 0s - loss: 0.4205 - accuracy: 0.8415\n",
            "Epoch 00092: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.4205 - accuracy: 0.8415 - val_loss: 0.4397 - val_accuracy: 0.8329\n",
            "Epoch 93/256\n",
            "814/821 [============================>.] - ETA: 0s - loss: 0.4209 - accuracy: 0.8412\n",
            "Epoch 00093: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.4202 - accuracy: 0.8415 - val_loss: 0.4387 - val_accuracy: 0.8329\n",
            "Epoch 94/256\n",
            "812/821 [============================>.] - ETA: 0s - loss: 0.4200 - accuracy: 0.8414\n",
            "Epoch 00094: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4206 - accuracy: 0.8415 - val_loss: 0.4406 - val_accuracy: 0.8329\n",
            "Epoch 95/256\n",
            "780/821 [===========================>..] - ETA: 0s - loss: 0.4246 - accuracy: 0.8391\n",
            "Epoch 00095: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4202 - accuracy: 0.8415 - val_loss: 0.4400 - val_accuracy: 0.8329\n",
            "Epoch 96/256\n",
            "803/821 [============================>.] - ETA: 0s - loss: 0.4187 - accuracy: 0.8425\n",
            "Epoch 00096: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4202 - accuracy: 0.8415 - val_loss: 0.4384 - val_accuracy: 0.8329\n",
            "Epoch 97/256\n",
            "807/821 [============================>.] - ETA: 0s - loss: 0.4209 - accuracy: 0.8411\n",
            "Epoch 00097: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4202 - accuracy: 0.8415 - val_loss: 0.4393 - val_accuracy: 0.8329\n",
            "Epoch 98/256\n",
            "794/821 [============================>.] - ETA: 0s - loss: 0.4220 - accuracy: 0.8404\n",
            "Epoch 00098: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4214 - accuracy: 0.8415 - val_loss: 0.4400 - val_accuracy: 0.8329\n",
            "Epoch 99/256\n",
            "815/821 [============================>.] - ETA: 0s - loss: 0.4191 - accuracy: 0.8426\n",
            "Epoch 00099: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.4205 - accuracy: 0.8415 - val_loss: 0.4390 - val_accuracy: 0.8329\n",
            "Epoch 100/256\n",
            "813/821 [============================>.] - ETA: 0s - loss: 0.4200 - accuracy: 0.8419\n",
            "Epoch 00100: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4203 - accuracy: 0.8415 - val_loss: 0.4387 - val_accuracy: 0.8329\n",
            "Epoch 101/256\n",
            "803/821 [============================>.] - ETA: 0s - loss: 0.4198 - accuracy: 0.8415\n",
            "Epoch 00101: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4202 - accuracy: 0.8415 - val_loss: 0.4384 - val_accuracy: 0.8329\n",
            "Epoch 102/256\n",
            "817/821 [============================>.] - ETA: 0s - loss: 0.4208 - accuracy: 0.8412\n",
            "Epoch 00102: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4201 - accuracy: 0.8415 - val_loss: 0.4388 - val_accuracy: 0.8329\n",
            "Epoch 103/256\n",
            "796/821 [============================>.] - ETA: 0s - loss: 0.4209 - accuracy: 0.8414\n",
            "Epoch 00103: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4202 - accuracy: 0.8415 - val_loss: 0.4383 - val_accuracy: 0.8329\n",
            "Epoch 104/256\n",
            "783/821 [===========================>..] - ETA: 0s - loss: 0.4197 - accuracy: 0.8423\n",
            "Epoch 00104: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4201 - accuracy: 0.8415 - val_loss: 0.4386 - val_accuracy: 0.8329\n",
            "Epoch 105/256\n",
            "780/821 [===========================>..] - ETA: 0s - loss: 0.4222 - accuracy: 0.8407\n",
            "Epoch 00105: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4197 - accuracy: 0.8415 - val_loss: 0.4409 - val_accuracy: 0.8329\n",
            "Epoch 106/256\n",
            "804/821 [============================>.] - ETA: 0s - loss: 0.4194 - accuracy: 0.8417\n",
            "Epoch 00106: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4199 - accuracy: 0.8415 - val_loss: 0.4389 - val_accuracy: 0.8329\n",
            "Epoch 107/256\n",
            "806/821 [============================>.] - ETA: 0s - loss: 0.4193 - accuracy: 0.8415\n",
            "Epoch 00107: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4202 - accuracy: 0.8415 - val_loss: 0.4388 - val_accuracy: 0.8329\n",
            "Epoch 108/256\n",
            "784/821 [===========================>..] - ETA: 0s - loss: 0.4223 - accuracy: 0.8399\n",
            "Epoch 00108: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4199 - accuracy: 0.8415 - val_loss: 0.4412 - val_accuracy: 0.8329\n",
            "Epoch 109/256\n",
            "806/821 [============================>.] - ETA: 0s - loss: 0.4212 - accuracy: 0.8412\n",
            "Epoch 00109: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4203 - accuracy: 0.8415 - val_loss: 0.4392 - val_accuracy: 0.8329\n",
            "Epoch 110/256\n",
            "802/821 [============================>.] - ETA: 0s - loss: 0.4214 - accuracy: 0.8410\n",
            "Epoch 00110: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4203 - accuracy: 0.8415 - val_loss: 0.4390 - val_accuracy: 0.8329\n",
            "Epoch 111/256\n",
            "811/821 [============================>.] - ETA: 0s - loss: 0.4216 - accuracy: 0.8406\n",
            "Epoch 00111: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4200 - accuracy: 0.8415 - val_loss: 0.4401 - val_accuracy: 0.8329\n",
            "Epoch 112/256\n",
            "818/821 [============================>.] - ETA: 0s - loss: 0.4202 - accuracy: 0.8417\n",
            "Epoch 00112: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4221 - accuracy: 0.8415 - val_loss: 0.4387 - val_accuracy: 0.8329\n",
            "Epoch 113/256\n",
            "797/821 [============================>.] - ETA: 0s - loss: 0.4184 - accuracy: 0.8432\n",
            "Epoch 00113: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4203 - accuracy: 0.8415 - val_loss: 0.4386 - val_accuracy: 0.8329\n",
            "Epoch 114/256\n",
            "805/821 [============================>.] - ETA: 0s - loss: 0.4183 - accuracy: 0.8425\n",
            "Epoch 00114: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4203 - accuracy: 0.8415 - val_loss: 0.4394 - val_accuracy: 0.8329\n",
            "Epoch 115/256\n",
            "806/821 [============================>.] - ETA: 0s - loss: 0.4214 - accuracy: 0.8406\n",
            "Epoch 00115: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4200 - accuracy: 0.8415 - val_loss: 0.4394 - val_accuracy: 0.8329\n",
            "Epoch 116/256\n",
            "812/821 [============================>.] - ETA: 0s - loss: 0.4198 - accuracy: 0.8421\n",
            "Epoch 00116: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4202 - accuracy: 0.8415 - val_loss: 0.4401 - val_accuracy: 0.8329\n",
            "Epoch 117/256\n",
            "789/821 [===========================>..] - ETA: 0s - loss: 0.4221 - accuracy: 0.8400\n",
            "Epoch 00117: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4196 - accuracy: 0.8415 - val_loss: 0.4430 - val_accuracy: 0.8329\n",
            "Epoch 118/256\n",
            "804/821 [============================>.] - ETA: 0s - loss: 0.4184 - accuracy: 0.8433\n",
            "Epoch 00118: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4208 - accuracy: 0.8415 - val_loss: 0.4386 - val_accuracy: 0.8329\n",
            "Epoch 119/256\n",
            "792/821 [===========================>..] - ETA: 0s - loss: 0.4199 - accuracy: 0.8412\n",
            "Epoch 00119: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4220 - accuracy: 0.8415 - val_loss: 0.4389 - val_accuracy: 0.8329\n",
            "Epoch 120/256\n",
            "793/821 [===========================>..] - ETA: 0s - loss: 0.4181 - accuracy: 0.8427\n",
            "Epoch 00120: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4201 - accuracy: 0.8415 - val_loss: 0.4407 - val_accuracy: 0.8329\n",
            "Epoch 121/256\n",
            "817/821 [============================>.] - ETA: 0s - loss: 0.4208 - accuracy: 0.8415\n",
            "Epoch 00121: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.4203 - accuracy: 0.8415 - val_loss: 0.4402 - val_accuracy: 0.8329\n",
            "Epoch 122/256\n",
            "809/821 [============================>.] - ETA: 0s - loss: 0.4212 - accuracy: 0.8412\n",
            "Epoch 00122: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4204 - accuracy: 0.8415 - val_loss: 0.4385 - val_accuracy: 0.8329\n",
            "Epoch 123/256\n",
            "818/821 [============================>.] - ETA: 0s - loss: 0.4196 - accuracy: 0.8417\n",
            "Epoch 00123: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.4201 - accuracy: 0.8415 - val_loss: 0.4406 - val_accuracy: 0.8329\n",
            "Epoch 124/256\n",
            "813/821 [============================>.] - ETA: 0s - loss: 0.4211 - accuracy: 0.8410\n",
            "Epoch 00124: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4202 - accuracy: 0.8415 - val_loss: 0.4393 - val_accuracy: 0.8329\n",
            "Epoch 125/256\n",
            "805/821 [============================>.] - ETA: 0s - loss: 0.4218 - accuracy: 0.8413\n",
            "Epoch 00125: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4205 - accuracy: 0.8415 - val_loss: 0.4389 - val_accuracy: 0.8329\n",
            "Epoch 126/256\n",
            "802/821 [============================>.] - ETA: 0s - loss: 0.4220 - accuracy: 0.8404\n",
            "Epoch 00126: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4203 - accuracy: 0.8415 - val_loss: 0.4387 - val_accuracy: 0.8329\n",
            "Epoch 127/256\n",
            "817/821 [============================>.] - ETA: 0s - loss: 0.4201 - accuracy: 0.8415\n",
            "Epoch 00127: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.4202 - accuracy: 0.8415 - val_loss: 0.4390 - val_accuracy: 0.8329\n",
            "Epoch 128/256\n",
            "790/821 [===========================>..] - ETA: 0s - loss: 0.4234 - accuracy: 0.8399\n",
            "Epoch 00128: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.4201 - accuracy: 0.8415 - val_loss: 0.4419 - val_accuracy: 0.8329\n",
            "Epoch 129/256\n",
            "795/821 [============================>.] - ETA: 0s - loss: 0.4188 - accuracy: 0.8428\n",
            "Epoch 00129: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4205 - accuracy: 0.8415 - val_loss: 0.4395 - val_accuracy: 0.8329\n",
            "Epoch 130/256\n",
            "813/821 [============================>.] - ETA: 0s - loss: 0.4191 - accuracy: 0.8423\n",
            "Epoch 00130: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.4204 - accuracy: 0.8415 - val_loss: 0.4387 - val_accuracy: 0.8329\n",
            "Epoch 131/256\n",
            "817/821 [============================>.] - ETA: 0s - loss: 0.4209 - accuracy: 0.8412\n",
            "Epoch 00131: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.4204 - accuracy: 0.8415 - val_loss: 0.4388 - val_accuracy: 0.8329\n",
            "Epoch 132/256\n",
            "803/821 [============================>.] - ETA: 0s - loss: 0.4207 - accuracy: 0.8415\n",
            "Epoch 00132: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.4202 - accuracy: 0.8415 - val_loss: 0.4395 - val_accuracy: 0.8329\n",
            "Epoch 133/256\n",
            "806/821 [============================>.] - ETA: 0s - loss: 0.4200 - accuracy: 0.8415\n",
            "Epoch 00133: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4203 - accuracy: 0.8415 - val_loss: 0.4388 - val_accuracy: 0.8329\n",
            "Epoch 134/256\n",
            "810/821 [============================>.] - ETA: 0s - loss: 0.4207 - accuracy: 0.8417\n",
            "Epoch 00134: val_loss did not improve from 0.43825\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4214 - accuracy: 0.8415 - val_loss: 0.4383 - val_accuracy: 0.8329\n",
            "Epoch 135/256\n",
            "816/821 [============================>.] - ETA: 0s - loss: 0.4206 - accuracy: 0.8416\n",
            "Epoch 00135: val_loss improved from 0.43825 to 0.43822, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4200 - accuracy: 0.8415 - val_loss: 0.4382 - val_accuracy: 0.8329\n",
            "Epoch 136/256\n",
            "797/821 [============================>.] - ETA: 0s - loss: 0.4216 - accuracy: 0.8403\n",
            "Epoch 00136: val_loss did not improve from 0.43822\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4196 - accuracy: 0.8415 - val_loss: 0.4421 - val_accuracy: 0.8329\n",
            "Epoch 137/256\n",
            "821/821 [==============================] - ETA: 0s - loss: 0.4200 - accuracy: 0.8415\n",
            "Epoch 00137: val_loss improved from 0.43822 to 0.43805, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.4200 - accuracy: 0.8415 - val_loss: 0.4381 - val_accuracy: 0.8329\n",
            "Epoch 138/256\n",
            "791/821 [===========================>..] - ETA: 0s - loss: 0.4202 - accuracy: 0.8417\n",
            "Epoch 00138: val_loss did not improve from 0.43805\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4204 - accuracy: 0.8415 - val_loss: 0.4387 - val_accuracy: 0.8329\n",
            "Epoch 139/256\n",
            "804/821 [============================>.] - ETA: 0s - loss: 0.4191 - accuracy: 0.8420\n",
            "Epoch 00139: val_loss did not improve from 0.43805\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4221 - accuracy: 0.8415 - val_loss: 0.4390 - val_accuracy: 0.8329\n",
            "Epoch 140/256\n",
            "788/821 [===========================>..] - ETA: 0s - loss: 0.4199 - accuracy: 0.8411\n",
            "Epoch 00140: val_loss did not improve from 0.43805\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.4200 - accuracy: 0.8415 - val_loss: 0.4418 - val_accuracy: 0.8329\n",
            "Epoch 141/256\n",
            "808/821 [============================>.] - ETA: 0s - loss: 0.4204 - accuracy: 0.8416\n",
            "Epoch 00141: val_loss did not improve from 0.43805\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4212 - accuracy: 0.8415 - val_loss: 0.4405 - val_accuracy: 0.8329\n",
            "Epoch 142/256\n",
            "794/821 [============================>.] - ETA: 0s - loss: 0.4229 - accuracy: 0.8404\n",
            "Epoch 00142: val_loss did not improve from 0.43805\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4204 - accuracy: 0.8415 - val_loss: 0.4395 - val_accuracy: 0.8329\n",
            "Epoch 143/256\n",
            "795/821 [============================>.] - ETA: 0s - loss: 0.4214 - accuracy: 0.8412\n",
            "Epoch 00143: val_loss did not improve from 0.43805\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.4203 - accuracy: 0.8415 - val_loss: 0.4386 - val_accuracy: 0.8329\n",
            "Epoch 144/256\n",
            "801/821 [============================>.] - ETA: 0s - loss: 0.4201 - accuracy: 0.8424\n",
            "Epoch 00144: val_loss did not improve from 0.43805\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4201 - accuracy: 0.8415 - val_loss: 0.4396 - val_accuracy: 0.8329\n",
            "Epoch 145/256\n",
            "815/821 [============================>.] - ETA: 0s - loss: 0.4201 - accuracy: 0.8417\n",
            "Epoch 00145: val_loss did not improve from 0.43805\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4200 - accuracy: 0.8415 - val_loss: 0.4402 - val_accuracy: 0.8329\n",
            "Epoch 146/256\n",
            "819/821 [============================>.] - ETA: 0s - loss: 0.4204 - accuracy: 0.8416\n",
            "Epoch 00146: val_loss improved from 0.43805 to 0.43802, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4219 - accuracy: 0.8415 - val_loss: 0.4380 - val_accuracy: 0.8329\n",
            "Epoch 147/256\n",
            "802/821 [============================>.] - ETA: 0s - loss: 0.4216 - accuracy: 0.8407\n",
            "Epoch 00147: val_loss did not improve from 0.43802\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4201 - accuracy: 0.8415 - val_loss: 0.4392 - val_accuracy: 0.8329\n",
            "Epoch 148/256\n",
            "810/821 [============================>.] - ETA: 0s - loss: 0.4193 - accuracy: 0.8426\n",
            "Epoch 00148: val_loss did not improve from 0.43802\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4200 - accuracy: 0.8415 - val_loss: 0.4398 - val_accuracy: 0.8329\n",
            "Epoch 149/256\n",
            "808/821 [============================>.] - ETA: 0s - loss: 0.4200 - accuracy: 0.8422\n",
            "Epoch 00149: val_loss did not improve from 0.43802\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4215 - accuracy: 0.8415 - val_loss: 0.4392 - val_accuracy: 0.8329\n",
            "Epoch 150/256\n",
            "797/821 [============================>.] - ETA: 0s - loss: 0.4231 - accuracy: 0.8403\n",
            "Epoch 00150: val_loss did not improve from 0.43802\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4201 - accuracy: 0.8415 - val_loss: 0.4406 - val_accuracy: 0.8329\n",
            "Epoch 151/256\n",
            "812/821 [============================>.] - ETA: 0s - loss: 0.4197 - accuracy: 0.8424\n",
            "Epoch 00151: val_loss did not improve from 0.43802\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4219 - accuracy: 0.8415 - val_loss: 0.4386 - val_accuracy: 0.8329\n",
            "Epoch 152/256\n",
            "808/821 [============================>.] - ETA: 0s - loss: 0.4209 - accuracy: 0.8413\n",
            "Epoch 00152: val_loss did not improve from 0.43802\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.4200 - accuracy: 0.8415 - val_loss: 0.4416 - val_accuracy: 0.8329\n",
            "Epoch 153/256\n",
            "778/821 [===========================>..] - ETA: 0s - loss: 0.4215 - accuracy: 0.8416\n",
            "Epoch 00153: val_loss did not improve from 0.43802\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4220 - accuracy: 0.8415 - val_loss: 0.4387 - val_accuracy: 0.8329\n",
            "Epoch 154/256\n",
            "809/821 [============================>.] - ETA: 0s - loss: 0.4229 - accuracy: 0.8399\n",
            "Epoch 00154: val_loss did not improve from 0.43802\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4199 - accuracy: 0.8415 - val_loss: 0.4411 - val_accuracy: 0.8329\n",
            "Epoch 155/256\n",
            "817/821 [============================>.] - ETA: 0s - loss: 0.4193 - accuracy: 0.8424\n",
            "Epoch 00155: val_loss did not improve from 0.43802\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4205 - accuracy: 0.8415 - val_loss: 0.4392 - val_accuracy: 0.8329\n",
            "Epoch 156/256\n",
            "803/821 [============================>.] - ETA: 0s - loss: 0.4196 - accuracy: 0.8425\n",
            "Epoch 00156: val_loss did not improve from 0.43802\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4202 - accuracy: 0.8415 - val_loss: 0.4392 - val_accuracy: 0.8329\n",
            "Epoch 157/256\n",
            "776/821 [===========================>..] - ETA: 0s - loss: 0.4203 - accuracy: 0.8421\n",
            "Epoch 00157: val_loss did not improve from 0.43802\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4200 - accuracy: 0.8415 - val_loss: 0.4390 - val_accuracy: 0.8329\n",
            "Epoch 158/256\n",
            "818/821 [============================>.] - ETA: 0s - loss: 0.4213 - accuracy: 0.8411\n",
            "Epoch 00158: val_loss did not improve from 0.43802\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4204 - accuracy: 0.8415 - val_loss: 0.4387 - val_accuracy: 0.8329\n",
            "Epoch 159/256\n",
            "818/821 [============================>.] - ETA: 0s - loss: 0.4204 - accuracy: 0.8417\n",
            "Epoch 00159: val_loss did not improve from 0.43802\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4204 - accuracy: 0.8415 - val_loss: 0.4397 - val_accuracy: 0.8329\n",
            "Epoch 160/256\n",
            "811/821 [============================>.] - ETA: 0s - loss: 0.4217 - accuracy: 0.8409\n",
            "Epoch 00160: val_loss did not improve from 0.43802\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4201 - accuracy: 0.8415 - val_loss: 0.4383 - val_accuracy: 0.8329\n",
            "Epoch 161/256\n",
            "815/821 [============================>.] - ETA: 0s - loss: 0.4202 - accuracy: 0.8417\n",
            "Epoch 00161: val_loss did not improve from 0.43802\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4199 - accuracy: 0.8415 - val_loss: 0.4387 - val_accuracy: 0.8329\n",
            "Epoch 162/256\n",
            "813/821 [============================>.] - ETA: 0s - loss: 0.4197 - accuracy: 0.8419\n",
            "Epoch 00162: val_loss did not improve from 0.43802\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4196 - accuracy: 0.8415 - val_loss: 0.4417 - val_accuracy: 0.8329\n",
            "Epoch 163/256\n",
            "781/821 [===========================>..] - ETA: 0s - loss: 0.4236 - accuracy: 0.8396\n",
            "Epoch 00163: val_loss did not improve from 0.43802\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4219 - accuracy: 0.8415 - val_loss: 0.4391 - val_accuracy: 0.8329\n",
            "Epoch 164/256\n",
            "801/821 [============================>.] - ETA: 0s - loss: 0.4210 - accuracy: 0.8408\n",
            "Epoch 00164: val_loss did not improve from 0.43802\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4202 - accuracy: 0.8415 - val_loss: 0.4388 - val_accuracy: 0.8329\n",
            "Epoch 165/256\n",
            "783/821 [===========================>..] - ETA: 0s - loss: 0.4214 - accuracy: 0.8413\n",
            "Epoch 00165: val_loss did not improve from 0.43802\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4203 - accuracy: 0.8415 - val_loss: 0.4396 - val_accuracy: 0.8329\n",
            "Epoch 166/256\n",
            "777/821 [===========================>..] - ETA: 0s - loss: 0.4185 - accuracy: 0.8430\n",
            "Epoch 00166: val_loss did not improve from 0.43802\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4207 - accuracy: 0.8415 - val_loss: 0.4388 - val_accuracy: 0.8329\n",
            "Epoch 167/256\n",
            "780/821 [===========================>..] - ETA: 0s - loss: 0.4184 - accuracy: 0.8420\n",
            "Epoch 00167: val_loss did not improve from 0.43802\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4201 - accuracy: 0.8415 - val_loss: 0.4402 - val_accuracy: 0.8329\n",
            "Epoch 168/256\n",
            "782/821 [===========================>..] - ETA: 0s - loss: 0.4233 - accuracy: 0.8405\n",
            "Epoch 00168: val_loss did not improve from 0.43802\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4204 - accuracy: 0.8415 - val_loss: 0.4395 - val_accuracy: 0.8329\n",
            "Epoch 169/256\n",
            "796/821 [============================>.] - ETA: 0s - loss: 0.4211 - accuracy: 0.8408\n",
            "Epoch 00169: val_loss did not improve from 0.43802\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.4200 - accuracy: 0.8415 - val_loss: 0.4416 - val_accuracy: 0.8329\n",
            "Epoch 170/256\n",
            "775/821 [===========================>..] - ETA: 0s - loss: 0.4227 - accuracy: 0.8410\n",
            "Epoch 00170: val_loss did not improve from 0.43802\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.4205 - accuracy: 0.8415 - val_loss: 0.4390 - val_accuracy: 0.8329\n",
            "Epoch 171/256\n",
            "778/821 [===========================>..] - ETA: 0s - loss: 0.4171 - accuracy: 0.8432\n",
            "Epoch 00171: val_loss did not improve from 0.43802\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4202 - accuracy: 0.8415 - val_loss: 0.4394 - val_accuracy: 0.8329\n",
            "Epoch 172/256\n",
            "809/821 [============================>.] - ETA: 0s - loss: 0.4210 - accuracy: 0.8412\n",
            "Epoch 00172: val_loss did not improve from 0.43802\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4205 - accuracy: 0.8415 - val_loss: 0.4387 - val_accuracy: 0.8329\n",
            "Epoch 173/256\n",
            "808/821 [============================>.] - ETA: 0s - loss: 0.4215 - accuracy: 0.8410\n",
            "Epoch 00173: val_loss did not improve from 0.43802\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4205 - accuracy: 0.8415 - val_loss: 0.4395 - val_accuracy: 0.8329\n",
            "Epoch 174/256\n",
            "787/821 [===========================>..] - ETA: 0s - loss: 0.4156 - accuracy: 0.8447\n",
            "Epoch 00174: val_loss did not improve from 0.43802\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4201 - accuracy: 0.8415 - val_loss: 0.4381 - val_accuracy: 0.8329\n",
            "Epoch 175/256\n",
            "786/821 [===========================>..] - ETA: 0s - loss: 0.4237 - accuracy: 0.8394\n",
            "Epoch 00175: val_loss did not improve from 0.43802\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4199 - accuracy: 0.8415 - val_loss: 0.4414 - val_accuracy: 0.8329\n",
            "Epoch 176/256\n",
            "798/821 [============================>.] - ETA: 0s - loss: 0.4221 - accuracy: 0.8402\n",
            "Epoch 00176: val_loss did not improve from 0.43802\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4203 - accuracy: 0.8415 - val_loss: 0.4400 - val_accuracy: 0.8329\n",
            "Epoch 177/256\n",
            "802/821 [============================>.] - ETA: 0s - loss: 0.4211 - accuracy: 0.8410\n",
            "Epoch 00177: val_loss did not improve from 0.43802\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.4201 - accuracy: 0.8415 - val_loss: 0.4404 - val_accuracy: 0.8329\n",
            "Epoch 178/256\n",
            "819/821 [============================>.] - ETA: 0s - loss: 0.4212 - accuracy: 0.8413\n",
            "Epoch 00178: val_loss did not improve from 0.43802\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4206 - accuracy: 0.8415 - val_loss: 0.4404 - val_accuracy: 0.8329\n",
            "Epoch 179/256\n",
            "799/821 [============================>.] - ETA: 0s - loss: 0.4205 - accuracy: 0.8411\n",
            "Epoch 00179: val_loss did not improve from 0.43802\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4203 - accuracy: 0.8415 - val_loss: 0.4386 - val_accuracy: 0.8329\n",
            "Epoch 180/256\n",
            "816/821 [============================>.] - ETA: 0s - loss: 0.4205 - accuracy: 0.8413\n",
            "Epoch 00180: val_loss did not improve from 0.43802\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4196 - accuracy: 0.8415 - val_loss: 0.4421 - val_accuracy: 0.8329\n",
            "Epoch 181/256\n",
            "804/821 [============================>.] - ETA: 0s - loss: 0.4201 - accuracy: 0.8424\n",
            "Epoch 00181: val_loss did not improve from 0.43802\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4203 - accuracy: 0.8415 - val_loss: 0.4389 - val_accuracy: 0.8329\n",
            "Epoch 182/256\n",
            "778/821 [===========================>..] - ETA: 0s - loss: 0.4223 - accuracy: 0.8409\n",
            "Epoch 00182: val_loss did not improve from 0.43802\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4203 - accuracy: 0.8415 - val_loss: 0.4392 - val_accuracy: 0.8329\n",
            "Epoch 183/256\n",
            "777/821 [===========================>..] - ETA: 0s - loss: 0.4226 - accuracy: 0.8404\n",
            "Epoch 00183: val_loss did not improve from 0.43802\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4204 - accuracy: 0.8415 - val_loss: 0.4385 - val_accuracy: 0.8329\n",
            "Epoch 184/256\n",
            "776/821 [===========================>..] - ETA: 0s - loss: 0.4214 - accuracy: 0.8415\n",
            "Epoch 00184: val_loss did not improve from 0.43802\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4202 - accuracy: 0.8415 - val_loss: 0.4398 - val_accuracy: 0.8329\n",
            "Epoch 185/256\n",
            "803/821 [============================>.] - ETA: 0s - loss: 0.4184 - accuracy: 0.8425\n",
            "Epoch 00185: val_loss did not improve from 0.43802\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4204 - accuracy: 0.8415 - val_loss: 0.4385 - val_accuracy: 0.8329\n",
            "Epoch 186/256\n",
            "784/821 [===========================>..] - ETA: 0s - loss: 0.4223 - accuracy: 0.8406\n",
            "Epoch 00186: val_loss did not improve from 0.43802\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4204 - accuracy: 0.8415 - val_loss: 0.4408 - val_accuracy: 0.8329\n",
            "Epoch 187/256\n",
            "808/821 [============================>.] - ETA: 0s - loss: 0.4213 - accuracy: 0.8407\n",
            "Epoch 00187: val_loss did not improve from 0.43802\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4202 - accuracy: 0.8415 - val_loss: 0.4383 - val_accuracy: 0.8329\n",
            "Epoch 188/256\n",
            "773/821 [===========================>..] - ETA: 0s - loss: 0.4233 - accuracy: 0.8389\n",
            "Epoch 00188: val_loss did not improve from 0.43802\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4197 - accuracy: 0.8415 - val_loss: 0.4434 - val_accuracy: 0.8329\n",
            "Epoch 189/256\n",
            "786/821 [===========================>..] - ETA: 0s - loss: 0.4211 - accuracy: 0.8410\n",
            "Epoch 00189: val_loss did not improve from 0.43802\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4203 - accuracy: 0.8415 - val_loss: 0.4432 - val_accuracy: 0.8329\n",
            "Epoch 190/256\n",
            "802/821 [============================>.] - ETA: 0s - loss: 0.4211 - accuracy: 0.8410\n",
            "Epoch 00190: val_loss did not improve from 0.43802\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4205 - accuracy: 0.8415 - val_loss: 0.4408 - val_accuracy: 0.8329\n",
            "Epoch 191/256\n",
            "820/821 [============================>.] - ETA: 0s - loss: 0.4201 - accuracy: 0.8415\n",
            "Epoch 00191: val_loss did not improve from 0.43802\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.4198 - accuracy: 0.8415 - val_loss: 0.4458 - val_accuracy: 0.8329\n",
            "Epoch 192/256\n",
            "780/821 [===========================>..] - ETA: 0s - loss: 0.4184 - accuracy: 0.8426\n",
            "Epoch 00192: val_loss did not improve from 0.43802\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4206 - accuracy: 0.8415 - val_loss: 0.4383 - val_accuracy: 0.8329\n",
            "Epoch 193/256\n",
            "813/821 [============================>.] - ETA: 0s - loss: 0.4194 - accuracy: 0.8423\n",
            "Epoch 00193: val_loss did not improve from 0.43802\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4203 - accuracy: 0.8415 - val_loss: 0.4398 - val_accuracy: 0.8329\n",
            "Epoch 194/256\n",
            "799/821 [============================>.] - ETA: 0s - loss: 0.4178 - accuracy: 0.8432\n",
            "Epoch 00194: val_loss did not improve from 0.43802\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4202 - accuracy: 0.8415 - val_loss: 0.4381 - val_accuracy: 0.8329\n",
            "Epoch 195/256\n",
            "785/821 [===========================>..] - ETA: 0s - loss: 0.4223 - accuracy: 0.8404\n",
            "Epoch 00195: val_loss did not improve from 0.43802\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4202 - accuracy: 0.8415 - val_loss: 0.4387 - val_accuracy: 0.8329\n",
            "Epoch 196/256\n",
            "774/821 [===========================>..] - ETA: 0s - loss: 0.4215 - accuracy: 0.8404\n",
            "Epoch 00196: val_loss did not improve from 0.43802\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4200 - accuracy: 0.8415 - val_loss: 0.4417 - val_accuracy: 0.8329\n",
            "Epoch 197/256\n",
            "799/821 [============================>.] - ETA: 0s - loss: 0.4206 - accuracy: 0.8411\n",
            "Epoch 00197: val_loss did not improve from 0.43802\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4200 - accuracy: 0.8415 - val_loss: 0.4418 - val_accuracy: 0.8329\n",
            "Epoch 198/256\n",
            "821/821 [==============================] - ETA: 0s - loss: 0.4206 - accuracy: 0.8415\n",
            "Epoch 00198: val_loss improved from 0.43802 to 0.43802, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4206 - accuracy: 0.8415 - val_loss: 0.4380 - val_accuracy: 0.8329\n",
            "Epoch 199/256\n",
            "796/821 [============================>.] - ETA: 0s - loss: 0.4213 - accuracy: 0.8414\n",
            "Epoch 00199: val_loss did not improve from 0.43802\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4205 - accuracy: 0.8415 - val_loss: 0.4395 - val_accuracy: 0.8329\n",
            "Epoch 200/256\n",
            "817/821 [============================>.] - ETA: 0s - loss: 0.4216 - accuracy: 0.8409\n",
            "Epoch 00200: val_loss did not improve from 0.43802\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.4203 - accuracy: 0.8415 - val_loss: 0.4391 - val_accuracy: 0.8329\n",
            "Epoch 201/256\n",
            "810/821 [============================>.] - ETA: 0s - loss: 0.4221 - accuracy: 0.8410\n",
            "Epoch 00201: val_loss did not improve from 0.43802\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4207 - accuracy: 0.8415 - val_loss: 0.4385 - val_accuracy: 0.8329\n",
            "Epoch 202/256\n",
            "782/821 [===========================>..] - ETA: 0s - loss: 0.4185 - accuracy: 0.8434\n",
            "Epoch 00202: val_loss did not improve from 0.43802\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4204 - accuracy: 0.8415 - val_loss: 0.4382 - val_accuracy: 0.8329\n",
            "Epoch 203/256\n",
            "812/821 [============================>.] - ETA: 0s - loss: 0.4197 - accuracy: 0.8421\n",
            "Epoch 00203: val_loss did not improve from 0.43802\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4221 - accuracy: 0.8415 - val_loss: 0.4384 - val_accuracy: 0.8329\n",
            "Epoch 204/256\n",
            "814/821 [============================>.] - ETA: 0s - loss: 0.4208 - accuracy: 0.8412\n",
            "Epoch 00204: val_loss did not improve from 0.43802\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4202 - accuracy: 0.8415 - val_loss: 0.4382 - val_accuracy: 0.8329\n",
            "Epoch 205/256\n",
            "799/821 [============================>.] - ETA: 0s - loss: 0.4206 - accuracy: 0.8420\n",
            "Epoch 00205: val_loss did not improve from 0.43802\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.4215 - accuracy: 0.8415 - val_loss: 0.4395 - val_accuracy: 0.8329\n",
            "Epoch 206/256\n",
            "796/821 [============================>.] - ETA: 0s - loss: 0.4222 - accuracy: 0.8405\n",
            "Epoch 00206: val_loss did not improve from 0.43802\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4202 - accuracy: 0.8415 - val_loss: 0.4380 - val_accuracy: 0.8329\n",
            "Epoch 207/256\n",
            "798/821 [============================>.] - ETA: 0s - loss: 0.4196 - accuracy: 0.8418\n",
            "Epoch 00207: val_loss did not improve from 0.43802\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4203 - accuracy: 0.8415 - val_loss: 0.4406 - val_accuracy: 0.8329\n",
            "Epoch 208/256\n",
            "809/821 [============================>.] - ETA: 0s - loss: 0.4175 - accuracy: 0.8430\n",
            "Epoch 00208: val_loss improved from 0.43802 to 0.43798, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4222 - accuracy: 0.8415 - val_loss: 0.4380 - val_accuracy: 0.8329\n",
            "Epoch 209/256\n",
            "802/821 [============================>.] - ETA: 0s - loss: 0.4190 - accuracy: 0.8423\n",
            "Epoch 00209: val_loss did not improve from 0.43798\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4199 - accuracy: 0.8415 - val_loss: 0.4412 - val_accuracy: 0.8329\n",
            "Epoch 210/256\n",
            "819/821 [============================>.] - ETA: 0s - loss: 0.4212 - accuracy: 0.8413\n",
            "Epoch 00210: val_loss did not improve from 0.43798\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4204 - accuracy: 0.8415 - val_loss: 0.4383 - val_accuracy: 0.8329\n",
            "Epoch 211/256\n",
            "807/821 [============================>.] - ETA: 0s - loss: 0.4206 - accuracy: 0.8417\n",
            "Epoch 00211: val_loss did not improve from 0.43798\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4213 - accuracy: 0.8415 - val_loss: 0.4406 - val_accuracy: 0.8329\n",
            "Epoch 212/256\n",
            "774/821 [===========================>..] - ETA: 0s - loss: 0.4194 - accuracy: 0.8421\n",
            "Epoch 00212: val_loss did not improve from 0.43798\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4204 - accuracy: 0.8415 - val_loss: 0.4396 - val_accuracy: 0.8329\n",
            "Epoch 213/256\n",
            "795/821 [============================>.] - ETA: 0s - loss: 0.4210 - accuracy: 0.8415\n",
            "Epoch 00213: val_loss did not improve from 0.43798\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4202 - accuracy: 0.8415 - val_loss: 0.4388 - val_accuracy: 0.8329\n",
            "Epoch 214/256\n",
            "774/821 [===========================>..] - ETA: 0s - loss: 0.4210 - accuracy: 0.8417\n",
            "Epoch 00214: val_loss did not improve from 0.43798\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4202 - accuracy: 0.8415 - val_loss: 0.4385 - val_accuracy: 0.8329\n",
            "Epoch 215/256\n",
            "810/821 [============================>.] - ETA: 0s - loss: 0.4214 - accuracy: 0.8407\n",
            "Epoch 00215: val_loss did not improve from 0.43798\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4202 - accuracy: 0.8415 - val_loss: 0.4412 - val_accuracy: 0.8329\n",
            "Epoch 216/256\n",
            "815/821 [============================>.] - ETA: 0s - loss: 0.4203 - accuracy: 0.8420\n",
            "Epoch 00216: val_loss did not improve from 0.43798\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4205 - accuracy: 0.8415 - val_loss: 0.4382 - val_accuracy: 0.8329\n",
            "Epoch 217/256\n",
            "817/821 [============================>.] - ETA: 0s - loss: 0.4198 - accuracy: 0.8415\n",
            "Epoch 00217: val_loss did not improve from 0.43798\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4189 - accuracy: 0.8415 - val_loss: 0.4432 - val_accuracy: 0.8329\n",
            "Epoch 218/256\n",
            "777/821 [===========================>..] - ETA: 0s - loss: 0.4202 - accuracy: 0.8427\n",
            "Epoch 00218: val_loss did not improve from 0.43798\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4206 - accuracy: 0.8415 - val_loss: 0.4387 - val_accuracy: 0.8329\n",
            "Epoch 219/256\n",
            "819/821 [============================>.] - ETA: 0s - loss: 0.4207 - accuracy: 0.8413\n",
            "Epoch 00219: val_loss did not improve from 0.43798\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.4201 - accuracy: 0.8415 - val_loss: 0.4391 - val_accuracy: 0.8329\n",
            "Epoch 220/256\n",
            "798/821 [============================>.] - ETA: 0s - loss: 0.4186 - accuracy: 0.8427\n",
            "Epoch 00220: val_loss did not improve from 0.43798\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4214 - accuracy: 0.8415 - val_loss: 0.4380 - val_accuracy: 0.8329\n",
            "Epoch 221/256\n",
            "816/821 [============================>.] - ETA: 0s - loss: 0.4203 - accuracy: 0.8419\n",
            "Epoch 00221: val_loss did not improve from 0.43798\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4207 - accuracy: 0.8415 - val_loss: 0.4386 - val_accuracy: 0.8329\n",
            "Epoch 222/256\n",
            "811/821 [============================>.] - ETA: 0s - loss: 0.4221 - accuracy: 0.8409\n",
            "Epoch 00222: val_loss did not improve from 0.43798\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.4204 - accuracy: 0.8415 - val_loss: 0.4403 - val_accuracy: 0.8329\n",
            "Epoch 223/256\n",
            "805/821 [============================>.] - ETA: 0s - loss: 0.4193 - accuracy: 0.8422\n",
            "Epoch 00223: val_loss did not improve from 0.43798\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4208 - accuracy: 0.8415 - val_loss: 0.4381 - val_accuracy: 0.8329\n",
            "Epoch 224/256\n",
            "784/821 [===========================>..] - ETA: 0s - loss: 0.4201 - accuracy: 0.8415\n",
            "Epoch 00224: val_loss did not improve from 0.43798\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.4202 - accuracy: 0.8415 - val_loss: 0.4391 - val_accuracy: 0.8329\n",
            "Epoch 225/256\n",
            "816/821 [============================>.] - ETA: 0s - loss: 0.4204 - accuracy: 0.8416\n",
            "Epoch 00225: val_loss did not improve from 0.43798\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4208 - accuracy: 0.8415 - val_loss: 0.4384 - val_accuracy: 0.8329\n",
            "Epoch 226/256\n",
            "773/821 [===========================>..] - ETA: 0s - loss: 0.4198 - accuracy: 0.8409\n",
            "Epoch 00226: val_loss did not improve from 0.43798\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.4201 - accuracy: 0.8415 - val_loss: 0.4388 - val_accuracy: 0.8329\n",
            "Epoch 227/256\n",
            "782/821 [===========================>..] - ETA: 0s - loss: 0.4211 - accuracy: 0.8408\n",
            "Epoch 00227: val_loss did not improve from 0.43798\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4201 - accuracy: 0.8415 - val_loss: 0.4390 - val_accuracy: 0.8329\n",
            "Epoch 228/256\n",
            "780/821 [===========================>..] - ETA: 0s - loss: 0.4212 - accuracy: 0.8413\n",
            "Epoch 00228: val_loss did not improve from 0.43798\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4204 - accuracy: 0.8415 - val_loss: 0.4394 - val_accuracy: 0.8329\n",
            "Epoch 229/256\n",
            "800/821 [============================>.] - ETA: 0s - loss: 0.4220 - accuracy: 0.8409\n",
            "Epoch 00229: val_loss did not improve from 0.43798\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4201 - accuracy: 0.8415 - val_loss: 0.4425 - val_accuracy: 0.8329\n",
            "Epoch 230/256\n",
            "819/821 [============================>.] - ETA: 0s - loss: 0.4206 - accuracy: 0.8416\n",
            "Epoch 00230: val_loss did not improve from 0.43798\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4207 - accuracy: 0.8415 - val_loss: 0.4401 - val_accuracy: 0.8329\n",
            "Epoch 231/256\n",
            "784/821 [===========================>..] - ETA: 0s - loss: 0.4197 - accuracy: 0.8428\n",
            "Epoch 00231: val_loss did not improve from 0.43798\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.4205 - accuracy: 0.8415 - val_loss: 0.4384 - val_accuracy: 0.8329\n",
            "Epoch 232/256\n",
            "812/821 [============================>.] - ETA: 0s - loss: 0.4201 - accuracy: 0.8417\n",
            "Epoch 00232: val_loss did not improve from 0.43798\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4207 - accuracy: 0.8415 - val_loss: 0.4384 - val_accuracy: 0.8329\n",
            "Epoch 233/256\n",
            "820/821 [============================>.] - ETA: 0s - loss: 0.4209 - accuracy: 0.8415\n",
            "Epoch 00233: val_loss did not improve from 0.43798\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4205 - accuracy: 0.8415 - val_loss: 0.4384 - val_accuracy: 0.8329\n",
            "Epoch 234/256\n",
            "779/821 [===========================>..] - ETA: 0s - loss: 0.4202 - accuracy: 0.8418\n",
            "Epoch 00234: val_loss did not improve from 0.43798\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4203 - accuracy: 0.8415 - val_loss: 0.4384 - val_accuracy: 0.8329\n",
            "Epoch 235/256\n",
            "791/821 [===========================>..] - ETA: 0s - loss: 0.4222 - accuracy: 0.8407\n",
            "Epoch 00235: val_loss did not improve from 0.43798\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.4197 - accuracy: 0.8415 - val_loss: 0.4434 - val_accuracy: 0.8329\n",
            "Epoch 236/256\n",
            "789/821 [===========================>..] - ETA: 0s - loss: 0.4188 - accuracy: 0.8428\n",
            "Epoch 00236: val_loss did not improve from 0.43798\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.4208 - accuracy: 0.8415 - val_loss: 0.4385 - val_accuracy: 0.8329\n",
            "Epoch 237/256\n",
            "821/821 [==============================] - ETA: 0s - loss: 0.4206 - accuracy: 0.8415\n",
            "Epoch 00237: val_loss did not improve from 0.43798\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4206 - accuracy: 0.8415 - val_loss: 0.4404 - val_accuracy: 0.8329\n",
            "Epoch 238/256\n",
            "802/821 [============================>.] - ETA: 0s - loss: 0.4168 - accuracy: 0.8435\n",
            "Epoch 00238: val_loss did not improve from 0.43798\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4206 - accuracy: 0.8415 - val_loss: 0.4399 - val_accuracy: 0.8329\n",
            "Epoch 239/256\n",
            "817/821 [============================>.] - ETA: 0s - loss: 0.4208 - accuracy: 0.8415\n",
            "Epoch 00239: val_loss did not improve from 0.43798\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4227 - accuracy: 0.8415 - val_loss: 0.4381 - val_accuracy: 0.8329\n",
            "Epoch 240/256\n",
            "790/821 [===========================>..] - ETA: 0s - loss: 0.4169 - accuracy: 0.8434\n",
            "Epoch 00240: val_loss improved from 0.43798 to 0.43788, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4221 - accuracy: 0.8415 - val_loss: 0.4379 - val_accuracy: 0.8329\n",
            "Epoch 241/256\n",
            "810/821 [============================>.] - ETA: 0s - loss: 0.4208 - accuracy: 0.8414\n",
            "Epoch 00241: val_loss did not improve from 0.43788\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4212 - accuracy: 0.8415 - val_loss: 0.4406 - val_accuracy: 0.8329\n",
            "Epoch 242/256\n",
            "775/821 [===========================>..] - ETA: 0s - loss: 0.4144 - accuracy: 0.8439\n",
            "Epoch 00242: val_loss did not improve from 0.43788\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4206 - accuracy: 0.8415 - val_loss: 0.4381 - val_accuracy: 0.8329\n",
            "Epoch 243/256\n",
            "812/821 [============================>.] - ETA: 0s - loss: 0.4215 - accuracy: 0.8408\n",
            "Epoch 00243: val_loss did not improve from 0.43788\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4204 - accuracy: 0.8415 - val_loss: 0.4407 - val_accuracy: 0.8329\n",
            "Epoch 244/256\n",
            "791/821 [===========================>..] - ETA: 0s - loss: 0.4206 - accuracy: 0.8413\n",
            "Epoch 00244: val_loss did not improve from 0.43788\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.4203 - accuracy: 0.8415 - val_loss: 0.4396 - val_accuracy: 0.8329\n",
            "Epoch 245/256\n",
            "813/821 [============================>.] - ETA: 0s - loss: 0.4197 - accuracy: 0.8419\n",
            "Epoch 00245: val_loss did not improve from 0.43788\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.4205 - accuracy: 0.8415 - val_loss: 0.4411 - val_accuracy: 0.8329\n",
            "Epoch 246/256\n",
            "776/821 [===========================>..] - ETA: 0s - loss: 0.4209 - accuracy: 0.8409\n",
            "Epoch 00246: val_loss did not improve from 0.43788\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4207 - accuracy: 0.8415 - val_loss: 0.4386 - val_accuracy: 0.8329\n",
            "Epoch 247/256\n",
            "799/821 [============================>.] - ETA: 0s - loss: 0.4209 - accuracy: 0.8420\n",
            "Epoch 00247: val_loss did not improve from 0.43788\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4206 - accuracy: 0.8415 - val_loss: 0.4393 - val_accuracy: 0.8329\n",
            "Epoch 248/256\n",
            "817/821 [============================>.] - ETA: 0s - loss: 0.4198 - accuracy: 0.8418\n",
            "Epoch 00248: val_loss did not improve from 0.43788\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4206 - accuracy: 0.8415 - val_loss: 0.4407 - val_accuracy: 0.8329\n",
            "Epoch 249/256\n",
            "788/821 [===========================>..] - ETA: 0s - loss: 0.4214 - accuracy: 0.8417\n",
            "Epoch 00249: val_loss did not improve from 0.43788\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.4207 - accuracy: 0.8415 - val_loss: 0.4381 - val_accuracy: 0.8329\n",
            "Epoch 250/256\n",
            "798/821 [============================>.] - ETA: 0s - loss: 0.4198 - accuracy: 0.8424\n",
            "Epoch 00250: val_loss did not improve from 0.43788\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4204 - accuracy: 0.8415 - val_loss: 0.4409 - val_accuracy: 0.8329\n",
            "Epoch 251/256\n",
            "785/821 [===========================>..] - ETA: 0s - loss: 0.4200 - accuracy: 0.8417\n",
            "Epoch 00251: val_loss did not improve from 0.43788\n",
            "821/821 [==============================] - 1s 2ms/step - loss: 0.4207 - accuracy: 0.8415 - val_loss: 0.4389 - val_accuracy: 0.8329\n",
            "Epoch 252/256\n",
            "810/821 [============================>.] - ETA: 0s - loss: 0.4198 - accuracy: 0.8426\n",
            "Epoch 00252: val_loss did not improve from 0.43788\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4201 - accuracy: 0.8415 - val_loss: 0.4383 - val_accuracy: 0.8329\n",
            "Epoch 253/256\n",
            "808/821 [============================>.] - ETA: 0s - loss: 0.4190 - accuracy: 0.8425\n",
            "Epoch 00253: val_loss did not improve from 0.43788\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4204 - accuracy: 0.8415 - val_loss: 0.4379 - val_accuracy: 0.8329\n",
            "Epoch 254/256\n",
            "798/821 [============================>.] - ETA: 0s - loss: 0.4206 - accuracy: 0.8415\n",
            "Epoch 00254: val_loss did not improve from 0.43788\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4207 - accuracy: 0.8415 - val_loss: 0.4381 - val_accuracy: 0.8329\n",
            "Epoch 255/256\n",
            "796/821 [============================>.] - ETA: 0s - loss: 0.4171 - accuracy: 0.8433\n",
            "Epoch 00255: val_loss did not improve from 0.43788\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4199 - accuracy: 0.8415 - val_loss: 0.4385 - val_accuracy: 0.8329\n",
            "Epoch 256/256\n",
            "802/821 [============================>.] - ETA: 0s - loss: 0.4216 - accuracy: 0.8404\n",
            "Epoch 00256: val_loss did not improve from 0.43788\n",
            "821/821 [==============================] - 1s 1ms/step - loss: 0.4204 - accuracy: 0.8415 - val_loss: 0.4381 - val_accuracy: 0.8329\n",
            "Accuracy: 83.29%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpgni4v6t_KF",
        "colab_type": "text"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v44jssTrt7_V",
        "colab_type": "text"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQWh54d4JJT5",
        "colab_type": "text"
      },
      "source": [
        "#Comparison of Performances after reduction of low significant features\n",
        "\n",
        "We will remove 4 features that are least significant(or non informative):\n",
        "1. Inclination\n",
        "2.Orbit Id\n",
        "3. Relative velocity  Km per sec\n",
        "4.  Jupiter Tisserand Invariant"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75vsciqTJZzg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "np.random.shuffle(Newdata)\n",
        "X = Newdata[:, [0,1,3,4,6,7,9,10,11,12,13,14,15,16]]\n",
        "Y = Newdata[:, -1]\n",
        "from sklearn.model_selection import train_test_split\n",
        "XTRAIN, XVALID, YTRAIN, YVALID = train_test_split(X, Y, test_size = 0.3, random_state = 123)\n",
        "mean = XTRAIN.mean(axis=0)\n",
        "XTRAIN -= mean\n",
        "std = XTRAIN.std(axis=0)\n",
        "XTRAIN /= std\n",
        "XVALID -= mean\n",
        "XVALID /= std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4K-trVasKWDr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "355c0532-3d37-4a21-d266-eede0856576d"
      },
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "callback_a = ModelCheckpoint(filepath = 'Reduced_Nasa_asteroid.hdf5', monitor='val_loss', save_best_only = True, save_weights_only = True, verbose = 1)\n",
        "callback_b = EarlyStopping(monitor='val_loss', mode='min', patience=20, verbose=1)\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(1, input_dim = len(XTRAIN[0, :]), activation='sigmoid'))\n",
        "model.add(Dense(4, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer = 'rmsprop', metrics=['accuracy'])\n",
        "history = model.fit(XTRAIN, YTRAIN, validation_data=(XVALID, YVALID), epochs=256, batch_size=4, callbacks = [callback_a])\n",
        "model.load_weights('Reduced_Nasa_asteroid.hdf5')\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "prediction = model.predict(XVALID)\n",
        "accuracy = accuracy_score(YVALID, prediction.round())\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
        "accuracy = accuracy_score(YVALID, prediction.round())\n",
        "precision = precision_score(YVALID, prediction.round())\n",
        "recall = recall_score(YVALID, prediction.round())\n",
        "f1score = f1_score(YVALID, prediction.round())\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/256\n",
            "809/820 [============================>.] - ETA: 0s - loss: 0.4627 - accuracy: 0.8368\n",
            "Epoch 00001: val_loss improved from inf to 0.39064, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "820/820 [==============================] - 1s 2ms/step - loss: 0.4624 - accuracy: 0.8363 - val_loss: 0.3906 - val_accuracy: 0.8415\n",
            "Epoch 2/256\n",
            "773/820 [===========================>..] - ETA: 0s - loss: 0.3757 - accuracy: 0.8373\n",
            "Epoch 00002: val_loss improved from 0.39064 to 0.34223, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.3730 - accuracy: 0.8378 - val_loss: 0.3422 - val_accuracy: 0.8415\n",
            "Epoch 3/256\n",
            "778/820 [===========================>..] - ETA: 0s - loss: 0.3308 - accuracy: 0.8345\n",
            "Epoch 00003: val_loss improved from 0.34223 to 0.29835, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.3251 - accuracy: 0.8378 - val_loss: 0.2984 - val_accuracy: 0.8415\n",
            "Epoch 4/256\n",
            "783/820 [===========================>..] - ETA: 0s - loss: 0.2829 - accuracy: 0.8391\n",
            "Epoch 00004: val_loss improved from 0.29835 to 0.25511, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.2811 - accuracy: 0.8381 - val_loss: 0.2551 - val_accuracy: 0.8479\n",
            "Epoch 5/256\n",
            "782/820 [===========================>..] - ETA: 0s - loss: 0.2454 - accuracy: 0.8718\n",
            "Epoch 00005: val_loss improved from 0.25511 to 0.22241, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.2421 - accuracy: 0.8747 - val_loss: 0.2224 - val_accuracy: 0.9026\n",
            "Epoch 6/256\n",
            "788/820 [===========================>..] - ETA: 0s - loss: 0.2124 - accuracy: 0.9102\n",
            "Epoch 00006: val_loss improved from 0.22241 to 0.19706, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.2130 - accuracy: 0.9104 - val_loss: 0.1971 - val_accuracy: 0.9168\n",
            "Epoch 7/256\n",
            "794/820 [============================>.] - ETA: 0s - loss: 0.1902 - accuracy: 0.9260\n",
            "Epoch 00007: val_loss improved from 0.19706 to 0.17639, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1893 - accuracy: 0.9271 - val_loss: 0.1764 - val_accuracy: 0.9268\n",
            "Epoch 8/256\n",
            "770/820 [===========================>..] - ETA: 0s - loss: 0.1695 - accuracy: 0.9386\n",
            "Epoch 00008: val_loss improved from 0.17639 to 0.16071, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1692 - accuracy: 0.9396 - val_loss: 0.1607 - val_accuracy: 0.9382\n",
            "Epoch 9/256\n",
            "792/820 [===========================>..] - ETA: 0s - loss: 0.1531 - accuracy: 0.9482\n",
            "Epoch 00009: val_loss improved from 0.16071 to 0.14914, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1535 - accuracy: 0.9473 - val_loss: 0.1491 - val_accuracy: 0.9431\n",
            "Epoch 10/256\n",
            "790/820 [===========================>..] - ETA: 0s - loss: 0.1410 - accuracy: 0.9541\n",
            "Epoch 00010: val_loss improved from 0.14914 to 0.13996, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1404 - accuracy: 0.9549 - val_loss: 0.1400 - val_accuracy: 0.9502\n",
            "Epoch 11/256\n",
            "806/820 [============================>.] - ETA: 0s - loss: 0.1326 - accuracy: 0.9563\n",
            "Epoch 00011: val_loss improved from 0.13996 to 0.13301, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1314 - accuracy: 0.9570 - val_loss: 0.1330 - val_accuracy: 0.9517\n",
            "Epoch 12/256\n",
            "783/820 [===========================>..] - ETA: 0s - loss: 0.1243 - accuracy: 0.9579\n",
            "Epoch 00012: val_loss improved from 0.13301 to 0.12834, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1246 - accuracy: 0.9570 - val_loss: 0.1283 - val_accuracy: 0.9538\n",
            "Epoch 13/256\n",
            "789/820 [===========================>..] - ETA: 0s - loss: 0.1202 - accuracy: 0.9566\n",
            "Epoch 00013: val_loss improved from 0.12834 to 0.12531, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1196 - accuracy: 0.9564 - val_loss: 0.1253 - val_accuracy: 0.9545\n",
            "Epoch 14/256\n",
            "772/820 [===========================>..] - ETA: 0s - loss: 0.1126 - accuracy: 0.9573\n",
            "Epoch 00014: val_loss improved from 0.12531 to 0.12105, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1156 - accuracy: 0.9576 - val_loss: 0.1210 - val_accuracy: 0.9559\n",
            "Epoch 15/256\n",
            "790/820 [===========================>..] - ETA: 0s - loss: 0.1114 - accuracy: 0.9589\n",
            "Epoch 00015: val_loss improved from 0.12105 to 0.11920, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "820/820 [==============================] - 1s 2ms/step - loss: 0.1121 - accuracy: 0.9582 - val_loss: 0.1192 - val_accuracy: 0.9574\n",
            "Epoch 16/256\n",
            "807/820 [============================>.] - ETA: 0s - loss: 0.1087 - accuracy: 0.9569\n",
            "Epoch 00016: val_loss improved from 0.11920 to 0.11786, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "820/820 [==============================] - 1s 2ms/step - loss: 0.1101 - accuracy: 0.9558 - val_loss: 0.1179 - val_accuracy: 0.9552\n",
            "Epoch 17/256\n",
            "787/820 [===========================>..] - ETA: 0s - loss: 0.1068 - accuracy: 0.9597\n",
            "Epoch 00017: val_loss did not improve from 0.11786\n",
            "820/820 [==============================] - 1s 2ms/step - loss: 0.1082 - accuracy: 0.9582 - val_loss: 0.1185 - val_accuracy: 0.9531\n",
            "Epoch 18/256\n",
            "788/820 [===========================>..] - ETA: 0s - loss: 0.1068 - accuracy: 0.9559\n",
            "Epoch 00018: val_loss improved from 0.11786 to 0.11471, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1070 - accuracy: 0.9558 - val_loss: 0.1147 - val_accuracy: 0.9559\n",
            "Epoch 19/256\n",
            "793/820 [============================>.] - ETA: 0s - loss: 0.1071 - accuracy: 0.9543\n",
            "Epoch 00019: val_loss improved from 0.11471 to 0.11341, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "820/820 [==============================] - 1s 2ms/step - loss: 0.1058 - accuracy: 0.9549 - val_loss: 0.1134 - val_accuracy: 0.9559\n",
            "Epoch 20/256\n",
            "781/820 [===========================>..] - ETA: 0s - loss: 0.1027 - accuracy: 0.9565\n",
            "Epoch 00020: val_loss improved from 0.11341 to 0.11259, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1055 - accuracy: 0.9555 - val_loss: 0.1126 - val_accuracy: 0.9574\n",
            "Epoch 21/256\n",
            "819/820 [============================>.] - ETA: 0s - loss: 0.1049 - accuracy: 0.9548\n",
            "Epoch 00021: val_loss improved from 0.11259 to 0.11227, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1048 - accuracy: 0.9549 - val_loss: 0.1123 - val_accuracy: 0.9566\n",
            "Epoch 22/256\n",
            "775/820 [===========================>..] - ETA: 0s - loss: 0.1035 - accuracy: 0.9581\n",
            "Epoch 00022: val_loss did not improve from 0.11227\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1040 - accuracy: 0.9576 - val_loss: 0.1125 - val_accuracy: 0.9545\n",
            "Epoch 23/256\n",
            "819/820 [============================>.] - ETA: 0s - loss: 0.1043 - accuracy: 0.9560\n",
            "Epoch 00023: val_loss did not improve from 0.11227\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1042 - accuracy: 0.9561 - val_loss: 0.1133 - val_accuracy: 0.9531\n",
            "Epoch 24/256\n",
            "807/820 [============================>.] - ETA: 0s - loss: 0.1036 - accuracy: 0.9572\n",
            "Epoch 00024: val_loss did not improve from 0.11227\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1040 - accuracy: 0.9567 - val_loss: 0.1126 - val_accuracy: 0.9552\n",
            "Epoch 25/256\n",
            "814/820 [============================>.] - ETA: 0s - loss: 0.1044 - accuracy: 0.9552\n",
            "Epoch 00025: val_loss improved from 0.11227 to 0.11220, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1041 - accuracy: 0.9552 - val_loss: 0.1122 - val_accuracy: 0.9531\n",
            "Epoch 26/256\n",
            "772/820 [===========================>..] - ETA: 0s - loss: 0.1032 - accuracy: 0.9563\n",
            "Epoch 00026: val_loss improved from 0.11220 to 0.11074, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1036 - accuracy: 0.9555 - val_loss: 0.1107 - val_accuracy: 0.9574\n",
            "Epoch 27/256\n",
            "800/820 [============================>.] - ETA: 0s - loss: 0.1055 - accuracy: 0.9572\n",
            "Epoch 00027: val_loss did not improve from 0.11074\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1050 - accuracy: 0.9567 - val_loss: 0.1110 - val_accuracy: 0.9552\n",
            "Epoch 28/256\n",
            "783/820 [===========================>..] - ETA: 0s - loss: 0.1048 - accuracy: 0.9553\n",
            "Epoch 00028: val_loss improved from 0.11074 to 0.10999, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1041 - accuracy: 0.9555 - val_loss: 0.1100 - val_accuracy: 0.9559\n",
            "Epoch 29/256\n",
            "820/820 [==============================] - ETA: 0s - loss: 0.1039 - accuracy: 0.9552\n",
            "Epoch 00029: val_loss did not improve from 0.10999\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1039 - accuracy: 0.9552 - val_loss: 0.1103 - val_accuracy: 0.9566\n",
            "Epoch 30/256\n",
            "782/820 [===========================>..] - ETA: 0s - loss: 0.1052 - accuracy: 0.9549\n",
            "Epoch 00030: val_loss did not improve from 0.10999\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1041 - accuracy: 0.9558 - val_loss: 0.1107 - val_accuracy: 0.9552\n",
            "Epoch 31/256\n",
            "784/820 [===========================>..] - ETA: 0s - loss: 0.1073 - accuracy: 0.9541\n",
            "Epoch 00031: val_loss did not improve from 0.10999\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1055 - accuracy: 0.9552 - val_loss: 0.1116 - val_accuracy: 0.9538\n",
            "Epoch 32/256\n",
            "782/820 [===========================>..] - ETA: 0s - loss: 0.1071 - accuracy: 0.9543\n",
            "Epoch 00032: val_loss did not improve from 0.10999\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1057 - accuracy: 0.9555 - val_loss: 0.1107 - val_accuracy: 0.9559\n",
            "Epoch 33/256\n",
            "782/820 [===========================>..] - ETA: 0s - loss: 0.1042 - accuracy: 0.9568\n",
            "Epoch 00033: val_loss did not improve from 0.10999\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1052 - accuracy: 0.9555 - val_loss: 0.1115 - val_accuracy: 0.9559\n",
            "Epoch 34/256\n",
            "776/820 [===========================>..] - ETA: 0s - loss: 0.1065 - accuracy: 0.9539\n",
            "Epoch 00034: val_loss did not improve from 0.10999\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1055 - accuracy: 0.9546 - val_loss: 0.1107 - val_accuracy: 0.9581\n",
            "Epoch 35/256\n",
            "800/820 [============================>.] - ETA: 0s - loss: 0.1052 - accuracy: 0.9559\n",
            "Epoch 00035: val_loss did not improve from 0.10999\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1057 - accuracy: 0.9555 - val_loss: 0.1124 - val_accuracy: 0.9566\n",
            "Epoch 36/256\n",
            "779/820 [===========================>..] - ETA: 0s - loss: 0.1089 - accuracy: 0.9544\n",
            "Epoch 00036: val_loss did not improve from 0.10999\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1061 - accuracy: 0.9558 - val_loss: 0.1105 - val_accuracy: 0.9545\n",
            "Epoch 37/256\n",
            "808/820 [============================>.] - ETA: 0s - loss: 0.1061 - accuracy: 0.9551\n",
            "Epoch 00037: val_loss did not improve from 0.10999\n",
            "820/820 [==============================] - 1s 2ms/step - loss: 0.1067 - accuracy: 0.9549 - val_loss: 0.1132 - val_accuracy: 0.9545\n",
            "Epoch 38/256\n",
            "799/820 [============================>.] - ETA: 0s - loss: 0.1079 - accuracy: 0.9537\n",
            "Epoch 00038: val_loss did not improve from 0.10999\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1072 - accuracy: 0.9540 - val_loss: 0.1111 - val_accuracy: 0.9552\n",
            "Epoch 39/256\n",
            "781/820 [===========================>..] - ETA: 0s - loss: 0.1067 - accuracy: 0.9561\n",
            "Epoch 00039: val_loss did not improve from 0.10999\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1082 - accuracy: 0.9552 - val_loss: 0.1118 - val_accuracy: 0.9566\n",
            "Epoch 40/256\n",
            "792/820 [===========================>..] - ETA: 0s - loss: 0.1066 - accuracy: 0.9527\n",
            "Epoch 00040: val_loss improved from 0.10999 to 0.10987, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1058 - accuracy: 0.9530 - val_loss: 0.1099 - val_accuracy: 0.9566\n",
            "Epoch 41/256\n",
            "796/820 [============================>.] - ETA: 0s - loss: 0.1092 - accuracy: 0.9551\n",
            "Epoch 00041: val_loss did not improve from 0.10987\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1089 - accuracy: 0.9549 - val_loss: 0.1107 - val_accuracy: 0.9566\n",
            "Epoch 42/256\n",
            "811/820 [============================>.] - ETA: 0s - loss: 0.1100 - accuracy: 0.9531\n",
            "Epoch 00042: val_loss did not improve from 0.10987\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1091 - accuracy: 0.9537 - val_loss: 0.1109 - val_accuracy: 0.9566\n",
            "Epoch 43/256\n",
            "799/820 [============================>.] - ETA: 0s - loss: 0.1090 - accuracy: 0.9553\n",
            "Epoch 00043: val_loss did not improve from 0.10987\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1097 - accuracy: 0.9540 - val_loss: 0.1144 - val_accuracy: 0.9531\n",
            "Epoch 44/256\n",
            "801/820 [============================>.] - ETA: 0s - loss: 0.1045 - accuracy: 0.9529\n",
            "Epoch 00044: val_loss did not improve from 0.10987\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1106 - accuracy: 0.9530 - val_loss: 0.1156 - val_accuracy: 0.9524\n",
            "Epoch 45/256\n",
            "795/820 [============================>.] - ETA: 0s - loss: 0.1104 - accuracy: 0.9531\n",
            "Epoch 00045: val_loss did not improve from 0.10987\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1092 - accuracy: 0.9537 - val_loss: 0.1130 - val_accuracy: 0.9559\n",
            "Epoch 46/256\n",
            "787/820 [===========================>..] - ETA: 0s - loss: 0.1095 - accuracy: 0.9543\n",
            "Epoch 00046: val_loss did not improve from 0.10987\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1087 - accuracy: 0.9546 - val_loss: 0.1126 - val_accuracy: 0.9545\n",
            "Epoch 47/256\n",
            "818/820 [============================>.] - ETA: 0s - loss: 0.1106 - accuracy: 0.9548\n",
            "Epoch 00047: val_loss did not improve from 0.10987\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1104 - accuracy: 0.9549 - val_loss: 0.1108 - val_accuracy: 0.9559\n",
            "Epoch 48/256\n",
            "785/820 [===========================>..] - ETA: 0s - loss: 0.1099 - accuracy: 0.9516\n",
            "Epoch 00048: val_loss did not improve from 0.10987\n",
            "820/820 [==============================] - 1s 2ms/step - loss: 0.1096 - accuracy: 0.9524 - val_loss: 0.1112 - val_accuracy: 0.9566\n",
            "Epoch 49/256\n",
            "793/820 [============================>.] - ETA: 0s - loss: 0.1115 - accuracy: 0.9537\n",
            "Epoch 00049: val_loss did not improve from 0.10987\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1101 - accuracy: 0.9540 - val_loss: 0.1131 - val_accuracy: 0.9545\n",
            "Epoch 50/256\n",
            "792/820 [===========================>..] - ETA: 0s - loss: 0.1123 - accuracy: 0.9517\n",
            "Epoch 00050: val_loss did not improve from 0.10987\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1101 - accuracy: 0.9530 - val_loss: 0.1120 - val_accuracy: 0.9559\n",
            "Epoch 51/256\n",
            "786/820 [===========================>..] - ETA: 0s - loss: 0.1101 - accuracy: 0.9529\n",
            "Epoch 00051: val_loss did not improve from 0.10987\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1100 - accuracy: 0.9527 - val_loss: 0.1112 - val_accuracy: 0.9566\n",
            "Epoch 52/256\n",
            "776/820 [===========================>..] - ETA: 0s - loss: 0.1133 - accuracy: 0.9549\n",
            "Epoch 00052: val_loss did not improve from 0.10987\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1107 - accuracy: 0.9555 - val_loss: 0.1146 - val_accuracy: 0.9545\n",
            "Epoch 53/256\n",
            "781/820 [===========================>..] - ETA: 0s - loss: 0.1124 - accuracy: 0.9533\n",
            "Epoch 00053: val_loss did not improve from 0.10987\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1113 - accuracy: 0.9537 - val_loss: 0.1132 - val_accuracy: 0.9559\n",
            "Epoch 54/256\n",
            "777/820 [===========================>..] - ETA: 0s - loss: 0.1108 - accuracy: 0.9530\n",
            "Epoch 00054: val_loss did not improve from 0.10987\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1099 - accuracy: 0.9530 - val_loss: 0.1108 - val_accuracy: 0.9574\n",
            "Epoch 55/256\n",
            "785/820 [===========================>..] - ETA: 0s - loss: 0.1123 - accuracy: 0.9532\n",
            "Epoch 00055: val_loss improved from 0.10987 to 0.10971, saving model to Reduced_Nasa_asteroid.hdf5\n",
            "820/820 [==============================] - 1s 2ms/step - loss: 0.1111 - accuracy: 0.9530 - val_loss: 0.1097 - val_accuracy: 0.9566\n",
            "Epoch 56/256\n",
            "794/820 [============================>.] - ETA: 0s - loss: 0.1118 - accuracy: 0.9534\n",
            "Epoch 00056: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1115 - accuracy: 0.9534 - val_loss: 0.1111 - val_accuracy: 0.9574\n",
            "Epoch 57/256\n",
            "818/820 [============================>.] - ETA: 0s - loss: 0.1108 - accuracy: 0.9523\n",
            "Epoch 00057: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1108 - accuracy: 0.9524 - val_loss: 0.1101 - val_accuracy: 0.9566\n",
            "Epoch 58/256\n",
            "805/820 [============================>.] - ETA: 0s - loss: 0.1130 - accuracy: 0.9562\n",
            "Epoch 00058: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 2ms/step - loss: 0.1123 - accuracy: 0.9561 - val_loss: 0.1116 - val_accuracy: 0.9552\n",
            "Epoch 59/256\n",
            "806/820 [============================>.] - ETA: 0s - loss: 0.1077 - accuracy: 0.9541\n",
            "Epoch 00059: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1109 - accuracy: 0.9530 - val_loss: 0.1110 - val_accuracy: 0.9552\n",
            "Epoch 60/256\n",
            "814/820 [============================>.] - ETA: 0s - loss: 0.1108 - accuracy: 0.9555\n",
            "Epoch 00060: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1101 - accuracy: 0.9558 - val_loss: 0.1166 - val_accuracy: 0.9538\n",
            "Epoch 61/256\n",
            "779/820 [===========================>..] - ETA: 0s - loss: 0.1108 - accuracy: 0.9547\n",
            "Epoch 00061: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1115 - accuracy: 0.9540 - val_loss: 0.1127 - val_accuracy: 0.9559\n",
            "Epoch 62/256\n",
            "782/820 [===========================>..] - ETA: 0s - loss: 0.1118 - accuracy: 0.9533\n",
            "Epoch 00062: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1105 - accuracy: 0.9537 - val_loss: 0.1100 - val_accuracy: 0.9545\n",
            "Epoch 63/256\n",
            "773/820 [===========================>..] - ETA: 0s - loss: 0.1153 - accuracy: 0.9547\n",
            "Epoch 00063: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1118 - accuracy: 0.9558 - val_loss: 0.1138 - val_accuracy: 0.9531\n",
            "Epoch 64/256\n",
            "786/820 [===========================>..] - ETA: 0s - loss: 0.1108 - accuracy: 0.9545\n",
            "Epoch 00064: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1124 - accuracy: 0.9540 - val_loss: 0.1138 - val_accuracy: 0.9538\n",
            "Epoch 65/256\n",
            "782/820 [===========================>..] - ETA: 0s - loss: 0.1130 - accuracy: 0.9527\n",
            "Epoch 00065: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1122 - accuracy: 0.9534 - val_loss: 0.1121 - val_accuracy: 0.9559\n",
            "Epoch 66/256\n",
            "800/820 [============================>.] - ETA: 0s - loss: 0.1138 - accuracy: 0.9544\n",
            "Epoch 00066: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1123 - accuracy: 0.9552 - val_loss: 0.1157 - val_accuracy: 0.9524\n",
            "Epoch 67/256\n",
            "819/820 [============================>.] - ETA: 0s - loss: 0.1132 - accuracy: 0.9527\n",
            "Epoch 00067: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1131 - accuracy: 0.9527 - val_loss: 0.1125 - val_accuracy: 0.9545\n",
            "Epoch 68/256\n",
            "797/820 [============================>.] - ETA: 0s - loss: 0.1109 - accuracy: 0.9558\n",
            "Epoch 00068: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1129 - accuracy: 0.9546 - val_loss: 0.1132 - val_accuracy: 0.9545\n",
            "Epoch 69/256\n",
            "819/820 [============================>.] - ETA: 0s - loss: 0.1134 - accuracy: 0.9539\n",
            "Epoch 00069: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1133 - accuracy: 0.9540 - val_loss: 0.1128 - val_accuracy: 0.9552\n",
            "Epoch 70/256\n",
            "777/820 [===========================>..] - ETA: 0s - loss: 0.1121 - accuracy: 0.9533\n",
            "Epoch 00070: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1116 - accuracy: 0.9530 - val_loss: 0.1106 - val_accuracy: 0.9566\n",
            "Epoch 71/256\n",
            "791/820 [===========================>..] - ETA: 0s - loss: 0.1122 - accuracy: 0.9545\n",
            "Epoch 00071: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 2ms/step - loss: 0.1128 - accuracy: 0.9540 - val_loss: 0.1118 - val_accuracy: 0.9559\n",
            "Epoch 72/256\n",
            "819/820 [============================>.] - ETA: 0s - loss: 0.1131 - accuracy: 0.9536\n",
            "Epoch 00072: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1130 - accuracy: 0.9537 - val_loss: 0.1111 - val_accuracy: 0.9552\n",
            "Epoch 73/256\n",
            "802/820 [============================>.] - ETA: 0s - loss: 0.1136 - accuracy: 0.9542\n",
            "Epoch 00073: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1130 - accuracy: 0.9540 - val_loss: 0.1134 - val_accuracy: 0.9559\n",
            "Epoch 74/256\n",
            "790/820 [===========================>..] - ETA: 0s - loss: 0.1146 - accuracy: 0.9535\n",
            "Epoch 00074: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1129 - accuracy: 0.9540 - val_loss: 0.1134 - val_accuracy: 0.9531\n",
            "Epoch 75/256\n",
            "814/820 [============================>.] - ETA: 0s - loss: 0.1133 - accuracy: 0.9527\n",
            "Epoch 00075: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 2ms/step - loss: 0.1128 - accuracy: 0.9527 - val_loss: 0.1130 - val_accuracy: 0.9559\n",
            "Epoch 76/256\n",
            "816/820 [============================>.] - ETA: 0s - loss: 0.1131 - accuracy: 0.9565\n",
            "Epoch 00076: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1131 - accuracy: 0.9564 - val_loss: 0.1154 - val_accuracy: 0.9531\n",
            "Epoch 77/256\n",
            "776/820 [===========================>..] - ETA: 0s - loss: 0.1152 - accuracy: 0.9549\n",
            "Epoch 00077: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1150 - accuracy: 0.9540 - val_loss: 0.1122 - val_accuracy: 0.9566\n",
            "Epoch 78/256\n",
            "819/820 [============================>.] - ETA: 0s - loss: 0.1122 - accuracy: 0.9551\n",
            "Epoch 00078: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1130 - accuracy: 0.9549 - val_loss: 0.1144 - val_accuracy: 0.9559\n",
            "Epoch 79/256\n",
            "780/820 [===========================>..] - ETA: 0s - loss: 0.1143 - accuracy: 0.9542\n",
            "Epoch 00079: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1139 - accuracy: 0.9546 - val_loss: 0.1151 - val_accuracy: 0.9552\n",
            "Epoch 80/256\n",
            "810/820 [============================>.] - ETA: 0s - loss: 0.1096 - accuracy: 0.9556\n",
            "Epoch 00080: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1124 - accuracy: 0.9543 - val_loss: 0.1107 - val_accuracy: 0.9566\n",
            "Epoch 81/256\n",
            "807/820 [============================>.] - ETA: 0s - loss: 0.1135 - accuracy: 0.9548\n",
            "Epoch 00081: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1132 - accuracy: 0.9546 - val_loss: 0.1137 - val_accuracy: 0.9559\n",
            "Epoch 82/256\n",
            "789/820 [===========================>..] - ETA: 0s - loss: 0.1113 - accuracy: 0.9528\n",
            "Epoch 00082: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1123 - accuracy: 0.9524 - val_loss: 0.1155 - val_accuracy: 0.9545\n",
            "Epoch 83/256\n",
            "794/820 [============================>.] - ETA: 0s - loss: 0.1106 - accuracy: 0.9534\n",
            "Epoch 00083: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1131 - accuracy: 0.9530 - val_loss: 0.1170 - val_accuracy: 0.9524\n",
            "Epoch 84/256\n",
            "774/820 [===========================>..] - ETA: 0s - loss: 0.1155 - accuracy: 0.9528\n",
            "Epoch 00084: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1124 - accuracy: 0.9540 - val_loss: 0.1163 - val_accuracy: 0.9517\n",
            "Epoch 85/256\n",
            "800/820 [============================>.] - ETA: 0s - loss: 0.1123 - accuracy: 0.9538\n",
            "Epoch 00085: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1132 - accuracy: 0.9530 - val_loss: 0.1125 - val_accuracy: 0.9552\n",
            "Epoch 86/256\n",
            "800/820 [============================>.] - ETA: 0s - loss: 0.1129 - accuracy: 0.9528\n",
            "Epoch 00086: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1126 - accuracy: 0.9530 - val_loss: 0.1128 - val_accuracy: 0.9552\n",
            "Epoch 87/256\n",
            "781/820 [===========================>..] - ETA: 0s - loss: 0.1117 - accuracy: 0.9520\n",
            "Epoch 00087: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1133 - accuracy: 0.9524 - val_loss: 0.1131 - val_accuracy: 0.9559\n",
            "Epoch 88/256\n",
            "802/820 [============================>.] - ETA: 0s - loss: 0.1113 - accuracy: 0.9517\n",
            "Epoch 00088: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1122 - accuracy: 0.9521 - val_loss: 0.1123 - val_accuracy: 0.9559\n",
            "Epoch 89/256\n",
            "812/820 [============================>.] - ETA: 0s - loss: 0.1130 - accuracy: 0.9529\n",
            "Epoch 00089: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1128 - accuracy: 0.9530 - val_loss: 0.1124 - val_accuracy: 0.9552\n",
            "Epoch 90/256\n",
            "801/820 [============================>.] - ETA: 0s - loss: 0.1125 - accuracy: 0.9544\n",
            "Epoch 00090: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1118 - accuracy: 0.9552 - val_loss: 0.1125 - val_accuracy: 0.9545\n",
            "Epoch 91/256\n",
            "804/820 [============================>.] - ETA: 0s - loss: 0.1130 - accuracy: 0.9549\n",
            "Epoch 00091: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1119 - accuracy: 0.9552 - val_loss: 0.1167 - val_accuracy: 0.9538\n",
            "Epoch 92/256\n",
            "816/820 [============================>.] - ETA: 0s - loss: 0.1120 - accuracy: 0.9516\n",
            "Epoch 00092: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1115 - accuracy: 0.9518 - val_loss: 0.1122 - val_accuracy: 0.9574\n",
            "Epoch 93/256\n",
            "775/820 [===========================>..] - ETA: 0s - loss: 0.1121 - accuracy: 0.9561\n",
            "Epoch 00093: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1123 - accuracy: 0.9552 - val_loss: 0.1140 - val_accuracy: 0.9559\n",
            "Epoch 94/256\n",
            "786/820 [===========================>..] - ETA: 0s - loss: 0.1104 - accuracy: 0.9536\n",
            "Epoch 00094: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1126 - accuracy: 0.9527 - val_loss: 0.1120 - val_accuracy: 0.9559\n",
            "Epoch 95/256\n",
            "787/820 [===========================>..] - ETA: 0s - loss: 0.1146 - accuracy: 0.9539\n",
            "Epoch 00095: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1138 - accuracy: 0.9543 - val_loss: 0.1146 - val_accuracy: 0.9552\n",
            "Epoch 96/256\n",
            "820/820 [==============================] - ETA: 0s - loss: 0.1142 - accuracy: 0.9549\n",
            "Epoch 00096: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1142 - accuracy: 0.9549 - val_loss: 0.1160 - val_accuracy: 0.9524\n",
            "Epoch 97/256\n",
            "818/820 [============================>.] - ETA: 0s - loss: 0.1140 - accuracy: 0.9548\n",
            "Epoch 00097: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1141 - accuracy: 0.9546 - val_loss: 0.1143 - val_accuracy: 0.9559\n",
            "Epoch 98/256\n",
            "781/820 [===========================>..] - ETA: 0s - loss: 0.1171 - accuracy: 0.9507\n",
            "Epoch 00098: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1138 - accuracy: 0.9524 - val_loss: 0.1139 - val_accuracy: 0.9545\n",
            "Epoch 99/256\n",
            "776/820 [===========================>..] - ETA: 0s - loss: 0.1170 - accuracy: 0.9510\n",
            "Epoch 00099: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1131 - accuracy: 0.9530 - val_loss: 0.1141 - val_accuracy: 0.9552\n",
            "Epoch 100/256\n",
            "787/820 [===========================>..] - ETA: 0s - loss: 0.1133 - accuracy: 0.9527\n",
            "Epoch 00100: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1125 - accuracy: 0.9527 - val_loss: 0.1121 - val_accuracy: 0.9559\n",
            "Epoch 101/256\n",
            "801/820 [============================>.] - ETA: 0s - loss: 0.1130 - accuracy: 0.9557\n",
            "Epoch 00101: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1134 - accuracy: 0.9552 - val_loss: 0.1176 - val_accuracy: 0.9531\n",
            "Epoch 102/256\n",
            "775/820 [===========================>..] - ETA: 0s - loss: 0.1171 - accuracy: 0.9532\n",
            "Epoch 00102: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1156 - accuracy: 0.9540 - val_loss: 0.1148 - val_accuracy: 0.9538\n",
            "Epoch 103/256\n",
            "796/820 [============================>.] - ETA: 0s - loss: 0.1124 - accuracy: 0.9541\n",
            "Epoch 00103: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1149 - accuracy: 0.9537 - val_loss: 0.1129 - val_accuracy: 0.9559\n",
            "Epoch 104/256\n",
            "780/820 [===========================>..] - ETA: 0s - loss: 0.1067 - accuracy: 0.9545\n",
            "Epoch 00104: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 2ms/step - loss: 0.1155 - accuracy: 0.9543 - val_loss: 0.1147 - val_accuracy: 0.9531\n",
            "Epoch 105/256\n",
            "787/820 [===========================>..] - ETA: 0s - loss: 0.1173 - accuracy: 0.9514\n",
            "Epoch 00105: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1150 - accuracy: 0.9521 - val_loss: 0.1140 - val_accuracy: 0.9559\n",
            "Epoch 106/256\n",
            "775/820 [===========================>..] - ETA: 0s - loss: 0.1056 - accuracy: 0.9545\n",
            "Epoch 00106: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1141 - accuracy: 0.9518 - val_loss: 0.1123 - val_accuracy: 0.9552\n",
            "Epoch 107/256\n",
            "818/820 [============================>.] - ETA: 0s - loss: 0.1160 - accuracy: 0.9539\n",
            "Epoch 00107: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1161 - accuracy: 0.9537 - val_loss: 0.1135 - val_accuracy: 0.9559\n",
            "Epoch 108/256\n",
            "783/820 [===========================>..] - ETA: 0s - loss: 0.1145 - accuracy: 0.9524\n",
            "Epoch 00108: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1153 - accuracy: 0.9521 - val_loss: 0.1139 - val_accuracy: 0.9559\n",
            "Epoch 109/256\n",
            "788/820 [===========================>..] - ETA: 0s - loss: 0.1161 - accuracy: 0.9527\n",
            "Epoch 00109: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1152 - accuracy: 0.9524 - val_loss: 0.1131 - val_accuracy: 0.9566\n",
            "Epoch 110/256\n",
            "811/820 [============================>.] - ETA: 0s - loss: 0.1166 - accuracy: 0.9541\n",
            "Epoch 00110: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1159 - accuracy: 0.9543 - val_loss: 0.1162 - val_accuracy: 0.9552\n",
            "Epoch 111/256\n",
            "793/820 [============================>.] - ETA: 0s - loss: 0.1106 - accuracy: 0.9524\n",
            "Epoch 00111: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1151 - accuracy: 0.9534 - val_loss: 0.1146 - val_accuracy: 0.9531\n",
            "Epoch 112/256\n",
            "797/820 [============================>.] - ETA: 0s - loss: 0.1124 - accuracy: 0.9520\n",
            "Epoch 00112: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1140 - accuracy: 0.9509 - val_loss: 0.1137 - val_accuracy: 0.9552\n",
            "Epoch 113/256\n",
            "774/820 [===========================>..] - ETA: 0s - loss: 0.1172 - accuracy: 0.9528\n",
            "Epoch 00113: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1151 - accuracy: 0.9534 - val_loss: 0.1176 - val_accuracy: 0.9531\n",
            "Epoch 114/256\n",
            "773/820 [===========================>..] - ETA: 0s - loss: 0.1157 - accuracy: 0.9528\n",
            "Epoch 00114: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1163 - accuracy: 0.9530 - val_loss: 0.1137 - val_accuracy: 0.9552\n",
            "Epoch 115/256\n",
            "818/820 [============================>.] - ETA: 0s - loss: 0.1144 - accuracy: 0.9520\n",
            "Epoch 00115: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1141 - accuracy: 0.9521 - val_loss: 0.1136 - val_accuracy: 0.9552\n",
            "Epoch 116/256\n",
            "789/820 [===========================>..] - ETA: 0s - loss: 0.1181 - accuracy: 0.9515\n",
            "Epoch 00116: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 2ms/step - loss: 0.1152 - accuracy: 0.9527 - val_loss: 0.1141 - val_accuracy: 0.9545\n",
            "Epoch 117/256\n",
            "794/820 [============================>.] - ETA: 0s - loss: 0.1165 - accuracy: 0.9543\n",
            "Epoch 00117: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 2ms/step - loss: 0.1156 - accuracy: 0.9543 - val_loss: 0.1158 - val_accuracy: 0.9531\n",
            "Epoch 118/256\n",
            "793/820 [============================>.] - ETA: 0s - loss: 0.1151 - accuracy: 0.9524\n",
            "Epoch 00118: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 2ms/step - loss: 0.1146 - accuracy: 0.9524 - val_loss: 0.1163 - val_accuracy: 0.9524\n",
            "Epoch 119/256\n",
            "786/820 [===========================>..] - ETA: 0s - loss: 0.1178 - accuracy: 0.9510\n",
            "Epoch 00119: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 2ms/step - loss: 0.1146 - accuracy: 0.9524 - val_loss: 0.1130 - val_accuracy: 0.9559\n",
            "Epoch 120/256\n",
            "820/820 [==============================] - ETA: 0s - loss: 0.1167 - accuracy: 0.9549\n",
            "Epoch 00120: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 2ms/step - loss: 0.1167 - accuracy: 0.9549 - val_loss: 0.1142 - val_accuracy: 0.9545\n",
            "Epoch 121/256\n",
            "791/820 [===========================>..] - ETA: 0s - loss: 0.1149 - accuracy: 0.9535\n",
            "Epoch 00121: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1149 - accuracy: 0.9537 - val_loss: 0.1147 - val_accuracy: 0.9552\n",
            "Epoch 122/256\n",
            "787/820 [===========================>..] - ETA: 0s - loss: 0.1103 - accuracy: 0.9530\n",
            "Epoch 00122: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1157 - accuracy: 0.9537 - val_loss: 0.1144 - val_accuracy: 0.9559\n",
            "Epoch 123/256\n",
            "808/820 [============================>.] - ETA: 0s - loss: 0.1187 - accuracy: 0.9530\n",
            "Epoch 00123: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1180 - accuracy: 0.9534 - val_loss: 0.1146 - val_accuracy: 0.9545\n",
            "Epoch 124/256\n",
            "785/820 [===========================>..] - ETA: 0s - loss: 0.1174 - accuracy: 0.9557\n",
            "Epoch 00124: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1166 - accuracy: 0.9546 - val_loss: 0.1177 - val_accuracy: 0.9524\n",
            "Epoch 125/256\n",
            "818/820 [============================>.] - ETA: 0s - loss: 0.1166 - accuracy: 0.9526\n",
            "Epoch 00125: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1164 - accuracy: 0.9527 - val_loss: 0.1175 - val_accuracy: 0.9545\n",
            "Epoch 126/256\n",
            "785/820 [===========================>..] - ETA: 0s - loss: 0.1192 - accuracy: 0.9532\n",
            "Epoch 00126: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1179 - accuracy: 0.9530 - val_loss: 0.1153 - val_accuracy: 0.9552\n",
            "Epoch 127/256\n",
            "819/820 [============================>.] - ETA: 0s - loss: 0.1172 - accuracy: 0.9530\n",
            "Epoch 00127: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1170 - accuracy: 0.9530 - val_loss: 0.1151 - val_accuracy: 0.9552\n",
            "Epoch 128/256\n",
            "776/820 [===========================>..] - ETA: 0s - loss: 0.1190 - accuracy: 0.9523\n",
            "Epoch 00128: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1164 - accuracy: 0.9530 - val_loss: 0.1147 - val_accuracy: 0.9566\n",
            "Epoch 129/256\n",
            "807/820 [============================>.] - ETA: 0s - loss: 0.1179 - accuracy: 0.9526\n",
            "Epoch 00129: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1165 - accuracy: 0.9534 - val_loss: 0.1150 - val_accuracy: 0.9545\n",
            "Epoch 130/256\n",
            "771/820 [===========================>..] - ETA: 0s - loss: 0.1196 - accuracy: 0.9517\n",
            "Epoch 00130: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1188 - accuracy: 0.9515 - val_loss: 0.1183 - val_accuracy: 0.9517\n",
            "Epoch 131/256\n",
            "803/820 [============================>.] - ETA: 0s - loss: 0.1179 - accuracy: 0.9511\n",
            "Epoch 00131: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1171 - accuracy: 0.9515 - val_loss: 0.1139 - val_accuracy: 0.9552\n",
            "Epoch 132/256\n",
            "775/820 [===========================>..] - ETA: 0s - loss: 0.1151 - accuracy: 0.9519\n",
            "Epoch 00132: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1174 - accuracy: 0.9518 - val_loss: 0.1167 - val_accuracy: 0.9545\n",
            "Epoch 133/256\n",
            "817/820 [============================>.] - ETA: 0s - loss: 0.1181 - accuracy: 0.9510\n",
            "Epoch 00133: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1176 - accuracy: 0.9512 - val_loss: 0.1156 - val_accuracy: 0.9552\n",
            "Epoch 134/256\n",
            "812/820 [============================>.] - ETA: 0s - loss: 0.1173 - accuracy: 0.9517\n",
            "Epoch 00134: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1169 - accuracy: 0.9515 - val_loss: 0.1144 - val_accuracy: 0.9559\n",
            "Epoch 135/256\n",
            "773/820 [===========================>..] - ETA: 0s - loss: 0.1190 - accuracy: 0.9538\n",
            "Epoch 00135: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1186 - accuracy: 0.9527 - val_loss: 0.1140 - val_accuracy: 0.9552\n",
            "Epoch 136/256\n",
            "785/820 [===========================>..] - ETA: 0s - loss: 0.1219 - accuracy: 0.9510\n",
            "Epoch 00136: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1187 - accuracy: 0.9524 - val_loss: 0.1177 - val_accuracy: 0.9531\n",
            "Epoch 137/256\n",
            "816/820 [============================>.] - ETA: 0s - loss: 0.1187 - accuracy: 0.9537\n",
            "Epoch 00137: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 2ms/step - loss: 0.1182 - accuracy: 0.9540 - val_loss: 0.1169 - val_accuracy: 0.9531\n",
            "Epoch 138/256\n",
            "780/820 [===========================>..] - ETA: 0s - loss: 0.1204 - accuracy: 0.9519\n",
            "Epoch 00138: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 2ms/step - loss: 0.1180 - accuracy: 0.9524 - val_loss: 0.1153 - val_accuracy: 0.9559\n",
            "Epoch 139/256\n",
            "802/820 [============================>.] - ETA: 0s - loss: 0.1155 - accuracy: 0.9548\n",
            "Epoch 00139: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 2ms/step - loss: 0.1186 - accuracy: 0.9534 - val_loss: 0.1158 - val_accuracy: 0.9552\n",
            "Epoch 140/256\n",
            "806/820 [============================>.] - ETA: 0s - loss: 0.1204 - accuracy: 0.9529\n",
            "Epoch 00140: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 2ms/step - loss: 0.1199 - accuracy: 0.9530 - val_loss: 0.1166 - val_accuracy: 0.9545\n",
            "Epoch 141/256\n",
            "777/820 [===========================>..] - ETA: 0s - loss: 0.1214 - accuracy: 0.9508\n",
            "Epoch 00141: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 2ms/step - loss: 0.1186 - accuracy: 0.9515 - val_loss: 0.1147 - val_accuracy: 0.9552\n",
            "Epoch 142/256\n",
            "784/820 [===========================>..] - ETA: 0s - loss: 0.1185 - accuracy: 0.9528\n",
            "Epoch 00142: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 2ms/step - loss: 0.1180 - accuracy: 0.9527 - val_loss: 0.1155 - val_accuracy: 0.9566\n",
            "Epoch 143/256\n",
            "809/820 [============================>.] - ETA: 0s - loss: 0.1190 - accuracy: 0.9530\n",
            "Epoch 00143: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 2ms/step - loss: 0.1186 - accuracy: 0.9530 - val_loss: 0.1150 - val_accuracy: 0.9559\n",
            "Epoch 144/256\n",
            "803/820 [============================>.] - ETA: 0s - loss: 0.1166 - accuracy: 0.9558\n",
            "Epoch 00144: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 2ms/step - loss: 0.1176 - accuracy: 0.9549 - val_loss: 0.1147 - val_accuracy: 0.9552\n",
            "Epoch 145/256\n",
            "789/820 [===========================>..] - ETA: 0s - loss: 0.1219 - accuracy: 0.9525\n",
            "Epoch 00145: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1196 - accuracy: 0.9534 - val_loss: 0.1167 - val_accuracy: 0.9566\n",
            "Epoch 146/256\n",
            "785/820 [===========================>..] - ETA: 0s - loss: 0.1166 - accuracy: 0.9541\n",
            "Epoch 00146: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1197 - accuracy: 0.9537 - val_loss: 0.1193 - val_accuracy: 0.9517\n",
            "Epoch 147/256\n",
            "780/820 [===========================>..] - ETA: 0s - loss: 0.1187 - accuracy: 0.9513\n",
            "Epoch 00147: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1188 - accuracy: 0.9518 - val_loss: 0.1162 - val_accuracy: 0.9552\n",
            "Epoch 148/256\n",
            "811/820 [============================>.] - ETA: 0s - loss: 0.1206 - accuracy: 0.9535\n",
            "Epoch 00148: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1198 - accuracy: 0.9537 - val_loss: 0.1153 - val_accuracy: 0.9545\n",
            "Epoch 149/256\n",
            "788/820 [===========================>..] - ETA: 0s - loss: 0.1206 - accuracy: 0.9534\n",
            "Epoch 00149: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1191 - accuracy: 0.9534 - val_loss: 0.1180 - val_accuracy: 0.9524\n",
            "Epoch 150/256\n",
            "774/820 [===========================>..] - ETA: 0s - loss: 0.1201 - accuracy: 0.9522\n",
            "Epoch 00150: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1186 - accuracy: 0.9527 - val_loss: 0.1169 - val_accuracy: 0.9531\n",
            "Epoch 151/256\n",
            "820/820 [==============================] - ETA: 0s - loss: 0.1199 - accuracy: 0.9521\n",
            "Epoch 00151: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1199 - accuracy: 0.9521 - val_loss: 0.1177 - val_accuracy: 0.9538\n",
            "Epoch 152/256\n",
            "780/820 [===========================>..] - ETA: 0s - loss: 0.1196 - accuracy: 0.9538\n",
            "Epoch 00152: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1202 - accuracy: 0.9534 - val_loss: 0.1197 - val_accuracy: 0.9517\n",
            "Epoch 153/256\n",
            "800/820 [============================>.] - ETA: 0s - loss: 0.1197 - accuracy: 0.9531\n",
            "Epoch 00153: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 2ms/step - loss: 0.1192 - accuracy: 0.9530 - val_loss: 0.1178 - val_accuracy: 0.9545\n",
            "Epoch 154/256\n",
            "820/820 [==============================] - ETA: 0s - loss: 0.1194 - accuracy: 0.9521\n",
            "Epoch 00154: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 2ms/step - loss: 0.1194 - accuracy: 0.9521 - val_loss: 0.1164 - val_accuracy: 0.9552\n",
            "Epoch 155/256\n",
            "810/820 [============================>.] - ETA: 0s - loss: 0.1203 - accuracy: 0.9525\n",
            "Epoch 00155: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 2ms/step - loss: 0.1191 - accuracy: 0.9530 - val_loss: 0.1186 - val_accuracy: 0.9531\n",
            "Epoch 156/256\n",
            "778/820 [===========================>..] - ETA: 0s - loss: 0.1208 - accuracy: 0.9508\n",
            "Epoch 00156: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 2ms/step - loss: 0.1194 - accuracy: 0.9515 - val_loss: 0.1206 - val_accuracy: 0.9495\n",
            "Epoch 157/256\n",
            "815/820 [============================>.] - ETA: 0s - loss: 0.1183 - accuracy: 0.9509\n",
            "Epoch 00157: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1177 - accuracy: 0.9512 - val_loss: 0.1151 - val_accuracy: 0.9552\n",
            "Epoch 158/256\n",
            "816/820 [============================>.] - ETA: 0s - loss: 0.1199 - accuracy: 0.9522\n",
            "Epoch 00158: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1193 - accuracy: 0.9524 - val_loss: 0.1154 - val_accuracy: 0.9545\n",
            "Epoch 159/256\n",
            "777/820 [===========================>..] - ETA: 0s - loss: 0.1181 - accuracy: 0.9533\n",
            "Epoch 00159: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1214 - accuracy: 0.9527 - val_loss: 0.1192 - val_accuracy: 0.9495\n",
            "Epoch 160/256\n",
            "784/820 [===========================>..] - ETA: 0s - loss: 0.1205 - accuracy: 0.9506\n",
            "Epoch 00160: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1200 - accuracy: 0.9503 - val_loss: 0.1161 - val_accuracy: 0.9545\n",
            "Epoch 161/256\n",
            "771/820 [===========================>..] - ETA: 0s - loss: 0.1196 - accuracy: 0.9530\n",
            "Epoch 00161: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1195 - accuracy: 0.9521 - val_loss: 0.1172 - val_accuracy: 0.9524\n",
            "Epoch 162/256\n",
            "776/820 [===========================>..] - ETA: 0s - loss: 0.1191 - accuracy: 0.9526\n",
            "Epoch 00162: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1198 - accuracy: 0.9527 - val_loss: 0.1176 - val_accuracy: 0.9531\n",
            "Epoch 163/256\n",
            "803/820 [============================>.] - ETA: 0s - loss: 0.1222 - accuracy: 0.9499\n",
            "Epoch 00163: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1211 - accuracy: 0.9506 - val_loss: 0.1148 - val_accuracy: 0.9538\n",
            "Epoch 164/256\n",
            "792/820 [===========================>..] - ETA: 0s - loss: 0.1235 - accuracy: 0.9530\n",
            "Epoch 00164: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1216 - accuracy: 0.9530 - val_loss: 0.1192 - val_accuracy: 0.9502\n",
            "Epoch 165/256\n",
            "786/820 [===========================>..] - ETA: 0s - loss: 0.1194 - accuracy: 0.9510\n",
            "Epoch 00165: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1193 - accuracy: 0.9509 - val_loss: 0.1158 - val_accuracy: 0.9531\n",
            "Epoch 166/256\n",
            "819/820 [============================>.] - ETA: 0s - loss: 0.1205 - accuracy: 0.9524\n",
            "Epoch 00166: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1204 - accuracy: 0.9524 - val_loss: 0.1165 - val_accuracy: 0.9545\n",
            "Epoch 167/256\n",
            "782/820 [===========================>..] - ETA: 0s - loss: 0.1237 - accuracy: 0.9508\n",
            "Epoch 00167: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1212 - accuracy: 0.9512 - val_loss: 0.1189 - val_accuracy: 0.9495\n",
            "Epoch 168/256\n",
            "802/820 [============================>.] - ETA: 0s - loss: 0.1173 - accuracy: 0.9511\n",
            "Epoch 00168: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 2ms/step - loss: 0.1201 - accuracy: 0.9506 - val_loss: 0.1167 - val_accuracy: 0.9559\n",
            "Epoch 169/256\n",
            "820/820 [==============================] - ETA: 0s - loss: 0.1200 - accuracy: 0.9512\n",
            "Epoch 00169: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1200 - accuracy: 0.9512 - val_loss: 0.1168 - val_accuracy: 0.9538\n",
            "Epoch 170/256\n",
            "803/820 [============================>.] - ETA: 0s - loss: 0.1211 - accuracy: 0.9524\n",
            "Epoch 00170: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1210 - accuracy: 0.9521 - val_loss: 0.1198 - val_accuracy: 0.9495\n",
            "Epoch 171/256\n",
            "780/820 [===========================>..] - ETA: 0s - loss: 0.1220 - accuracy: 0.9510\n",
            "Epoch 00171: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1199 - accuracy: 0.9518 - val_loss: 0.1159 - val_accuracy: 0.9545\n",
            "Epoch 172/256\n",
            "780/820 [===========================>..] - ETA: 0s - loss: 0.1222 - accuracy: 0.9532\n",
            "Epoch 00172: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1206 - accuracy: 0.9540 - val_loss: 0.1173 - val_accuracy: 0.9524\n",
            "Epoch 173/256\n",
            "772/820 [===========================>..] - ETA: 0s - loss: 0.1221 - accuracy: 0.9521\n",
            "Epoch 00173: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1206 - accuracy: 0.9515 - val_loss: 0.1158 - val_accuracy: 0.9559\n",
            "Epoch 174/256\n",
            "797/820 [============================>.] - ETA: 0s - loss: 0.1217 - accuracy: 0.9523\n",
            "Epoch 00174: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1204 - accuracy: 0.9524 - val_loss: 0.1164 - val_accuracy: 0.9545\n",
            "Epoch 175/256\n",
            "817/820 [============================>.] - ETA: 0s - loss: 0.1211 - accuracy: 0.9526\n",
            "Epoch 00175: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1209 - accuracy: 0.9527 - val_loss: 0.1164 - val_accuracy: 0.9552\n",
            "Epoch 176/256\n",
            "777/820 [===========================>..] - ETA: 0s - loss: 0.1231 - accuracy: 0.9517\n",
            "Epoch 00176: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1212 - accuracy: 0.9518 - val_loss: 0.1179 - val_accuracy: 0.9538\n",
            "Epoch 177/256\n",
            "798/820 [============================>.] - ETA: 0s - loss: 0.1210 - accuracy: 0.9530\n",
            "Epoch 00177: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 2ms/step - loss: 0.1217 - accuracy: 0.9524 - val_loss: 0.1216 - val_accuracy: 0.9481\n",
            "Epoch 178/256\n",
            "781/820 [===========================>..] - ETA: 0s - loss: 0.1223 - accuracy: 0.9494\n",
            "Epoch 00178: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1198 - accuracy: 0.9503 - val_loss: 0.1176 - val_accuracy: 0.9538\n",
            "Epoch 179/256\n",
            "805/820 [============================>.] - ETA: 0s - loss: 0.1216 - accuracy: 0.9531\n",
            "Epoch 00179: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 2ms/step - loss: 0.1217 - accuracy: 0.9527 - val_loss: 0.1159 - val_accuracy: 0.9559\n",
            "Epoch 180/256\n",
            "779/820 [===========================>..] - ETA: 0s - loss: 0.1181 - accuracy: 0.9525\n",
            "Epoch 00180: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1220 - accuracy: 0.9534 - val_loss: 0.1181 - val_accuracy: 0.9517\n",
            "Epoch 181/256\n",
            "809/820 [============================>.] - ETA: 0s - loss: 0.1219 - accuracy: 0.9502\n",
            "Epoch 00181: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1218 - accuracy: 0.9497 - val_loss: 0.1197 - val_accuracy: 0.9488\n",
            "Epoch 182/256\n",
            "814/820 [============================>.] - ETA: 0s - loss: 0.1212 - accuracy: 0.9509\n",
            "Epoch 00182: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1207 - accuracy: 0.9512 - val_loss: 0.1182 - val_accuracy: 0.9502\n",
            "Epoch 183/256\n",
            "782/820 [===========================>..] - ETA: 0s - loss: 0.1189 - accuracy: 0.9527\n",
            "Epoch 00183: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1199 - accuracy: 0.9521 - val_loss: 0.1163 - val_accuracy: 0.9552\n",
            "Epoch 184/256\n",
            "816/820 [============================>.] - ETA: 0s - loss: 0.1215 - accuracy: 0.9525\n",
            "Epoch 00184: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 2ms/step - loss: 0.1213 - accuracy: 0.9524 - val_loss: 0.1170 - val_accuracy: 0.9545\n",
            "Epoch 185/256\n",
            "783/820 [===========================>..] - ETA: 0s - loss: 0.1227 - accuracy: 0.9527\n",
            "Epoch 00185: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1229 - accuracy: 0.9524 - val_loss: 0.1164 - val_accuracy: 0.9559\n",
            "Epoch 186/256\n",
            "785/820 [===========================>..] - ETA: 0s - loss: 0.1192 - accuracy: 0.9535\n",
            "Epoch 00186: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1224 - accuracy: 0.9515 - val_loss: 0.1160 - val_accuracy: 0.9559\n",
            "Epoch 187/256\n",
            "818/820 [============================>.] - ETA: 0s - loss: 0.1239 - accuracy: 0.9520\n",
            "Epoch 00187: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1236 - accuracy: 0.9521 - val_loss: 0.1168 - val_accuracy: 0.9538\n",
            "Epoch 188/256\n",
            "802/820 [============================>.] - ETA: 0s - loss: 0.1211 - accuracy: 0.9526\n",
            "Epoch 00188: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1226 - accuracy: 0.9521 - val_loss: 0.1147 - val_accuracy: 0.9545\n",
            "Epoch 189/256\n",
            "779/820 [===========================>..] - ETA: 0s - loss: 0.1246 - accuracy: 0.9515\n",
            "Epoch 00189: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1228 - accuracy: 0.9512 - val_loss: 0.1155 - val_accuracy: 0.9545\n",
            "Epoch 190/256\n",
            "779/820 [===========================>..] - ETA: 0s - loss: 0.1245 - accuracy: 0.9522\n",
            "Epoch 00190: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1230 - accuracy: 0.9530 - val_loss: 0.1168 - val_accuracy: 0.9552\n",
            "Epoch 191/256\n",
            "818/820 [============================>.] - ETA: 0s - loss: 0.1231 - accuracy: 0.9514\n",
            "Epoch 00191: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1228 - accuracy: 0.9515 - val_loss: 0.1163 - val_accuracy: 0.9552\n",
            "Epoch 192/256\n",
            "777/820 [===========================>..] - ETA: 0s - loss: 0.1232 - accuracy: 0.9521\n",
            "Epoch 00192: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1234 - accuracy: 0.9518 - val_loss: 0.1161 - val_accuracy: 0.9559\n",
            "Epoch 193/256\n",
            "804/820 [============================>.] - ETA: 0s - loss: 0.1249 - accuracy: 0.9524\n",
            "Epoch 00193: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1242 - accuracy: 0.9524 - val_loss: 0.1188 - val_accuracy: 0.9510\n",
            "Epoch 194/256\n",
            "817/820 [============================>.] - ETA: 0s - loss: 0.1220 - accuracy: 0.9504\n",
            "Epoch 00194: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1219 - accuracy: 0.9506 - val_loss: 0.1159 - val_accuracy: 0.9545\n",
            "Epoch 195/256\n",
            "776/820 [===========================>..] - ETA: 0s - loss: 0.1270 - accuracy: 0.9504\n",
            "Epoch 00195: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1238 - accuracy: 0.9515 - val_loss: 0.1174 - val_accuracy: 0.9552\n",
            "Epoch 196/256\n",
            "789/820 [===========================>..] - ETA: 0s - loss: 0.1205 - accuracy: 0.9537\n",
            "Epoch 00196: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1232 - accuracy: 0.9521 - val_loss: 0.1167 - val_accuracy: 0.9559\n",
            "Epoch 197/256\n",
            "817/820 [============================>.] - ETA: 0s - loss: 0.1218 - accuracy: 0.9520\n",
            "Epoch 00197: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1230 - accuracy: 0.9518 - val_loss: 0.1161 - val_accuracy: 0.9545\n",
            "Epoch 198/256\n",
            "815/820 [============================>.] - ETA: 0s - loss: 0.1229 - accuracy: 0.9525\n",
            "Epoch 00198: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1235 - accuracy: 0.9521 - val_loss: 0.1167 - val_accuracy: 0.9552\n",
            "Epoch 199/256\n",
            "792/820 [===========================>..] - ETA: 0s - loss: 0.1248 - accuracy: 0.9533\n",
            "Epoch 00199: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1247 - accuracy: 0.9527 - val_loss: 0.1180 - val_accuracy: 0.9538\n",
            "Epoch 200/256\n",
            "811/820 [============================>.] - ETA: 0s - loss: 0.1255 - accuracy: 0.9516\n",
            "Epoch 00200: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1245 - accuracy: 0.9521 - val_loss: 0.1182 - val_accuracy: 0.9524\n",
            "Epoch 201/256\n",
            "819/820 [============================>.] - ETA: 0s - loss: 0.1234 - accuracy: 0.9509\n",
            "Epoch 00201: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1233 - accuracy: 0.9509 - val_loss: 0.1184 - val_accuracy: 0.9510\n",
            "Epoch 202/256\n",
            "780/820 [===========================>..] - ETA: 0s - loss: 0.1278 - accuracy: 0.9503\n",
            "Epoch 00202: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1240 - accuracy: 0.9515 - val_loss: 0.1170 - val_accuracy: 0.9545\n",
            "Epoch 203/256\n",
            "782/820 [===========================>..] - ETA: 0s - loss: 0.1223 - accuracy: 0.9517\n",
            "Epoch 00203: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1228 - accuracy: 0.9506 - val_loss: 0.1156 - val_accuracy: 0.9552\n",
            "Epoch 204/256\n",
            "786/820 [===========================>..] - ETA: 0s - loss: 0.1264 - accuracy: 0.9523\n",
            "Epoch 00204: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 2ms/step - loss: 0.1247 - accuracy: 0.9521 - val_loss: 0.1167 - val_accuracy: 0.9559\n",
            "Epoch 205/256\n",
            "804/820 [============================>.] - ETA: 0s - loss: 0.1271 - accuracy: 0.9540\n",
            "Epoch 00205: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1259 - accuracy: 0.9540 - val_loss: 0.1168 - val_accuracy: 0.9552\n",
            "Epoch 206/256\n",
            "813/820 [============================>.] - ETA: 0s - loss: 0.1233 - accuracy: 0.9511\n",
            "Epoch 00206: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1239 - accuracy: 0.9509 - val_loss: 0.1177 - val_accuracy: 0.9538\n",
            "Epoch 207/256\n",
            "789/820 [===========================>..] - ETA: 0s - loss: 0.1234 - accuracy: 0.9509\n",
            "Epoch 00207: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1246 - accuracy: 0.9500 - val_loss: 0.1177 - val_accuracy: 0.9545\n",
            "Epoch 208/256\n",
            "804/820 [============================>.] - ETA: 0s - loss: 0.1250 - accuracy: 0.9506\n",
            "Epoch 00208: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1244 - accuracy: 0.9509 - val_loss: 0.1188 - val_accuracy: 0.9510\n",
            "Epoch 209/256\n",
            "776/820 [===========================>..] - ETA: 0s - loss: 0.1241 - accuracy: 0.9517\n",
            "Epoch 00209: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1234 - accuracy: 0.9509 - val_loss: 0.1163 - val_accuracy: 0.9552\n",
            "Epoch 210/256\n",
            "782/820 [===========================>..] - ETA: 0s - loss: 0.1272 - accuracy: 0.9520\n",
            "Epoch 00210: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 2ms/step - loss: 0.1257 - accuracy: 0.9521 - val_loss: 0.1173 - val_accuracy: 0.9559\n",
            "Epoch 211/256\n",
            "784/820 [===========================>..] - ETA: 0s - loss: 0.1178 - accuracy: 0.9538\n",
            "Epoch 00211: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 2ms/step - loss: 0.1247 - accuracy: 0.9537 - val_loss: 0.1181 - val_accuracy: 0.9552\n",
            "Epoch 212/256\n",
            "787/820 [===========================>..] - ETA: 0s - loss: 0.1262 - accuracy: 0.9511\n",
            "Epoch 00212: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 2ms/step - loss: 0.1245 - accuracy: 0.9512 - val_loss: 0.1176 - val_accuracy: 0.9566\n",
            "Epoch 213/256\n",
            "775/820 [===========================>..] - ETA: 0s - loss: 0.1261 - accuracy: 0.9526\n",
            "Epoch 00213: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1255 - accuracy: 0.9524 - val_loss: 0.1190 - val_accuracy: 0.9517\n",
            "Epoch 214/256\n",
            "775/820 [===========================>..] - ETA: 0s - loss: 0.1240 - accuracy: 0.9490\n",
            "Epoch 00214: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1241 - accuracy: 0.9503 - val_loss: 0.1175 - val_accuracy: 0.9538\n",
            "Epoch 215/256\n",
            "805/820 [============================>.] - ETA: 0s - loss: 0.1240 - accuracy: 0.9516\n",
            "Epoch 00215: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1237 - accuracy: 0.9515 - val_loss: 0.1167 - val_accuracy: 0.9552\n",
            "Epoch 216/256\n",
            "778/820 [===========================>..] - ETA: 0s - loss: 0.1276 - accuracy: 0.9508\n",
            "Epoch 00216: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1248 - accuracy: 0.9518 - val_loss: 0.1189 - val_accuracy: 0.9524\n",
            "Epoch 217/256\n",
            "815/820 [============================>.] - ETA: 0s - loss: 0.1253 - accuracy: 0.9491\n",
            "Epoch 00217: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1251 - accuracy: 0.9494 - val_loss: 0.1176 - val_accuracy: 0.9538\n",
            "Epoch 218/256\n",
            "795/820 [============================>.] - ETA: 0s - loss: 0.1226 - accuracy: 0.9513\n",
            "Epoch 00218: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 2ms/step - loss: 0.1255 - accuracy: 0.9512 - val_loss: 0.1187 - val_accuracy: 0.9552\n",
            "Epoch 219/256\n",
            "788/820 [===========================>..] - ETA: 0s - loss: 0.1238 - accuracy: 0.9518\n",
            "Epoch 00219: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 2ms/step - loss: 0.1248 - accuracy: 0.9509 - val_loss: 0.1190 - val_accuracy: 0.9510\n",
            "Epoch 220/256\n",
            "819/820 [============================>.] - ETA: 0s - loss: 0.1227 - accuracy: 0.9527\n",
            "Epoch 00220: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1225 - accuracy: 0.9527 - val_loss: 0.1212 - val_accuracy: 0.9495\n",
            "Epoch 221/256\n",
            "783/820 [===========================>..] - ETA: 0s - loss: 0.1242 - accuracy: 0.9515\n",
            "Epoch 00221: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1233 - accuracy: 0.9506 - val_loss: 0.1188 - val_accuracy: 0.9524\n",
            "Epoch 222/256\n",
            "820/820 [==============================] - ETA: 0s - loss: 0.1241 - accuracy: 0.9506\n",
            "Epoch 00222: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1241 - accuracy: 0.9506 - val_loss: 0.1191 - val_accuracy: 0.9524\n",
            "Epoch 223/256\n",
            "793/820 [============================>.] - ETA: 0s - loss: 0.1239 - accuracy: 0.9505\n",
            "Epoch 00223: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1232 - accuracy: 0.9512 - val_loss: 0.1173 - val_accuracy: 0.9545\n",
            "Epoch 224/256\n",
            "810/820 [============================>.] - ETA: 0s - loss: 0.1239 - accuracy: 0.9522\n",
            "Epoch 00224: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1241 - accuracy: 0.9518 - val_loss: 0.1215 - val_accuracy: 0.9495\n",
            "Epoch 225/256\n",
            "798/820 [============================>.] - ETA: 0s - loss: 0.1151 - accuracy: 0.9533\n",
            "Epoch 00225: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1246 - accuracy: 0.9515 - val_loss: 0.1209 - val_accuracy: 0.9502\n",
            "Epoch 226/256\n",
            "784/820 [===========================>..] - ETA: 0s - loss: 0.1240 - accuracy: 0.9487\n",
            "Epoch 00226: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 2ms/step - loss: 0.1229 - accuracy: 0.9488 - val_loss: 0.1191 - val_accuracy: 0.9517\n",
            "Epoch 227/256\n",
            "792/820 [===========================>..] - ETA: 0s - loss: 0.1236 - accuracy: 0.9520\n",
            "Epoch 00227: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 2ms/step - loss: 0.1253 - accuracy: 0.9524 - val_loss: 0.1192 - val_accuracy: 0.9517\n",
            "Epoch 228/256\n",
            "782/820 [===========================>..] - ETA: 0s - loss: 0.1240 - accuracy: 0.9524\n",
            "Epoch 00228: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 2ms/step - loss: 0.1250 - accuracy: 0.9518 - val_loss: 0.1180 - val_accuracy: 0.9538\n",
            "Epoch 229/256\n",
            "814/820 [============================>.] - ETA: 0s - loss: 0.1262 - accuracy: 0.9521\n",
            "Epoch 00229: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 2ms/step - loss: 0.1259 - accuracy: 0.9521 - val_loss: 0.1192 - val_accuracy: 0.9510\n",
            "Epoch 230/256\n",
            "780/820 [===========================>..] - ETA: 0s - loss: 0.1241 - accuracy: 0.9503\n",
            "Epoch 00230: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 2ms/step - loss: 0.1243 - accuracy: 0.9512 - val_loss: 0.1208 - val_accuracy: 0.9502\n",
            "Epoch 231/256\n",
            "784/820 [===========================>..] - ETA: 0s - loss: 0.1238 - accuracy: 0.9503\n",
            "Epoch 00231: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 2ms/step - loss: 0.1228 - accuracy: 0.9506 - val_loss: 0.1163 - val_accuracy: 0.9545\n",
            "Epoch 232/256\n",
            "790/820 [===========================>..] - ETA: 0s - loss: 0.1218 - accuracy: 0.9525\n",
            "Epoch 00232: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1249 - accuracy: 0.9521 - val_loss: 0.1176 - val_accuracy: 0.9545\n",
            "Epoch 233/256\n",
            "778/820 [===========================>..] - ETA: 0s - loss: 0.1239 - accuracy: 0.9508\n",
            "Epoch 00233: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 2ms/step - loss: 0.1239 - accuracy: 0.9500 - val_loss: 0.1185 - val_accuracy: 0.9552\n",
            "Epoch 234/256\n",
            "793/820 [============================>.] - ETA: 0s - loss: 0.1252 - accuracy: 0.9518\n",
            "Epoch 00234: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 2ms/step - loss: 0.1245 - accuracy: 0.9518 - val_loss: 0.1198 - val_accuracy: 0.9510\n",
            "Epoch 235/256\n",
            "807/820 [============================>.] - ETA: 0s - loss: 0.1203 - accuracy: 0.9514\n",
            "Epoch 00235: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1253 - accuracy: 0.9509 - val_loss: 0.1199 - val_accuracy: 0.9502\n",
            "Epoch 236/256\n",
            "818/820 [============================>.] - ETA: 0s - loss: 0.1265 - accuracy: 0.9520\n",
            "Epoch 00236: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1264 - accuracy: 0.9521 - val_loss: 0.1208 - val_accuracy: 0.9495\n",
            "Epoch 237/256\n",
            "817/820 [============================>.] - ETA: 0s - loss: 0.1258 - accuracy: 0.9532\n",
            "Epoch 00237: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1253 - accuracy: 0.9534 - val_loss: 0.1186 - val_accuracy: 0.9538\n",
            "Epoch 238/256\n",
            "775/820 [===========================>..] - ETA: 0s - loss: 0.1284 - accuracy: 0.9490\n",
            "Epoch 00238: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1249 - accuracy: 0.9503 - val_loss: 0.1190 - val_accuracy: 0.9531\n",
            "Epoch 239/256\n",
            "805/820 [============================>.] - ETA: 0s - loss: 0.1273 - accuracy: 0.9522\n",
            "Epoch 00239: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1263 - accuracy: 0.9521 - val_loss: 0.1196 - val_accuracy: 0.9531\n",
            "Epoch 240/256\n",
            "795/820 [============================>.] - ETA: 0s - loss: 0.1254 - accuracy: 0.9506\n",
            "Epoch 00240: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1241 - accuracy: 0.9506 - val_loss: 0.1189 - val_accuracy: 0.9524\n",
            "Epoch 241/256\n",
            "769/820 [===========================>..] - ETA: 0s - loss: 0.1272 - accuracy: 0.9486\n",
            "Epoch 00241: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1247 - accuracy: 0.9500 - val_loss: 0.1214 - val_accuracy: 0.9495\n",
            "Epoch 242/256\n",
            "812/820 [============================>.] - ETA: 0s - loss: 0.1240 - accuracy: 0.9492\n",
            "Epoch 00242: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1228 - accuracy: 0.9497 - val_loss: 0.1171 - val_accuracy: 0.9552\n",
            "Epoch 243/256\n",
            "791/820 [===========================>..] - ETA: 0s - loss: 0.1218 - accuracy: 0.9523\n",
            "Epoch 00243: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1249 - accuracy: 0.9509 - val_loss: 0.1176 - val_accuracy: 0.9545\n",
            "Epoch 244/256\n",
            "819/820 [============================>.] - ETA: 0s - loss: 0.1257 - accuracy: 0.9518\n",
            "Epoch 00244: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1257 - accuracy: 0.9518 - val_loss: 0.1189 - val_accuracy: 0.9517\n",
            "Epoch 245/256\n",
            "804/820 [============================>.] - ETA: 0s - loss: 0.1244 - accuracy: 0.9527\n",
            "Epoch 00245: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1252 - accuracy: 0.9521 - val_loss: 0.1186 - val_accuracy: 0.9545\n",
            "Epoch 246/256\n",
            "817/820 [============================>.] - ETA: 0s - loss: 0.1253 - accuracy: 0.9523\n",
            "Epoch 00246: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1249 - accuracy: 0.9524 - val_loss: 0.1165 - val_accuracy: 0.9552\n",
            "Epoch 247/256\n",
            "804/820 [============================>.] - ETA: 0s - loss: 0.1260 - accuracy: 0.9534\n",
            "Epoch 00247: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1258 - accuracy: 0.9530 - val_loss: 0.1204 - val_accuracy: 0.9488\n",
            "Epoch 248/256\n",
            "817/820 [============================>.] - ETA: 0s - loss: 0.1264 - accuracy: 0.9501\n",
            "Epoch 00248: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1266 - accuracy: 0.9500 - val_loss: 0.1183 - val_accuracy: 0.9545\n",
            "Epoch 249/256\n",
            "818/820 [============================>.] - ETA: 0s - loss: 0.1266 - accuracy: 0.9526\n",
            "Epoch 00249: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1268 - accuracy: 0.9524 - val_loss: 0.1176 - val_accuracy: 0.9552\n",
            "Epoch 250/256\n",
            "787/820 [===========================>..] - ETA: 0s - loss: 0.1253 - accuracy: 0.9501\n",
            "Epoch 00250: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 2ms/step - loss: 0.1251 - accuracy: 0.9503 - val_loss: 0.1199 - val_accuracy: 0.9524\n",
            "Epoch 251/256\n",
            "799/820 [============================>.] - ETA: 0s - loss: 0.1262 - accuracy: 0.9503\n",
            "Epoch 00251: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1252 - accuracy: 0.9506 - val_loss: 0.1195 - val_accuracy: 0.9517\n",
            "Epoch 252/256\n",
            "782/820 [===========================>..] - ETA: 0s - loss: 0.1273 - accuracy: 0.9495\n",
            "Epoch 00252: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 2ms/step - loss: 0.1254 - accuracy: 0.9503 - val_loss: 0.1189 - val_accuracy: 0.9517\n",
            "Epoch 253/256\n",
            "797/820 [============================>.] - ETA: 0s - loss: 0.1257 - accuracy: 0.9526\n",
            "Epoch 00253: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1265 - accuracy: 0.9518 - val_loss: 0.1200 - val_accuracy: 0.9495\n",
            "Epoch 254/256\n",
            "778/820 [===========================>..] - ETA: 0s - loss: 0.1296 - accuracy: 0.9505\n",
            "Epoch 00254: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1264 - accuracy: 0.9518 - val_loss: 0.1208 - val_accuracy: 0.9481\n",
            "Epoch 255/256\n",
            "794/820 [============================>.] - ETA: 0s - loss: 0.1270 - accuracy: 0.9512\n",
            "Epoch 00255: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1267 - accuracy: 0.9509 - val_loss: 0.1183 - val_accuracy: 0.9524\n",
            "Epoch 256/256\n",
            "803/820 [============================>.] - ETA: 0s - loss: 0.1222 - accuracy: 0.9493\n",
            "Epoch 00256: val_loss did not improve from 0.10971\n",
            "820/820 [==============================] - 1s 1ms/step - loss: 0.1266 - accuracy: 0.9494 - val_loss: 0.1174 - val_accuracy: 0.9545\n",
            "Accuracy: 95.66%\n",
            "Accuracy: 95.66%\n",
            "Precision: 84.32%\n",
            "Recall: 89.24%\n",
            "F1-score: 0.87\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AA7k2ycWc2BH",
        "colab_type": "text"
      },
      "source": [
        "Accuracy with all the features =95.73\n",
        "\n",
        "Accuracy after removing 4 least significant features= 95.66 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6SZ_KfidYOV",
        "colab_type": "text"
      },
      "source": [
        "#D) Peer Review:\n",
        "After Peer Review session, I added implementation of Early stopping for training the dataset."
      ]
    }
  ]
}